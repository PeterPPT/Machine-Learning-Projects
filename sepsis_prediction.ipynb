{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sepsis Prediction using Recurrent Neural Network\n",
    "* Download the Prediction of Sepsis dataset from Kaggle: https://www.kaggle.com/datasets/salikhussaini49/prediction-of-sepsis and save it in storage/input\n",
    "* Context - \n",
    "    * Sepsis is a major public health concern with significant morbidity, mortality, and healthcare expenses. Early detection and antibiotic treatment of sepsis improve outcomes. Hospital can also save cost for managing sepsis if sepsis can be predicted in advance.\n",
    "* Challenge objective - \n",
    "    * To predict sepsis 6 hours before the clinical prediction of sepsis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from processor.library.preprocess.preprocess import preprocess\n",
    "from processor.library.feature_engineering.Risk_score import Risk_score\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score,roc_curve\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Hour    HR  O2Sat  Temp    SBP   MAP   DBP  Resp  EtCO2  BaseExcess  ...  \\\n",
      "0     0   NaN    NaN   NaN    NaN   NaN   NaN   NaN    NaN         NaN  ...   \n",
      "1     1  65.0  100.0   NaN    NaN  72.0   NaN  16.5    NaN         NaN  ...   \n",
      "2     2  78.0  100.0   NaN    NaN  42.5   NaN   NaN    NaN         NaN  ...   \n",
      "3     3  73.0  100.0   NaN    NaN   NaN   NaN  17.0    NaN         NaN  ...   \n",
      "4     4  70.0  100.0   NaN  129.0  74.0  69.0  14.0    NaN         NaN  ...   \n",
      "\n",
      "   Fibrinogen  Platelets    Age  Gender  Unit1  Unit2  HospAdmTime  ICULOS  \\\n",
      "0         NaN        NaN  68.54       0    NaN    NaN        -0.02       1   \n",
      "1         NaN        NaN  68.54       0    NaN    NaN        -0.02       2   \n",
      "2         NaN        NaN  68.54       0    NaN    NaN        -0.02       3   \n",
      "3         NaN        NaN  68.54       0    NaN    NaN        -0.02       4   \n",
      "4         NaN      330.0  68.54       0    NaN    NaN        -0.02       5   \n",
      "\n",
      "   SepsisLabel  Patient_ID  \n",
      "0            0       17072  \n",
      "1            0       17072  \n",
      "2            0       17072  \n",
      "3            0       17072  \n",
      "4            0       17072  \n",
      "\n",
      "[5 rows x 43 columns]\n"
     ]
    }
   ],
   "source": [
    "# Get current directory\n",
    "current_dir = os.getcwd()\n",
    "# Join current directory with the path having dataset\n",
    "file_path = os.path.join(current_dir,\"storage/input/sepsis_dataset.csv\")\n",
    "# Read dataset \n",
    "df = pd.read_csv(file_path)\n",
    "# Remove unnamed column\n",
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "# Print out raw data\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df:  (1552210, 43)\n",
      "No. of unique patient id:  40336\n"
     ]
    }
   ],
   "source": [
    "# Print out shape of raw data\n",
    "print('Shape of df: ', df.shape)\n",
    "# Print out the no: of unique patient id\n",
    "print('No. of unique patient id: ', len(df[\"Patient_ID\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df2:  (1552210, 40)\n"
     ]
    }
   ],
   "source": [
    "# drop Unit 1, Unit 2, and ICULOS columns\n",
    "df2 = df.drop(['Unit1', 'Unit2', 'ICULOS'], axis=1)\n",
    "print('Shape of df2: ', df2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train / test split according to patient ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test data by patient id\n",
    "group_split = GroupShuffleSplit(n_splits=2, test_size=.2, random_state=42)\n",
    "split = group_split.split(df2.drop(['SepsisLabel','Patient_ID'], axis=1),groups=df2['Patient_ID'])\n",
    "train_idx, test_idx = next(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of xtrain:  (1241213, 39)\n",
      "size of ytrain:  (1241213, 2)\n",
      "size of x_eval:  (310997, 39)\n",
      "size of yeval:  (310997, 2)\n"
     ]
    }
   ],
   "source": [
    "# Training data - xtrain\n",
    "x_train = df2.iloc[train_idx]\n",
    "# Train label \n",
    "y_train = x_train[['SepsisLabel','Patient_ID']]\n",
    "# drop sepsis label column from x_train\n",
    "x_train = x_train.drop(['SepsisLabel'], axis=1)\n",
    "print('size of xtrain: ', x_train.shape)\n",
    "print('size of ytrain: ', y_train.shape)\n",
    "\n",
    "# Evaluation data\n",
    "x_eval = df2.iloc[test_idx]\n",
    "y_eval = x_eval[['SepsisLabel','Patient_ID']]\n",
    "# drop sepsis label column\n",
    "x_eval = x_eval.drop(['SepsisLabel'], axis=1)\n",
    "print('size of x_eval: ', x_eval.shape)\n",
    "print('size of yeval: ', y_eval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column name list\n",
    "column_names = list(x_train.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for missing data in training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are there missing value in dataset:  True\n",
      "Missing values columns:  ['HR', 'O2Sat', 'Temp', 'SBP', 'MAP', 'DBP', 'Resp', 'EtCO2', 'BaseExcess', 'HCO3', 'FiO2', 'pH', 'PaCO2', 'SaO2', 'AST', 'BUN', 'Alkalinephos', 'Calcium', 'Chloride', 'Creatinine', 'Bilirubin_direct', 'Glucose', 'Lactate', 'Magnesium', 'Phosphate', 'Potassium', 'Bilirubin_total', 'TroponinI', 'Hct', 'Hgb', 'PTT', 'WBC', 'Fibrinogen', 'Platelets', 'HospAdmTime']\n",
      "Total number columns that has missing value:  35\n"
     ]
    }
   ],
   "source": [
    "# Check for missing data in xtrain\n",
    "pre_process = preprocess()\n",
    "result = pre_process.check_missingvalue(x_train)\n",
    "print(\"Are there missing value in dataset: \", result['HasMissingValue'])\n",
    "print(\"Missing values columns: \", result['missing_value_columns'])\n",
    "print(\"Total number columns that has missing value: \", len(result['missing_value_columns']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing data imputation by using Sklearn iterative imputation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value imputation using iterative imputation from sklearn\n",
    "imp = IterativeImputer(max_iter=10, random_state=0)\n",
    "imp.fit(x_train)\n",
    "\n",
    "x_train = imp.transform(x_train)\n",
    "x_eval = imp.transform(x_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy array to dataframe\n",
    "x_train = pd.DataFrame(x_train, columns = column_names)\n",
    "x_eval = pd.DataFrame(x_eval, columns = column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for missing data again after imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are there missing value in x_train:  False\n",
      "Are there missing value in x_eval:  False\n"
     ]
    }
   ],
   "source": [
    "# Check for missing data again after imputation\n",
    "result = pre_process.check_missingvalue(x_train)\n",
    "print(\"Are there missing value in x_train: \", result['HasMissingValue'])\n",
    "\n",
    "result = pre_process.check_missingvalue(x_eval)\n",
    "print(\"Are there missing value in x_eval: \", result['HasMissingValue'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Risk Scores - NEWs and SIRs scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Risk scores - NEWs and SIRs score\n",
    "rs = Risk_score()\n",
    "x_train = rs.Calculate_risk_score(x_train)\n",
    "x_eval = rs.Calculate_risk_score(x_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for balance / imbalance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/.local/share/virtualenvs/deep_learning_test-Kesrgl1c/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/home/peter/.local/share/virtualenvs/deep_learning_test-Kesrgl1c/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/home/peter/.local/share/virtualenvs/deep_learning_test-Kesrgl1c/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Counts unique class value\n",
    "class_ = y_train[\"SepsisLabel\"].value_counts()\n",
    "class_ = pd.DataFrame({'Class': class_.index,'values': class_.values})\n",
    "# plot bar chart of different class count\n",
    "ax = sns.barplot(x = 'Class', y = 'values', data = class_)\n",
    "# Add values above bars\n",
    "for i, v in enumerate(class_['values']):\n",
    "   ax.text(i, v + 0.3, str(v), ha='center')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Recurrent Neural Network, RNN using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of patient data:  32268\n",
      "Print the 4th element (3rd index) from output: \n",
      "data:  torch.Size([36, 38])\n",
      "tensor([[ 86.4284,  97.4984,  36.9425,  ...,   0.0000,   0.0000,   0.0000],\n",
      "        [119.0000,  98.0000,  37.3910,  ...,   0.0000,   4.0000,   1.0000],\n",
      "        [120.0000,  97.0000,  37.4114,  ...,   0.0000,   4.0000,   1.0000],\n",
      "        ...,\n",
      "        [109.0000, 100.0000,  37.3056,  ...,   0.0000,   3.0000,   1.0000],\n",
      "        [121.0000,  98.0000,  37.4789,  ...,   0.0000,   4.0000,   1.0000],\n",
      "        [124.0000,  98.0000,  36.8300,  ...,   0.0000,   5.0000,   1.0000]],\n",
      "       dtype=torch.float64)\n",
      "label:  torch.Size([36])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Total number of records:  36\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Creating implementation of custom dataset and dataloader\n",
    "class Patient_dataset(Dataset):\n",
    "    # Initializing operations\n",
    "    def __init__(self, input_f, labels):\n",
    "        '''\n",
    "            This function will perform initializing operation: reading input data (input_f), get unique patient ids and concatenate input data with label.\n",
    "        '''\n",
    "        self.data = input_f\n",
    "        self.patients = self.data['Patient_ID'].unique()\n",
    "        self.data = pd.concat([self.data.reset_index().drop([\"index\"],axis=1),labels.reset_index()[\"SepsisLabel\"]],axis=1)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "            This function returns the size of the input data\n",
    "        '''\n",
    "        return len(self.patients) \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "            This function will loads and returns a sample from the dataset at given index, idx. Based on the index, the function will \n",
    "                identify the location of the record, \n",
    "                convert that to a tensor,\n",
    "                retrieve the corresponding label and\n",
    "                return the tensor data, corresponding label and length of dataset in a tuple.\n",
    "        '''\n",
    "        intermediate = (self.data.loc[self.data[\"Patient_ID\"] == self.patients[idx],:].drop([\"Patient_ID\",\"Hour\",\"HospAdmTime\"],axis=1))\n",
    "        output = intermediate.drop([\"SepsisLabel\"],axis=1)\n",
    "        labels = intermediate[\"SepsisLabel\"]\n",
    "\n",
    "        return torch.tensor(output.values), torch.tensor(labels.values), output.shape[0]\n",
    "\n",
    "patient_df = Patient_dataset(x_train,y_train)\n",
    "\n",
    "print('Length of patient data: ', patient_df.__len__())\n",
    "print('Print the 4th element (3rd index) from output: ')\n",
    "print('data: ', patient_df.__getitem__(3)[0].shape)\n",
    "print(patient_df.__getitem__(3)[0])\n",
    "\n",
    "print('label: ', patient_df.__getitem__(3)[1].shape)\n",
    "print(patient_df.__getitem__(3)[1])\n",
    "\n",
    "print('Total number of records: ', patient_df.__getitem__(3)[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusting variable length of sequences with collate function\n",
    "* Because different patients have different number of records (number of rows), there are non-uniform sequence in the dataset which means some patients have few records while the other patients have more records. \n",
    "\n",
    "* To handle sequences of non-uniform length, shorter sequences are needed to be padded to the length of the longest sequence in a batch.\n",
    "This can be done using the collate_fn parameter of the DataLoader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "\n",
    "    # aggrate iterables in a tuple\n",
    "    sequences, labels, lengths = zip(*batch)\n",
    "    # convert length into 64-bit integer (signed)\n",
    "    lengths = torch.LongTensor(lengths)\n",
    "    # convert each sequence data into Tensor datatype\n",
    "    sequences = [torch.Tensor(s) for s in sequences]\n",
    "    # convert each label into Tensor datatype\n",
    "    labels = [torch.Tensor(l) for l in labels]\n",
    "\n",
    "    # pad_sequence function will stacks list of Tensors along a new dimension and pads them to equal length.\n",
    "    # pad_sequence function will return a Tensor size (B x T x N). \n",
    "    #   B = batch size\n",
    "    #   T = length of longest sequence (longest patient's records)\n",
    "    #   N = length of features dimension\n",
    "    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    labels_padded = pad_sequence(labels,batch_first=True,padding_value=-1)\n",
    "    \n",
    "    return sequences_padded, labels_padded, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an iterable dataset object (DataLoader) and iterate over it in batches, which are then fed into the model for processing\n",
    "patient_dataloader = DataLoader(patient_df, batch_size=64, collate_fn=collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:  torch.Size([64, 84, 38])\n",
      "tensor([[[ 88.0853,  97.6027,  36.9762,  ...,   0.0000,   0.0000,   0.0000],\n",
      "         [ 95.0000, 100.0000,  37.1100,  ...,   0.0000,   1.0000,   1.0000],\n",
      "         [ 80.5000, 100.0000,  37.0380,  ...,   0.0000,   2.0000,   0.0000],\n",
      "         ...,\n",
      "         [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[ 84.8457,  97.3184,  37.0091,  ...,   1.0000,   0.0000,   0.0000],\n",
      "         [ 73.5000, 100.0000,  36.0500,  ...,   1.0000,   0.0000,   0.0000],\n",
      "         [ 74.0000, 100.0000,  36.4000,  ...,   1.0000,   0.0000,   0.0000],\n",
      "         ...,\n",
      "         [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[ 85.8853,  97.4086,  36.8566,  ...,   0.0000,   0.0000,   0.0000],\n",
      "         [ 72.0000, 100.0000,  36.8000,  ...,   0.0000,   3.0000,   0.0000],\n",
      "         [ 70.0000, 100.0000,  36.9500,  ...,   0.0000,   1.0000,   0.0000],\n",
      "         ...,\n",
      "         [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 87.7732,  97.5918,  36.9816,  ...,   0.0000,   0.0000,   0.0000],\n",
      "         [106.5000,  94.5000,  37.1533,  ...,   0.0000,   5.0000,   1.0000],\n",
      "         [101.0000, 100.0000,  37.1635,  ...,   0.0000,   4.0000,   1.0000],\n",
      "         ...,\n",
      "         [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[ 78.1610,  96.9009,  36.6707,  ...,   0.0000,   0.0000,   0.0000],\n",
      "         [ 77.0000,  98.0000,  36.4652,  ...,   0.0000,   2.0000,   0.0000],\n",
      "         [ 88.5000,  92.5000,  36.8000,  ...,   0.0000,   4.0000,   0.0000],\n",
      "         ...,\n",
      "         [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[ 87.0000, 100.0000,  36.7000,  ...,   1.0000,   1.0000,   0.0000],\n",
      "         [ 85.0000, 100.0000,  36.7500,  ...,   1.0000,   1.0000,   0.0000],\n",
      "         [ 79.0000,  98.0000,  36.6000,  ...,   1.0000,   2.0000,   0.0000],\n",
      "         ...,\n",
      "         [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]]],\n",
      "       dtype=torch.float64)\n",
      "\n",
      "\n",
      "Train label:  torch.Size([64, 84])\n",
      "tensor([[ 0,  0,  0,  ..., -1, -1, -1],\n",
      "        [ 0,  0,  0,  ..., -1, -1, -1],\n",
      "        [ 0,  0,  0,  ..., -1, -1, -1],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ..., -1, -1, -1],\n",
      "        [ 0,  0,  0,  ..., -1, -1, -1],\n",
      "        [ 0,  0,  0,  ..., -1, -1, -1]])\n",
      "\n",
      "\n",
      "Size in each batch:  tensor([33, 22, 27, 30, 49, 28, 50, 58,  8, 45, 46, 28, 17, 46, 24, 40, 22, 41,\n",
      "        32, 57, 47, 38, 47, 34, 42, 41, 59, 25, 46, 39, 44, 44, 14, 44, 70, 19,\n",
      "        43, 43, 49, 56, 48, 31, 26, 41, 32, 39, 43, 38, 25, 84, 51, 44, 52, 36,\n",
      "        47, 58, 41, 37, 13, 57, 18, 31, 37, 35])\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels, size_in_each_batch = next(iter(patient_dataloader))\n",
    "print('Train data: ', train_data.shape)\n",
    "print(train_data)\n",
    "print('\\n')\n",
    "print('Train label: ', train_labels.shape) # list type\n",
    "print(train_labels)\n",
    "print('\\n')\n",
    "print('Size in each batch: ', size_in_each_batch)\n",
    "\n",
    "# print('shape of dataloader: ', i[0].shape, i[1].shape,i[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pack padded sequences\n",
    "* It is difficult to batch the variable length sequences. Hence, it is necessary to pad all the sequences that will result in the longest sequence. In example, there are 4 sequences with variable length (for example, [89, 41, 45, 37]). If we want to pass these sequences to Recurrent Neural Network (RNN), we have to pad all the sequences to maximum sequence leng in each batch, which is 89.\n",
    "\n",
    "* If we do the matrix multiplication this \"padded batch of sequences\" with weight matrix, there will be computation intensive and most of the computed results would be 0s because of zero padding to max length of sequence.\n",
    "\n",
    "* Therefore, to optimize the processing of variable-length sequences, PyTorch uses packing sequences that allows us to process variable-length sequences without the need for padding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([0, 0, 0,  ..., 1, 1, 1]), batch_sizes=tensor([64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 63, 63, 60, 58, 56,\n",
       "        53, 52, 51, 50, 50, 49, 46, 44, 41, 39, 38, 38, 38, 37, 37, 37, 36, 36,\n",
       "        34, 29, 25, 24, 21, 20, 20, 18, 17, 16, 12, 12, 12, 12, 11, 10,  9,  8,\n",
       "         6,  6,  6,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3,  3,  3,  2,  2,  2,  2,  2,  2,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1]), sorted_indices=tensor([13, 50, 20, 23, 29, 18, 25, 32, 11, 28,  9, 10, 27, 55, 52, 17, 34, 37,\n",
       "        47, 56, 30, 51, 60, 35, 39, 26, 16, 59,  2, 40,  6, 36,  0, 49, 24, 62,\n",
       "        19, 54,  4, 44, 12, 53,  1,  5, 43, 22, 61, 46,  7, 21, 63, 31, 14, 57,\n",
       "        41, 45,  3, 42, 33, 38, 15,  8, 58, 48]), unsorted_indices=tensor([32, 42, 28, 56, 38, 43, 30, 48, 61, 10, 11,  8, 40,  0, 52, 60, 26, 15,\n",
       "         5, 36,  2, 49, 45,  3, 34,  6, 25, 12,  9,  4, 20, 51,  7, 58, 16, 23,\n",
       "        31, 17, 59, 24, 29, 54, 57, 44, 39, 55, 47, 18, 63, 33,  1, 21, 14, 41,\n",
       "        37, 13, 19, 53, 62, 27, 22, 46, 35, 50]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "i = next(iter(patient_dataloader))\n",
    "# Print one of pack padded sequences\n",
    "pack_padded_sequence(i[1],torch.as_tensor(i[2]),batch_first=True,enforce_sorted=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define Simple RNN\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, layer_size, output_size, nonlinearity):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "\n",
    "        # # Defining some parameters - Hidden layer size and number of layer\n",
    "        # self.hidden_dim = hidden_size\n",
    "        # self.n_layers = layer_size\n",
    "\n",
    "        # Define multi-layer RNN \n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=layer_size, batch_first=True, nonlinearity=nonlinearity)\n",
    "        # Applied layer normalization \n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        '''\n",
    "            Forward pass.\n",
    "        '''\n",
    "        # Convert to floating point datatype.\n",
    "        x = x.to(torch.float32)\n",
    "\n",
    "        # pack the padded sequences to opimize computation\n",
    "        x = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        # Pass both input and hidden state to into model and get outputs\n",
    "        x, _ = self.rnn(x)\n",
    "        # Pad the packed sequence before passing to fully connected network\n",
    "        x, _ = nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
    "        # Normalize input\n",
    "        x = self.norm(x)\n",
    "        # Pass fully connected network\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of features in the input data\n",
    "input_size = int(train_data.shape[2])\n",
    "# Number of features in the hidden state, h\n",
    "hidden_size = int(input_size/2)\n",
    "# number of recurrent layer\n",
    "layer_size = 2\n",
    "# Because it is binary classification problem, output have to be a vector of length 1\n",
    "output_size = 1\n",
    "# Nonlinearity function. Can be either 'tanh' or 'relu'\n",
    "nonlinearity = 'relu'\n",
    "\n",
    "# Instantiate the RNN model with hyperparameter\n",
    "model = SimpleRNN(input_size=input_size, hidden_size=hidden_size, layer_size=layer_size, output_size=output_size, nonlinearity=nonlinearity)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the ratio between majority class (class 0) and minority class (class 1)\n",
    "* This weight ratio will be used in weight loss to penalized majority class during training of RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0  count:  1218544\n",
      "class 1  count:  22669\n",
      "Positibe weight value:  0.01860334957129164\n"
     ]
    }
   ],
   "source": [
    "# implement weighted loss\n",
    "# Total number of class 0\n",
    "class0_count = (y_train['SepsisLabel']==0).sum()\n",
    "print('class 0  count: ', class0_count)\n",
    "# Total number of class 1\n",
    "class1_count = (y_train['SepsisLabel']==1).sum()\n",
    "print('class 1  count: ', class1_count)\n",
    "\n",
    "# # Ratio\n",
    "if class0_count > class1_count: \n",
    "    # weight_ratio = 1/class0_count/len(y_train)\n",
    "    weight_ratio = class1_count/class0_count\n",
    "else:\n",
    "    # weight_ratio = 1/class1_count/len(y_train)\n",
    "    weight_ratio = class0_count/class1_count\n",
    "\n",
    "pos_weight = math.sqrt(weight_ratio**2)\n",
    "print('Positibe weight value: ', pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.03236616775393486\n",
      "Loss: 0.03232957050204277\n",
      "Loss: 0.02325868047773838\n",
      "Loss: 0.03799263387918472\n",
      "Loss: 0.030926857143640518\n",
      "Loss: 0.032350171357393265\n",
      "Loss: 0.027422336861491203\n",
      "Loss: 0.02473219856619835\n",
      "Loss: 0.040272850543260574\n",
      "Loss: 0.02650294452905655\n",
      "Loss: 0.02018027752637863\n",
      "Loss: 0.03535814955830574\n",
      "Loss: 0.018626578152179718\n",
      "Loss: 0.022990580648183823\n",
      "Loss: 0.027325084432959557\n",
      "Loss: 0.019270632416009903\n",
      "Loss: 0.03280510753393173\n",
      "Loss: 0.025455109775066376\n",
      "Loss: 0.01883293315768242\n",
      "Loss: 0.014445607550442219\n",
      "Loss: 0.025002628564834595\n",
      "Loss: 0.043582845479249954\n",
      "Loss: 0.01701677031815052\n",
      "Loss: 0.05745823308825493\n",
      "Loss: 0.024702202528715134\n",
      "Loss: 0.024015657603740692\n",
      "Loss: 0.03250914812088013\n",
      "Loss: 0.02458496391773224\n",
      "Loss: 0.014651858247816563\n",
      "Loss: 0.021253284066915512\n",
      "Loss: 0.02209407463669777\n",
      "Loss: 0.032522596418857574\n",
      "Loss: 0.02692720852792263\n",
      "Loss: 0.029656069353222847\n",
      "Loss: 0.01629670150578022\n",
      "Loss: 0.03253084421157837\n",
      "Loss: 0.026459727436304092\n",
      "Loss: 0.026853393763303757\n",
      "Loss: 0.033299770206213\n",
      "Loss: 0.03228205814957619\n",
      "Loss: 0.022146007046103477\n",
      "Loss: 0.0437656007707119\n",
      "Loss: 0.04821553826332092\n",
      "Loss: 0.03398002311587334\n",
      "Loss: 0.05780491232872009\n",
      "Loss: 0.030376136302947998\n",
      "Loss: 0.05056207999587059\n",
      "Loss: 0.04379495978355408\n",
      "Loss: 0.05566184222698212\n",
      "Loss: 0.028210142627358437\n",
      "Loss: 0.028545554727315903\n",
      "Loss: 0.04169337823987007\n",
      "Loss: 0.04392876476049423\n",
      "Loss: 0.025353752076625824\n",
      "Loss: 0.021555978804826736\n",
      "Loss: 0.033639807254076004\n",
      "Loss: 0.03185654804110527\n",
      "Loss: 0.02687940187752247\n",
      "Loss: 0.0292764101177454\n",
      "Loss: 0.028873829171061516\n",
      "Loss: 0.025622347369790077\n",
      "Loss: 0.03278791159391403\n",
      "Loss: 0.02606746181845665\n",
      "Loss: 0.03177119418978691\n",
      "Loss: 0.036509595811367035\n",
      "Loss: 0.026602094992995262\n",
      "Loss: 0.022771470248699188\n",
      "Loss: 0.04615902528166771\n",
      "Loss: 0.026998508721590042\n",
      "Loss: 0.04489823430776596\n",
      "Loss: 0.034903865307569504\n",
      "Loss: 0.037060197442770004\n",
      "Loss: 0.02007392793893814\n",
      "Loss: 0.03307735547423363\n",
      "Loss: 0.03787602484226227\n",
      "Loss: 0.030244946479797363\n",
      "Loss: 0.034765489399433136\n",
      "Loss: 0.029569540172815323\n",
      "Loss: 0.034408535808324814\n",
      "Loss: 0.028122317045927048\n",
      "Loss: 0.03193537890911102\n",
      "Loss: 0.031006306409835815\n",
      "Loss: 0.031478602439165115\n",
      "Loss: 0.028996163979172707\n",
      "Loss: 0.03607361018657684\n",
      "Loss: 0.03623504936695099\n",
      "Loss: 0.020911352708935738\n",
      "Loss: 0.03270486742258072\n",
      "Loss: 0.026899276301264763\n",
      "Loss: 0.033455848693847656\n",
      "Loss: 0.021386999636888504\n",
      "Loss: 0.0240655355155468\n",
      "Loss: 0.02531471475958824\n",
      "Loss: 0.037145402282476425\n",
      "Loss: 0.03978914022445679\n",
      "Loss: 0.017493264749646187\n",
      "Loss: 0.03299817070364952\n",
      "Loss: 0.03588731214404106\n",
      "Loss: 0.026390457525849342\n",
      "Loss: 0.052676860243082047\n",
      "Loss: 0.03808743134140968\n",
      "Loss: 0.031998638063669205\n",
      "Loss: 0.018788013607263565\n",
      "Loss: 0.031147141009569168\n",
      "Loss: 0.037914764136075974\n",
      "Loss: 0.023921115323901176\n",
      "Loss: 0.028217704966664314\n",
      "Loss: 0.02629229798913002\n",
      "Loss: 0.0360586903989315\n",
      "Loss: 0.023375123739242554\n",
      "Loss: 0.024455690756440163\n",
      "Loss: 0.046652987599372864\n",
      "Loss: 0.03285743668675423\n",
      "Loss: 0.027143362909555435\n",
      "Loss: 0.036733441054821014\n",
      "Loss: 0.018720341846346855\n",
      "Loss: 0.03811289370059967\n",
      "Loss: 0.024744104593992233\n",
      "Loss: 0.03412075340747833\n",
      "Loss: 0.03801320865750313\n",
      "Loss: 0.03669140487909317\n",
      "Loss: 0.03266708552837372\n",
      "Loss: 0.02876324951648712\n",
      "Loss: 0.038633741438388824\n",
      "Loss: 0.025227516889572144\n",
      "Loss: 0.03902577608823776\n",
      "Loss: 0.02630964107811451\n",
      "Loss: 0.027576707303524017\n",
      "Loss: 0.025204038247466087\n",
      "Loss: 0.026540927588939667\n",
      "Loss: 0.020876551046967506\n",
      "Loss: 0.026361536234617233\n",
      "Loss: 0.02639099955558777\n",
      "Loss: 0.03090447559952736\n",
      "Loss: 0.03608524799346924\n",
      "Loss: 0.024570612236857414\n",
      "Loss: 0.02503991313278675\n",
      "Loss: 0.027126917615532875\n",
      "Loss: 0.0422404482960701\n",
      "Loss: 0.03339498117566109\n",
      "Loss: 0.034123942255973816\n",
      "Loss: 0.034113653004169464\n",
      "Loss: 0.02832399308681488\n",
      "Loss: 0.03235289826989174\n",
      "Loss: 0.020843736827373505\n",
      "Loss: 0.025706132873892784\n",
      "Loss: 0.03820720314979553\n",
      "Loss: 0.03251821547746658\n",
      "Loss: 0.031993597745895386\n",
      "Loss: 0.026409925892949104\n",
      "Loss: 0.02474123053252697\n",
      "Loss: 0.03988415002822876\n",
      "Loss: 0.040110670030117035\n",
      "Loss: 0.02595023438334465\n",
      "Loss: 0.032474584877491\n",
      "Loss: 0.01881076581776142\n",
      "Loss: 0.023825015872716904\n",
      "Loss: 0.022632136940956116\n",
      "Loss: 0.02453765459358692\n",
      "Loss: 0.029538553208112717\n",
      "Loss: 0.025713536888360977\n",
      "Loss: 0.03287815675139427\n",
      "Loss: 0.03140644356608391\n",
      "Loss: 0.032460469752550125\n",
      "Loss: 0.037726979702711105\n",
      "Loss: 0.020907392725348473\n",
      "Loss: 0.03882161155343056\n",
      "Loss: 0.02026795782148838\n",
      "Loss: 0.01845674403011799\n",
      "Loss: 0.02836032211780548\n",
      "Loss: 0.03191964700818062\n",
      "Loss: 0.02413271926343441\n",
      "Loss: 0.027724681422114372\n",
      "Loss: 0.02304002456367016\n",
      "Loss: 0.024894917383790016\n",
      "Loss: 0.06250529736280441\n",
      "Loss: 0.02388566918671131\n",
      "Loss: 0.029589707031846046\n",
      "Loss: 0.030596671625971794\n",
      "Loss: 0.04186636209487915\n",
      "Loss: 0.019663996994495392\n",
      "Loss: 0.02859341911971569\n",
      "Loss: 0.046585965901613235\n",
      "Loss: 0.038266249001026154\n",
      "Loss: 0.0253143347799778\n",
      "Loss: 0.02604808285832405\n",
      "Loss: 0.027978507801890373\n",
      "Loss: 0.039506565779447556\n",
      "Loss: 0.03172822296619415\n",
      "Loss: 0.019743628799915314\n",
      "Loss: 0.03857307508587837\n",
      "Loss: 0.02256375551223755\n",
      "Loss: 0.028009722009301186\n",
      "Loss: 0.029460029676556587\n",
      "Loss: 0.03290063515305519\n",
      "Loss: 0.02342349663376808\n",
      "Loss: 0.04274861514568329\n",
      "Loss: 0.03573853522539139\n",
      "Loss: 0.023454679176211357\n",
      "Loss: 0.02828490361571312\n",
      "Loss: 0.02335534244775772\n",
      "Loss: 0.037557024508714676\n",
      "Loss: 0.025926169008016586\n",
      "Loss: 0.026102203875780106\n",
      "Loss: 0.03914022818207741\n",
      "Loss: 0.02765876054763794\n",
      "Loss: 0.028888076543807983\n",
      "Loss: 0.01983656734228134\n",
      "Loss: 0.026154782623052597\n",
      "Loss: 0.03184576705098152\n",
      "Loss: 0.03501104563474655\n",
      "Loss: 0.04630635306239128\n",
      "Loss: 0.017104191705584526\n",
      "Loss: 0.034124597907066345\n",
      "Loss: 0.01875285431742668\n",
      "Loss: 0.022340163588523865\n",
      "Loss: 0.034950315952301025\n",
      "Loss: 0.0309925377368927\n",
      "Loss: 0.02656685560941696\n",
      "Loss: 0.031960345804691315\n",
      "Loss: 0.03046768717467785\n",
      "Loss: 0.02718939259648323\n",
      "Loss: 0.028878867626190186\n",
      "Loss: 0.034611042588949203\n",
      "Loss: 0.023992130532860756\n",
      "Loss: 0.043650560081005096\n",
      "Loss: 0.024321017786860466\n",
      "Loss: 0.028435148298740387\n",
      "Loss: 0.025886524468660355\n",
      "Loss: 0.04014812409877777\n",
      "Loss: 0.04380062222480774\n",
      "Loss: 0.02824660949409008\n",
      "Loss: 0.03383343666791916\n",
      "Loss: 0.0398615300655365\n",
      "Loss: 0.02811978943645954\n",
      "Loss: 0.03447418659925461\n",
      "Loss: 0.025223085656762123\n",
      "Loss: 0.02031783200800419\n",
      "Loss: 0.028770780190825462\n",
      "Loss: 0.039999160915613174\n",
      "Loss: 0.04243161901831627\n",
      "Loss: 0.027165992185473442\n",
      "Loss: 0.02801606059074402\n",
      "Loss: 0.027456417679786682\n",
      "Loss: 0.028906524181365967\n",
      "Loss: 0.025317559018731117\n",
      "Loss: 0.030763525515794754\n",
      "Loss: 0.026412775740027428\n",
      "Loss: 0.01986813358962536\n",
      "Loss: 0.03308552876114845\n",
      "Loss: 0.02333955653011799\n",
      "Loss: 0.027676288038492203\n",
      "Loss: 0.05039152875542641\n",
      "Loss: 0.024723991751670837\n",
      "Loss: 0.038697563111782074\n",
      "Loss: 0.019450649619102478\n",
      "Loss: 0.02827838808298111\n",
      "Loss: 0.033647917211055756\n",
      "Loss: 0.03526647016406059\n",
      "Loss: 0.027083341032266617\n",
      "Loss: 0.040580060333013535\n",
      "Loss: 0.030639078468084335\n",
      "Loss: 0.024106230586767197\n",
      "Loss: 0.027518093585968018\n",
      "Loss: 0.05079444497823715\n",
      "Loss: 0.02748897112905979\n",
      "Loss: 0.03211566060781479\n",
      "Loss: 0.02662361040711403\n",
      "Loss: 0.0326748825609684\n",
      "Loss: 0.035155195742845535\n",
      "Loss: 0.025173446163535118\n",
      "Loss: 0.030434846878051758\n",
      "Loss: 0.039777517318725586\n",
      "Loss: 0.032480694353580475\n",
      "Loss: 0.03706004098057747\n",
      "Loss: 0.02854357846081257\n",
      "Loss: 0.02886221557855606\n",
      "Loss: 0.025641359388828278\n",
      "Loss: 0.026307521387934685\n",
      "Loss: 0.023545358330011368\n",
      "Loss: 0.024049703031778336\n",
      "Loss: 0.02849539928138256\n",
      "Loss: 0.03830898553133011\n",
      "Loss: 0.0317513681948185\n",
      "Loss: 0.03505127504467964\n",
      "Loss: 0.031522225588560104\n",
      "Loss: 0.035258810967206955\n",
      "Loss: 0.018264641985297203\n",
      "Loss: 0.024287540465593338\n",
      "Loss: 0.04003223031759262\n",
      "Loss: 0.044495582580566406\n",
      "Loss: 0.04803011566400528\n",
      "Loss: 0.02644401229918003\n",
      "Loss: 0.02494734153151512\n",
      "Loss: 0.023972488939762115\n",
      "Loss: 0.029809771105647087\n",
      "Loss: 0.02146907150745392\n",
      "Loss: 0.029190342873334885\n",
      "Loss: 0.027937181293964386\n",
      "Loss: 0.02365349978208542\n",
      "Loss: 0.02662436105310917\n",
      "Loss: 0.022225234657526016\n",
      "Loss: 0.05062180012464523\n",
      "Loss: 0.037558794021606445\n",
      "Loss: 0.025014160200953484\n",
      "Loss: 0.03504243120551109\n",
      "Loss: 0.0392591767013073\n",
      "Loss: 0.022920357063412666\n",
      "Loss: 0.023713326081633568\n",
      "Loss: 0.04496251791715622\n",
      "Loss: 0.030057307332754135\n",
      "Loss: 0.053181540220975876\n",
      "Loss: 0.033505287021398544\n",
      "Loss: 0.03687475621700287\n",
      "Loss: 0.027949204668402672\n",
      "Loss: 0.03488168865442276\n",
      "Loss: 0.03025699034333229\n",
      "Loss: 0.035302091389894485\n",
      "Loss: 0.040836773812770844\n",
      "Loss: 0.03546549007296562\n",
      "Loss: 0.029946181923151016\n",
      "Loss: 0.03611304983496666\n",
      "Loss: 0.03277454897761345\n",
      "Loss: 0.027651354670524597\n",
      "Loss: 0.04221508651971817\n",
      "Loss: 0.02749168872833252\n",
      "Loss: 0.02819763496518135\n",
      "Loss: 0.02683090791106224\n",
      "Loss: 0.03376618027687073\n",
      "Loss: 0.030333396047353745\n",
      "Loss: 0.03472228720784187\n",
      "Loss: 0.023446982726454735\n",
      "Loss: 0.02465227246284485\n",
      "Loss: 0.018695270642638206\n",
      "Loss: 0.025645311921834946\n",
      "Loss: 0.05756404623389244\n",
      "Loss: 0.03363014757633209\n",
      "Loss: 0.021555475890636444\n",
      "Loss: 0.03778814896941185\n",
      "Loss: 0.03616270422935486\n",
      "Loss: 0.01927080564200878\n",
      "Loss: 0.024282339960336685\n",
      "Loss: 0.036898959428071976\n",
      "Loss: 0.02587125450372696\n",
      "Loss: 0.03708459064364433\n",
      "Loss: 0.02290135622024536\n",
      "Loss: 0.03177725151181221\n",
      "Loss: 0.03211231529712677\n",
      "Loss: 0.027410246431827545\n",
      "Loss: 0.0217928197234869\n",
      "Loss: 0.028359055519104004\n",
      "Loss: 0.03664518520236015\n",
      "Loss: 0.0236594770103693\n",
      "Loss: 0.033278752118349075\n",
      "Loss: 0.0265506599098444\n",
      "Loss: 0.026581697165966034\n",
      "Loss: 0.02961360104382038\n",
      "Loss: 0.02272038906812668\n",
      "Loss: 0.03662668168544769\n",
      "Loss: 0.03190544992685318\n",
      "Loss: 0.03813169524073601\n",
      "Loss: 0.04120505973696709\n",
      "Loss: 0.044421639293432236\n",
      "Loss: 0.025715123862028122\n",
      "Loss: 0.027393454685807228\n",
      "Loss: 0.03311804309487343\n",
      "Loss: 0.029588749632239342\n",
      "Loss: 0.03848843649029732\n",
      "Loss: 0.034748028963804245\n",
      "Loss: 0.030355216935276985\n",
      "Loss: 0.02586429938673973\n",
      "Loss: 0.03351186588406563\n",
      "Loss: 0.04217521846294403\n",
      "Loss: 0.02940225601196289\n",
      "Loss: 0.047615133225917816\n",
      "Loss: 0.04221448302268982\n",
      "Loss: 0.02811189368367195\n",
      "Loss: 0.023142792284488678\n",
      "Loss: 0.027187498286366463\n",
      "Loss: 0.021798906847834587\n",
      "Loss: 0.02261127158999443\n",
      "Loss: 0.031898535788059235\n",
      "Loss: 0.026830919086933136\n",
      "Loss: 0.029526662081480026\n",
      "Loss: 0.02475135400891304\n",
      "Loss: 0.024997683241963387\n",
      "Loss: 0.025653943419456482\n",
      "Loss: 0.030325429514050484\n",
      "Loss: 0.04123857617378235\n",
      "Loss: 0.03312171623110771\n",
      "Loss: 0.03741171211004257\n",
      "Loss: 0.024961154907941818\n",
      "Loss: 0.028276439756155014\n",
      "Loss: 0.0266319140791893\n",
      "Loss: 0.03259760141372681\n",
      "Loss: 0.020574767142534256\n",
      "Loss: 0.026012087240815163\n",
      "Loss: 0.030561571940779686\n",
      "Loss: 0.031616054475307465\n",
      "Loss: 0.023492177948355675\n",
      "Loss: 0.027123695239424706\n",
      "Loss: 0.0332205556333065\n",
      "Loss: 0.03488578647375107\n",
      "Loss: 0.02385985665023327\n",
      "Loss: 0.037687309086322784\n",
      "Loss: 0.044500987976789474\n",
      "Loss: 0.025851406157016754\n",
      "Loss: 0.021976228803396225\n",
      "Loss: 0.04077400639653206\n",
      "Loss: 0.0413561575114727\n",
      "Loss: 0.024315450340509415\n",
      "Loss: 0.02333560213446617\n",
      "Loss: 0.03934512659907341\n",
      "Loss: 0.018740996718406677\n",
      "Loss: 0.023029863834381104\n",
      "Loss: 0.02970990352332592\n",
      "Loss: 0.02318759635090828\n",
      "Loss: 0.04498288780450821\n",
      "Loss: 0.028303340077400208\n",
      "Loss: 0.018659157678484917\n",
      "Loss: 0.028037279844284058\n",
      "Loss: 0.030638566240668297\n",
      "Loss: 0.04263709485530853\n",
      "Loss: 0.01628682389855385\n",
      "Loss: 0.024316875264048576\n",
      "Loss: 0.027843888849020004\n",
      "Loss: 0.02823133021593094\n",
      "Loss: 0.023394212126731873\n",
      "Loss: 0.01735086552798748\n",
      "Loss: 0.03569523245096207\n",
      "Loss: 0.029138915240764618\n",
      "Loss: 0.0245388001203537\n",
      "Loss: 0.02414642833173275\n",
      "Loss: 0.034451887011528015\n",
      "Loss: 0.038111086934804916\n",
      "Loss: 0.027370493859052658\n",
      "Loss: 0.03861863911151886\n",
      "Loss: 0.03256310895085335\n",
      "Loss: 0.03777744248509407\n",
      "Loss: 0.03939002379775047\n",
      "Loss: 0.031613897532224655\n",
      "Loss: 0.03749646991491318\n",
      "Loss: 0.03448033705353737\n",
      "Loss: 0.0244786087423563\n",
      "Loss: 0.021352414041757584\n",
      "Loss: 0.021736586466431618\n",
      "Loss: 0.029338333755731583\n",
      "Loss: 0.026095466688275337\n",
      "Loss: 0.03629565238952637\n",
      "Loss: 0.03341161832213402\n",
      "Loss: 0.01511041633784771\n",
      "Loss: 0.027808241546154022\n",
      "Loss: 0.025805603712797165\n",
      "Loss: 0.04403136298060417\n",
      "Loss: 0.02721933275461197\n",
      "Loss: 0.02726656384766102\n",
      "Loss: 0.027746107429265976\n",
      "Loss: 0.049711015075445175\n",
      "Loss: 0.05325711891055107\n",
      "Loss: 0.044645022600889206\n",
      "Loss: 0.04749796912074089\n",
      "Loss: 0.0383053794503212\n",
      "Loss: 0.020147157832980156\n",
      "Loss: 0.055447056889534\n",
      "Loss: 0.04427523538470268\n",
      "Loss: 0.023728907108306885\n",
      "Loss: 0.03963666409254074\n",
      "Loss: 0.032425131648778915\n",
      "Loss: 0.027907127514481544\n",
      "Loss: 0.02894490398466587\n",
      "Loss: 0.03455933928489685\n",
      "Loss: 0.02865588665008545\n",
      "Loss: 0.02747868373990059\n",
      "Loss: 0.04284254088997841\n",
      "Loss: 0.030145611613988876\n",
      "Loss: 0.024408552795648575\n",
      "Loss: 0.033043272793293\n",
      "Loss: 0.026695983484387398\n",
      "Loss: 0.03963681682944298\n",
      "Loss: 0.030942369252443314\n",
      "Loss: 0.026210084557533264\n",
      "Loss: 0.03431098163127899\n",
      "Loss: 0.023304905742406845\n",
      "Loss: 0.02147444523870945\n",
      "Loss: 0.04182703047990799\n",
      "Loss: 0.025457434356212616\n",
      "Loss: 0.02814687043428421\n",
      "Loss: 0.0422845222055912\n",
      "Loss: 0.02765362337231636\n",
      "Loss: 0.03822531923651695\n",
      "Loss: 0.028428515419363976\n",
      "Loss: 0.03557329997420311\n",
      "Loss: 0.027116484940052032\n",
      "Loss: 0.0360516719520092\n",
      "Loss: 0.02993619069457054\n",
      "Loss: 0.03882727026939392\n",
      "Loss: 0.03966057300567627\n",
      "Loss: 0.027683397755026817\n",
      "Loss: 0.03906203806400299\n",
      "Loss: 0.03633567690849304\n",
      "Loss: 0.031501252204179764\n",
      "Loss: 0.031216103583574295\n",
      "Loss: 0.03140147402882576\n",
      "Loss: 0.029130998998880386\n",
      "Loss: 0.023412689566612244\n",
      "Loss: 0.019888117909431458\n",
      "Loss: 0.017508158460259438\n",
      "Loss: 0.02557499334216118\n",
      "Loss: 0.027742203325033188\n",
      "Loss: 0.03245590254664421\n",
      "Loss: 0.03141093999147415\n",
      "Loss: 0.02792411297559738\n",
      "Loss: 0.059736210852861404\n",
      "Loss: 0.02855468913912773\n",
      "Loss: 0.030254658311605453\n",
      "Loss: 0.026901494711637497\n",
      "Loss: 0.02196326106786728\n",
      "Loss: 0.04159442335367203\n",
      "Loss: 0.02611127868294716\n",
      "Loss: 0.0235936027020216\n",
      "Loss: 0.03632672131061554\n",
      "Loss: 0.03432126343250275\n",
      "Loss: 0.030483655631542206\n",
      "Loss: 0.03745086491107941\n",
      "Loss: 0.040078453719615936\n",
      "Loss: 0.02861974574625492\n",
      "Loss: 0.03303613141179085\n",
      "Loss: 0.04406791180372238\n",
      "Loss: 0.03141201287508011\n",
      "Loss: 0.023616058751940727\n",
      "Loss: 0.02748512662947178\n",
      "Loss: 0.03436443582177162\n",
      "Loss: 0.023189209401607513\n",
      "Loss: 0.031149670481681824\n",
      "Loss: 0.03017951361835003\n",
      "Loss: 0.022130753844976425\n",
      "Loss: 0.01969659887254238\n",
      "Loss: 0.03876453638076782\n",
      "Loss: 0.037864699959754944\n",
      "Loss: 0.027850983664393425\n",
      "Loss: 0.02259167656302452\n",
      "Loss: 0.03997817635536194\n",
      "Loss: 0.03167777135968208\n",
      "Loss: 0.027089174836874008\n",
      "Loss: 0.02508261799812317\n",
      "Loss: 0.03181077539920807\n",
      "Loss: 0.04745909944176674\n",
      "Loss: 0.02749592810869217\n",
      "Loss: 0.031742580235004425\n",
      "Loss: 0.026867007836699486\n",
      "Loss: 0.028787456452846527\n",
      "Loss: 0.03319361433386803\n",
      "Loss: 0.027167487889528275\n",
      "Loss: 0.028291357681155205\n",
      "Loss: 0.0361955463886261\n",
      "Loss: 0.031108757480978966\n",
      "Loss: 0.02909386157989502\n",
      "Loss: 0.027949949726462364\n",
      "Loss: 0.043863508850336075\n",
      "Loss: 0.023892557248473167\n",
      "Loss: 0.03726283460855484\n",
      "Loss: 0.028278181329369545\n",
      "Loss: 0.025147201493382454\n",
      "Loss: 0.023944562301039696\n",
      "Loss: 0.04088332876563072\n",
      "Loss: 0.034791331738233566\n",
      "Loss: 0.0313098281621933\n",
      "Loss: 0.03667105361819267\n",
      "Loss: 0.025680378079414368\n",
      "Loss: 0.020498810335993767\n",
      "Loss: 0.03694545850157738\n",
      "Loss: 0.03116310015320778\n",
      "Loss: 0.03370625153183937\n",
      "Loss: 0.021282820031046867\n",
      "Loss: 0.03270205855369568\n",
      "Loss: 0.020920364186167717\n",
      "Loss: 0.020502876490354538\n",
      "Loss: 0.02843906171619892\n",
      "Loss: 0.02908306010067463\n",
      "Loss: 0.03208409994840622\n",
      "Loss: 0.027041954919695854\n",
      "Loss: 0.027448218315839767\n",
      "Loss: 0.01598975993692875\n",
      "Loss: 0.01967509090900421\n",
      "Loss: 0.05239017680287361\n",
      "Loss: 0.02455517277121544\n",
      "Loss: 0.018959052860736847\n",
      "Loss: 0.03388103470206261\n",
      "Loss: 0.027088254690170288\n",
      "Loss: 0.03251638635993004\n",
      "Loss: 0.029208030551671982\n",
      "Loss: 0.03613123670220375\n",
      "Loss: 0.03252359852194786\n",
      "Loss: 0.03734869509935379\n",
      "Loss: 0.03290179371833801\n",
      "Loss: 0.02303127385675907\n",
      "Loss: 0.04093047231435776\n",
      "Loss: 0.030664050951600075\n",
      "Loss: 0.030448105186223984\n",
      "Loss: 0.025945022702217102\n",
      "Loss: 0.03316071629524231\n",
      "Loss: 0.03295406326651573\n",
      "Loss: 0.032137542963027954\n",
      "Loss: 0.029815036803483963\n",
      "Loss: 0.025471828877925873\n",
      "Loss: 0.023701578378677368\n",
      "Loss: 0.02841128222644329\n",
      "Loss: 0.026752391830086708\n",
      "Loss: 0.028348637744784355\n",
      "Loss: 0.049310680478811264\n",
      "Loss: 0.030199751257896423\n",
      "Loss: 0.023861434310674667\n",
      "Loss: 0.025471089407801628\n",
      "Loss: 0.027305319905281067\n",
      "Loss: 0.040844254195690155\n",
      "Loss: 0.03729180991649628\n",
      "Loss: 0.0318288616836071\n",
      "Loss: 0.033057067543268204\n",
      "Loss: 0.031388286501169205\n",
      "Loss: 0.032652612775564194\n",
      "Loss: 0.025581825524568558\n",
      "Loss: 0.030527155846357346\n",
      "Loss: 0.027106873691082\n",
      "Loss: 0.04972434788942337\n",
      "Loss: 0.02820490300655365\n",
      "Loss: 0.03167445585131645\n",
      "Loss: 0.01778208650648594\n",
      "Loss: 0.021435650065541267\n",
      "Loss: 0.017412714660167694\n",
      "Loss: 0.035624440759420395\n",
      "Loss: 0.034197039902210236\n",
      "Loss: 0.029256870970129967\n",
      "Loss: 0.03697226196527481\n",
      "Loss: 0.015171924605965614\n",
      "Loss: 0.04309691861271858\n",
      "Loss: 0.04017726704478264\n",
      "Loss: 0.016463926061987877\n",
      "Loss: 0.043399933725595474\n",
      "Loss: 0.020200248807668686\n",
      "Loss: 0.020919643342494965\n",
      "Loss: 0.033575475215911865\n",
      "Loss: 0.027296053245663643\n",
      "Loss: 0.02752065099775791\n",
      "Loss: 0.026845335960388184\n",
      "Loss: 0.02274252288043499\n",
      "Loss: 0.02605992555618286\n",
      "Loss: 0.021603453904390335\n",
      "Loss: 0.02741820551455021\n",
      "Loss: 0.034152526408433914\n",
      "Loss: 0.02864725887775421\n",
      "Loss: 0.03436528891324997\n",
      "Loss: 0.03436241298913956\n",
      "Loss: 0.02674500085413456\n",
      "Loss: 0.037081532180309296\n",
      "Loss: 0.02563333883881569\n",
      "Loss: 0.02523956261575222\n",
      "Loss: 0.03380035609006882\n",
      "Loss: 0.025193341076374054\n",
      "Loss: 0.030731644481420517\n",
      "Loss: 0.02915305458009243\n",
      "Loss: 0.034767914563417435\n",
      "Loss: 0.02844659611582756\n",
      "Loss: 0.02350386418402195\n",
      "Loss: 0.024376658722758293\n",
      "Loss: 0.025157274678349495\n",
      "Loss: 0.021889736875891685\n",
      "Loss: 0.016970319673419\n",
      "Loss: 0.025652427226305008\n",
      "Loss: 0.021651551127433777\n",
      "Loss: 0.023721858859062195\n",
      "Loss: 0.025336600840091705\n",
      "Loss: 0.01643163152039051\n",
      "Loss: 0.03754502162337303\n",
      "Loss: 0.03088126704096794\n",
      "Loss: 0.02445029467344284\n",
      "Loss: 0.03172319382429123\n",
      "Loss: 0.01615876518189907\n",
      "Loss: 0.025079870596528053\n",
      "Loss: 0.0365772545337677\n",
      "Loss: 0.028636811301112175\n",
      "Loss: 0.019070789217948914\n",
      "Loss: 0.03268491476774216\n",
      "Loss: 0.02632731944322586\n",
      "Loss: 0.03144107386469841\n",
      "Loss: 0.02878992073237896\n",
      "Loss: 0.035142675042152405\n",
      "Loss: 0.026725128293037415\n",
      "Loss: 0.02479635551571846\n",
      "Loss: 0.0424240380525589\n",
      "Loss: 0.04007609933614731\n",
      "Loss: 0.03935129567980766\n",
      "Loss: 0.02407853677868843\n",
      "Loss: 0.03382503241300583\n",
      "Loss: 0.022735312581062317\n",
      "Loss: 0.03205252066254616\n",
      "Loss: 0.02783999592065811\n",
      "Loss: 0.024658361449837685\n",
      "Loss: 0.0311493668705225\n",
      "Loss: 0.026028849184513092\n",
      "Loss: 0.05250641703605652\n",
      "Loss: 0.028836671262979507\n",
      "Loss: 0.017312753945589066\n",
      "Loss: 0.04179023951292038\n",
      "Loss: 0.020716512575745583\n",
      "Loss: 0.02009858563542366\n",
      "Loss: 0.020718539133667946\n",
      "Loss: 0.032855335623025894\n",
      "Loss: 0.021038591861724854\n",
      "Loss: 0.025054244324564934\n",
      "Loss: 0.01939707063138485\n",
      "Loss: 0.022560371086001396\n",
      "Loss: 0.021154353395104408\n",
      "Loss: 0.04247424378991127\n",
      "Loss: 0.026041172444820404\n",
      "Loss: 0.027474813163280487\n",
      "Loss: 0.028044303879141808\n",
      "Loss: 0.03576761484146118\n",
      "Loss: 0.03809598088264465\n",
      "Loss: 0.021096188575029373\n",
      "Loss: 0.024049794301390648\n",
      "Loss: 0.03023359179496765\n",
      "Loss: 0.028291963040828705\n",
      "Loss: 0.023894885554909706\n",
      "Loss: 0.030018489807844162\n",
      "Loss: 0.02537236176431179\n",
      "Loss: 0.019720837473869324\n",
      "Loss: 0.026387333869934082\n",
      "Loss: 0.02901814877986908\n",
      "Loss: 0.03798921778798103\n",
      "Loss: 0.022563906386494637\n",
      "Loss: 0.02627912536263466\n",
      "Loss: 0.014248590916395187\n",
      "Loss: 0.029907438904047012\n",
      "Loss: 0.03159139305353165\n",
      "Loss: 0.02546459250152111\n",
      "Loss: 0.020935652777552605\n",
      "Loss: 0.03761624917387962\n",
      "Loss: 0.019346116110682487\n",
      "Loss: 0.034555330872535706\n",
      "Loss: 0.026157744228839874\n",
      "Loss: 0.03174585849046707\n",
      "Loss: 0.02876070700585842\n",
      "Loss: 0.02620844356715679\n",
      "Loss: 0.021373288705945015\n",
      "Loss: 0.031206339597702026\n",
      "Loss: 0.020325491204857826\n",
      "Loss: 0.023512424901127815\n",
      "Loss: 0.02611996978521347\n",
      "Loss: 0.04949192702770233\n",
      "Loss: 0.02917233482003212\n",
      "Loss: 0.027063589543104172\n",
      "Loss: 0.021617643535137177\n",
      "Loss: 0.03430474177002907\n",
      "Loss: 0.04529764503240585\n",
      "Loss: 0.024743065237998962\n",
      "Loss: 0.033905819058418274\n",
      "Loss: 0.032715145498514175\n",
      "Loss: 0.03408053144812584\n",
      "Loss: 0.041295718401670456\n",
      "Loss: 0.02883724868297577\n",
      "Loss: 0.03468093276023865\n",
      "Loss: 0.02468191646039486\n",
      "Loss: 0.03818196430802345\n",
      "Loss: 0.036729663610458374\n",
      "Loss: 0.044543396681547165\n",
      "Loss: 0.023907333612442017\n",
      "Loss: 0.022198179736733437\n",
      "Loss: 0.038138262927532196\n",
      "Loss: 0.020770473405718803\n",
      "Loss: 0.026984699070453644\n",
      "Loss: 0.03583338484168053\n",
      "Loss: 0.02491576410830021\n",
      "Loss: 0.038568489253520966\n",
      "Loss: 0.020504653453826904\n",
      "Loss: 0.03213346377015114\n",
      "Loss: 0.021718358621001244\n",
      "Loss: 0.025903556495904922\n",
      "Loss: 0.025629539042711258\n",
      "Loss: 0.023842163383960724\n",
      "Loss: 0.01764754205942154\n",
      "Loss: 0.03448622673749924\n",
      "Loss: 0.024938805028796196\n",
      "Loss: 0.05131201073527336\n",
      "Loss: 0.04284876957535744\n",
      "Loss: 0.02328912913799286\n",
      "Loss: 0.031092578545212746\n",
      "Loss: 0.03170814365148544\n",
      "Loss: 0.03074897825717926\n",
      "Loss: 0.03286132961511612\n",
      "Loss: 0.0306854248046875\n",
      "Loss: 0.02598145417869091\n",
      "Loss: 0.025868721306324005\n",
      "Loss: 0.02794233337044716\n",
      "Loss: 0.030693909153342247\n",
      "Loss: 0.024779342114925385\n",
      "Loss: 0.033320702612400055\n",
      "Loss: 0.03160260245203972\n",
      "Loss: 0.0304093174636364\n",
      "Loss: 0.02315542660653591\n",
      "Loss: 0.03932775557041168\n",
      "Loss: 0.03831646591424942\n",
      "Loss: 0.0228965412825346\n",
      "Loss: 0.019695919007062912\n",
      "Loss: 0.02292388677597046\n",
      "Loss: 0.03373994305729866\n",
      "Loss: 0.023625822737812996\n",
      "Loss: 0.025668159127235413\n",
      "Loss: 0.024661598727107048\n",
      "Loss: 0.026201672852039337\n",
      "Loss: 0.033495597541332245\n",
      "Loss: 0.02274235151708126\n",
      "Loss: 0.021917320787906647\n",
      "Loss: 0.027332037687301636\n",
      "Loss: 0.025239266455173492\n",
      "Loss: 0.03462117910385132\n",
      "Loss: 0.03297114372253418\n",
      "Loss: 0.03859328106045723\n",
      "Loss: 0.042533013969659805\n",
      "Loss: 0.022620735689997673\n",
      "Loss: 0.019888151437044144\n",
      "Loss: 0.03191422298550606\n",
      "Loss: 0.020830560475587845\n",
      "Loss: 0.02577904425561428\n",
      "Loss: 0.02442743629217148\n",
      "Loss: 0.026275981217622757\n",
      "Loss: 0.01918346993625164\n",
      "Loss: 0.02430136688053608\n",
      "Loss: 0.017722351476550102\n",
      "Loss: 0.03791451081633568\n",
      "Loss: 0.036734689027071\n",
      "Loss: 0.02986779436469078\n",
      "Loss: 0.027455784380435944\n",
      "Loss: 0.03304389491677284\n",
      "Loss: 0.024130897596478462\n",
      "Loss: 0.024080827832221985\n",
      "Loss: 0.02829311415553093\n",
      "Loss: 0.016411641612648964\n",
      "Loss: 0.03733910620212555\n",
      "Loss: 0.01689395122230053\n",
      "Loss: 0.033626265823841095\n",
      "Loss: 0.029214950278401375\n",
      "Loss: 0.022846633568406105\n",
      "Loss: 0.02759554423391819\n",
      "Loss: 0.023503722622990608\n",
      "Loss: 0.03164951503276825\n",
      "Loss: 0.02838682383298874\n",
      "Loss: 0.027073532342910767\n",
      "Loss: 0.024628527462482452\n",
      "Loss: 0.025403376668691635\n",
      "Loss: 0.03412473201751709\n",
      "Loss: 0.02585536800324917\n",
      "Loss: 0.048722535371780396\n",
      "Loss: 0.02381301298737526\n",
      "Loss: 0.035557616502046585\n",
      "Loss: 0.03644917160272598\n",
      "Loss: 0.03415832296013832\n",
      "Loss: 0.029741521924734116\n",
      "Loss: 0.026833346113562584\n",
      "Loss: 0.03363673388957977\n",
      "Loss: 0.03340435400605202\n",
      "Loss: 0.0242996197193861\n",
      "Loss: 0.034140367060899734\n",
      "Loss: 0.02908930554986\n",
      "Loss: 0.02494954504072666\n",
      "Loss: 0.023096591234207153\n",
      "Loss: 0.024696379899978638\n",
      "Loss: 0.021455716341733932\n",
      "Loss: 0.02996312826871872\n",
      "Loss: 0.03869990259408951\n",
      "Loss: 0.03235958144068718\n",
      "Loss: 0.03318949416279793\n",
      "Loss: 0.01589128188788891\n",
      "Loss: 0.027015862986445427\n",
      "Loss: 0.0435573011636734\n",
      "Loss: 0.023431314155459404\n",
      "Loss: 0.0372241847217083\n",
      "Loss: 0.03712383657693863\n",
      "Loss: 0.023974096402525902\n",
      "Loss: 0.020661644637584686\n",
      "Loss: 0.027261320501565933\n",
      "Loss: 0.02363486960530281\n",
      "Loss: 0.03358129784464836\n",
      "Loss: 0.018930990248918533\n",
      "Loss: 0.024939879775047302\n",
      "Loss: 0.019836291670799255\n",
      "Loss: 0.0282669048756361\n",
      "Loss: 0.036330848932266235\n",
      "Loss: 0.025278959423303604\n",
      "Loss: 0.01994347758591175\n",
      "Loss: 0.03160391002893448\n",
      "Loss: 0.02400708571076393\n",
      "Loss: 0.03282419592142105\n",
      "Loss: 0.027385316789150238\n",
      "Loss: 0.03312266245484352\n",
      "Loss: 0.02188708446919918\n",
      "Loss: 0.027981098741292953\n",
      "Loss: 0.02016700990498066\n",
      "Loss: 0.0442555695772171\n",
      "Loss: 0.026805764064192772\n",
      "Loss: 0.026231812313199043\n",
      "Loss: 0.023139387369155884\n",
      "Loss: 0.037452347576618195\n",
      "Loss: 0.027574222534894943\n",
      "Loss: 0.02733255736529827\n",
      "Loss: 0.029902486130595207\n",
      "Loss: 0.02621561847627163\n",
      "Loss: 0.019105400890111923\n",
      "Loss: 0.039146702736616135\n",
      "Loss: 0.027986574918031693\n",
      "Loss: 0.021753814071416855\n",
      "Loss: 0.022738641127943993\n",
      "Loss: 0.01829330436885357\n",
      "Loss: 0.027409549802541733\n",
      "Loss: 0.030786588788032532\n",
      "Loss: 0.05189227685332298\n",
      "Loss: 0.029329495504498482\n",
      "Loss: 0.038278400897979736\n",
      "Loss: 0.0400373637676239\n",
      "Loss: 0.043680790811777115\n",
      "Loss: 0.026532001793384552\n",
      "Loss: 0.03562374785542488\n",
      "Loss: 0.03915117681026459\n",
      "Loss: 0.019762761890888214\n",
      "Loss: 0.03296119347214699\n",
      "Loss: 0.027743682265281677\n",
      "Loss: 0.02513464167714119\n",
      "Loss: 0.024677827954292297\n",
      "Loss: 0.024792062118649483\n",
      "Loss: 0.03937146067619324\n",
      "Loss: 0.04736701399087906\n",
      "Loss: 0.043524209409952164\n",
      "Loss: 0.030539069324731827\n",
      "Loss: 0.03823932632803917\n",
      "Loss: 0.0292112547904253\n",
      "Loss: 0.02876831218600273\n",
      "Loss: 0.03663872554898262\n",
      "Loss: 0.027408137917518616\n",
      "Loss: 0.04000916704535484\n",
      "Loss: 0.02468384988605976\n",
      "Loss: 0.032387539744377136\n",
      "Loss: 0.03460405766963959\n",
      "Loss: 0.03922257572412491\n",
      "Loss: 0.02887704037129879\n",
      "Loss: 0.020696280524134636\n",
      "Loss: 0.031029636040329933\n",
      "Loss: 0.028236808255314827\n",
      "Loss: 0.023334268480539322\n",
      "Loss: 0.034951191395521164\n",
      "Loss: 0.02574963867664337\n",
      "Loss: 0.021450022235512733\n",
      "Loss: 0.02507535181939602\n",
      "Loss: 0.038234297186136246\n",
      "Loss: 0.01886271871626377\n",
      "Loss: 0.026090968400239944\n",
      "Loss: 0.024098794907331467\n",
      "Loss: 0.02183283492922783\n",
      "Loss: 0.05113178491592407\n",
      "Loss: 0.03421803191304207\n",
      "Loss: 0.02502862736582756\n",
      "Loss: 0.018001820892095566\n",
      "Loss: 0.017517952248454094\n",
      "Loss: 0.020959872752428055\n",
      "Loss: 0.022251686081290245\n",
      "Loss: 0.02004615217447281\n",
      "Loss: 0.03177238255739212\n",
      "Loss: 0.01529346127063036\n",
      "Loss: 0.02929232269525528\n",
      "Loss: 0.05686338245868683\n",
      "Loss: 0.044282231479883194\n",
      "Loss: 0.050106458365917206\n",
      "Loss: 0.024038884788751602\n",
      "Loss: 0.03184562921524048\n",
      "Loss: 0.02827327698469162\n",
      "Loss: 0.024314362555742264\n",
      "Loss: 0.02902134135365486\n",
      "Loss: 0.032463930547237396\n",
      "Loss: 0.03891909494996071\n",
      "Loss: 0.027141747996211052\n",
      "Loss: 0.03378154709935188\n",
      "Loss: 0.026414288207888603\n",
      "Loss: 0.026345517486333847\n",
      "Loss: 0.02257402241230011\n",
      "Loss: 0.034291766583919525\n",
      "Loss: 0.025312870740890503\n",
      "Loss: 0.02522451803088188\n",
      "Loss: 0.025034235790371895\n",
      "Loss: 0.029566720128059387\n",
      "Loss: 0.03324378281831741\n",
      "Loss: 0.022569656372070312\n",
      "Loss: 0.02782880701124668\n",
      "Loss: 0.022740423679351807\n",
      "Loss: 0.02446148730814457\n",
      "Loss: 0.04143867269158363\n",
      "Loss: 0.02744288556277752\n",
      "Loss: 0.036476247012615204\n",
      "Loss: 0.01857003942131996\n",
      "Loss: 0.02402472123503685\n",
      "Loss: 0.031111720949411392\n",
      "Loss: 0.024590078741312027\n",
      "Loss: 0.02967950515449047\n",
      "Loss: 0.019757762551307678\n",
      "Loss: 0.027925901114940643\n",
      "Loss: 0.029935143887996674\n",
      "Loss: 0.03357715532183647\n",
      "Loss: 0.05576954036951065\n",
      "Loss: 0.02971164509654045\n",
      "Loss: 0.03228114917874336\n",
      "Loss: 0.033321578055620193\n",
      "Loss: 0.020642679184675217\n",
      "Loss: 0.01945289969444275\n",
      "Loss: 0.028252413496375084\n",
      "Loss: 0.027965649962425232\n",
      "Loss: 0.03422102332115173\n",
      "Loss: 0.020799260586500168\n",
      "Loss: 0.02949424460530281\n",
      "Loss: 0.03045905940234661\n",
      "Loss: 0.030733663588762283\n",
      "Loss: 0.026028862223029137\n",
      "Loss: 0.04018232226371765\n",
      "Loss: 0.032073162496089935\n",
      "Loss: 0.03687789663672447\n",
      "Loss: 0.030126770958304405\n",
      "Loss: 0.037238821387290955\n",
      "Loss: 0.02615329995751381\n",
      "Loss: 0.021500539034605026\n",
      "Loss: 0.029723038896918297\n",
      "Loss: 0.02622704952955246\n",
      "Loss: 0.024034321308135986\n",
      "Loss: 0.022337745875120163\n",
      "Loss: 0.027713678777217865\n",
      "Loss: 0.02695365622639656\n",
      "Loss: 0.044855792075395584\n",
      "Loss: 0.049412474036216736\n",
      "Loss: 0.02909339964389801\n",
      "Loss: 0.030758501961827278\n",
      "Loss: 0.033322446048259735\n",
      "Loss: 0.030896173790097237\n",
      "Loss: 0.018913879990577698\n",
      "Loss: 0.022839270532131195\n",
      "Loss: 0.02017904818058014\n",
      "Loss: 0.036762628704309464\n",
      "Loss: 0.029347937554121017\n",
      "Loss: 0.022812332957983017\n",
      "Loss: 0.03989081457257271\n",
      "Loss: 0.026564110070466995\n",
      "Loss: 0.030615651980042458\n",
      "Loss: 0.038190633058547974\n",
      "Loss: 0.022987637668848038\n",
      "Loss: 0.02319628931581974\n",
      "Loss: 0.02856242097914219\n",
      "Loss: 0.03822575882077217\n",
      "Loss: 0.03335525467991829\n",
      "Loss: 0.02415481023490429\n",
      "Loss: 0.029378827661275864\n",
      "Loss: 0.02090173028409481\n",
      "Loss: 0.02490866184234619\n",
      "Loss: 0.019098909571766853\n",
      "Loss: 0.033132851123809814\n",
      "Loss: 0.03733816742897034\n",
      "Loss: 0.02020876482129097\n",
      "Loss: 0.020087510347366333\n",
      "Loss: 0.020667240023612976\n",
      "Loss: 0.017164047807455063\n",
      "Loss: 0.01987743377685547\n",
      "Loss: 0.03310570493340492\n",
      "Loss: 0.03452010452747345\n",
      "Loss: 0.048555873334407806\n",
      "Loss: 0.03226093202829361\n",
      "Loss: 0.02718322165310383\n",
      "Loss: 0.024621659889817238\n",
      "Loss: 0.025317905470728874\n",
      "Loss: 0.027860945090651512\n",
      "Loss: 0.023304665461182594\n",
      "Loss: 0.029966291040182114\n",
      "Loss: 0.04133276268839836\n",
      "Loss: 0.03177519887685776\n",
      "Loss: 0.03208669647574425\n",
      "Loss: 0.0320737324655056\n",
      "Loss: 0.02269691228866577\n",
      "Loss: 0.02292494848370552\n",
      "Loss: 0.033638354390859604\n",
      "Loss: 0.024451345205307007\n",
      "Loss: 0.030564043670892715\n",
      "Loss: 0.020372755825519562\n",
      "Loss: 0.035649944096803665\n",
      "Loss: 0.022684477269649506\n",
      "Loss: 0.031161222606897354\n",
      "Loss: 0.03525915369391441\n",
      "Loss: 0.030727073550224304\n",
      "Loss: 0.020443053916096687\n",
      "Loss: 0.027214858680963516\n",
      "Loss: 0.0235357154160738\n",
      "Loss: 0.037655312567949295\n",
      "Loss: 0.029380016028881073\n",
      "Loss: 0.027299558743834496\n",
      "Loss: 0.027539651840925217\n",
      "Loss: 0.0181686133146286\n",
      "Loss: 0.021928172558546066\n",
      "Loss: 0.02865152433514595\n",
      "Loss: 0.037588417530059814\n",
      "Loss: 0.01612880639731884\n",
      "Loss: 0.03593994677066803\n",
      "Loss: 0.018979627639055252\n",
      "Loss: 0.021568013355135918\n",
      "Loss: 0.032888077199459076\n",
      "Loss: 0.024081364274024963\n",
      "Loss: 0.01818981021642685\n",
      "Loss: 0.03186822310090065\n",
      "Loss: 0.012852968648076057\n",
      "Loss: 0.022482188418507576\n",
      "Loss: 0.01568525843322277\n",
      "Loss: 0.028425943106412888\n",
      "Loss: 0.04330045357346535\n",
      "Loss: 0.044521089643239975\n",
      "Loss: 0.013145891949534416\n",
      "Loss: 0.02446340024471283\n",
      "Loss: 0.023406900465488434\n",
      "Loss: 0.03214922547340393\n",
      "Loss: 0.0233454629778862\n",
      "Loss: 0.028348226100206375\n",
      "Loss: 0.023872144520282745\n",
      "Loss: 0.027578964829444885\n",
      "Loss: 0.01834462583065033\n",
      "Loss: 0.02247498370707035\n",
      "Loss: 0.03254203125834465\n",
      "Loss: 0.020999278873205185\n",
      "Loss: 0.026295380666851997\n",
      "Loss: 0.03078632615506649\n",
      "Loss: 0.02658819779753685\n",
      "Loss: 0.04517854005098343\n",
      "Loss: 0.0217273011803627\n",
      "Loss: 0.016077641397714615\n",
      "Loss: 0.024709653109312057\n",
      "Loss: 0.016750887036323547\n",
      "Loss: 0.02895376831293106\n",
      "Loss: 0.029280569404363632\n",
      "Loss: 0.028670910745859146\n",
      "Loss: 0.02467786706984043\n",
      "Loss: 0.020506691187620163\n",
      "Loss: 0.06940732151269913\n",
      "Loss: 0.0348198227584362\n",
      "Loss: 0.014030037447810173\n",
      "Loss: 0.023984001949429512\n",
      "Loss: 0.02208414115011692\n",
      "Loss: 0.02071402221918106\n",
      "Loss: 0.02342146635055542\n",
      "Loss: 0.02152516134083271\n",
      "Loss: 0.03790587559342384\n",
      "Loss: 0.028695058077573776\n",
      "Loss: 0.016014177352190018\n",
      "Loss: 0.042749956250190735\n",
      "Loss: 0.021326027810573578\n",
      "Loss: 0.02115572616457939\n",
      "Loss: 0.02614375576376915\n",
      "Loss: 0.028490563854575157\n",
      "Loss: 0.019831256940960884\n",
      "Loss: 0.0336211659014225\n",
      "Loss: 0.022249815985560417\n",
      "Loss: 0.02737056091427803\n",
      "Loss: 0.027479004114866257\n",
      "Loss: 0.026551594957709312\n",
      "Loss: 0.018096566200256348\n",
      "Loss: 0.02512478455901146\n",
      "Loss: 0.016660509631037712\n",
      "Loss: 0.02812005579471588\n",
      "Loss: 0.018612194806337357\n",
      "Loss: 0.026873311027884483\n",
      "Loss: 0.04976598918437958\n",
      "Loss: 0.03364939987659454\n",
      "Loss: 0.037808746099472046\n",
      "Loss: 0.026005864143371582\n",
      "Loss: 0.0323648601770401\n",
      "Loss: 0.01764836348593235\n",
      "Loss: 0.0188523568212986\n",
      "Loss: 0.02451363205909729\n",
      "Loss: 0.02897132933139801\n",
      "Loss: 0.03279365226626396\n",
      "Loss: 0.03138698264956474\n",
      "Loss: 0.02573346346616745\n",
      "Loss: 0.03639030084013939\n",
      "Loss: 0.024442408233880997\n",
      "Loss: 0.021627463400363922\n",
      "Loss: 0.02004951238632202\n",
      "Loss: 0.022028887644410133\n",
      "Loss: 0.02516341395676136\n",
      "Loss: 0.035598188638687134\n",
      "Loss: 0.025452740490436554\n",
      "Loss: 0.027963068336248398\n",
      "Loss: 0.030340667814016342\n",
      "Loss: 0.030995376408100128\n",
      "Loss: 0.03796275705099106\n",
      "Loss: 0.03256417438387871\n",
      "Loss: 0.02360568195581436\n",
      "Loss: 0.028184613212943077\n",
      "Loss: 0.021438827738165855\n",
      "Loss: 0.03052225336432457\n",
      "Loss: 0.030344663187861443\n",
      "Loss: 0.0358085036277771\n",
      "Loss: 0.046883225440979004\n",
      "Loss: 0.022106651216745377\n",
      "Loss: 0.02551032416522503\n",
      "Loss: 0.02613392286002636\n",
      "Loss: 0.019356204196810722\n",
      "Loss: 0.03104529343545437\n",
      "Loss: 0.023437131196260452\n",
      "Loss: 0.02794610522687435\n",
      "Loss: 0.05048453062772751\n",
      "Loss: 0.030663639307022095\n",
      "Loss: 0.033106908202171326\n",
      "Loss: 0.019358860328793526\n",
      "Loss: 0.03146845102310181\n",
      "Loss: 0.024866174906492233\n",
      "Loss: 0.029030250385403633\n",
      "Loss: 0.02633710950613022\n",
      "Loss: 0.029417358338832855\n",
      "Loss: 0.025306684896349907\n",
      "Loss: 0.027269825339317322\n",
      "Loss: 0.021381694823503494\n",
      "Loss: 0.025507086887955666\n",
      "Loss: 0.026643645018339157\n",
      "Loss: 0.03411855548620224\n",
      "Loss: 0.022808516398072243\n",
      "Loss: 0.027334993705153465\n",
      "Loss: 0.025159813463687897\n",
      "Loss: 0.028853897005319595\n",
      "Loss: 0.024329399690032005\n",
      "Loss: 0.047363340854644775\n",
      "Loss: 0.018163710832595825\n",
      "Loss: 0.020348798483610153\n",
      "Loss: 0.02146701142191887\n",
      "Loss: 0.032036155462265015\n",
      "Loss: 0.03101387992501259\n",
      "Loss: 0.030671339482069016\n",
      "Loss: 0.03151167556643486\n",
      "Loss: 0.025260917842388153\n",
      "Loss: 0.02518869936466217\n",
      "Loss: 0.02678009867668152\n",
      "Loss: 0.02334214746952057\n",
      "Loss: 0.03688973933458328\n",
      "Loss: 0.038443710654973984\n",
      "Loss: 0.030428482219576836\n",
      "Loss: 0.028444157913327217\n",
      "Loss: 0.02320830151438713\n",
      "Loss: 0.037977710366249084\n",
      "Loss: 0.02466428652405739\n",
      "Loss: 0.022844232618808746\n",
      "Loss: 0.029807541519403458\n",
      "Loss: 0.02803851291537285\n",
      "Loss: 0.02461293712258339\n",
      "Loss: 0.03329857066273689\n",
      "Loss: 0.019307833164930344\n",
      "Loss: 0.019704438745975494\n",
      "Loss: 0.051818981766700745\n",
      "Loss: 0.041158050298690796\n",
      "Loss: 0.039641086012125015\n",
      "Loss: 0.02627377025783062\n",
      "Loss: 0.03188818320631981\n",
      "Loss: 0.028321247547864914\n",
      "Loss: 0.028944576159119606\n",
      "Loss: 0.03895718604326248\n",
      "Loss: 0.03217040002346039\n",
      "Loss: 0.03315659239888191\n",
      "Loss: 0.03539473935961723\n",
      "Loss: 0.019139040261507034\n",
      "Loss: 0.025714147835969925\n",
      "Loss: 0.027112537994980812\n",
      "Loss: 0.015054034069180489\n",
      "Loss: 0.02636142447590828\n",
      "Loss: 0.03590942174196243\n",
      "Loss: 0.026376474648714066\n",
      "Loss: 0.036746539175510406\n",
      "Loss: 0.04062134400010109\n",
      "Loss: 0.02700471505522728\n",
      "Loss: 0.035090550780296326\n",
      "Loss: 0.0315035879611969\n",
      "Loss: 0.051560863852500916\n",
      "Loss: 0.023925598710775375\n",
      "Loss: 0.026852069422602654\n",
      "Loss: 0.029593851417303085\n",
      "Loss: 0.027130618691444397\n",
      "Loss: 0.02868635393679142\n",
      "Loss: 0.03960390016436577\n",
      "Loss: 0.026407407596707344\n",
      "Loss: 0.02610604092478752\n",
      "Loss: 0.026436101645231247\n",
      "Loss: 0.03456367552280426\n",
      "Loss: 0.02519063465297222\n",
      "Loss: 0.026678066700696945\n",
      "Loss: 0.022334713488817215\n",
      "Loss: 0.036482807248830795\n",
      "Loss: 0.022073624655604362\n",
      "Loss: 0.022479020059108734\n",
      "Loss: 0.041241493076086044\n",
      "Loss: 0.039540521800518036\n",
      "Loss: 0.0354769304394722\n",
      "Loss: 0.02159130573272705\n",
      "Loss: 0.021487649530172348\n",
      "Loss: 0.0327567532658577\n",
      "Loss: 0.027532806620001793\n",
      "Loss: 0.045067332684993744\n",
      "Loss: 0.024719515815377235\n",
      "Loss: 0.022266345098614693\n",
      "Loss: 0.030754979699850082\n",
      "Loss: 0.02407732792198658\n",
      "Loss: 0.03301367536187172\n",
      "Loss: 0.030023032799363136\n",
      "Loss: 0.014907866716384888\n",
      "Loss: 0.03254896029829979\n",
      "Loss: 0.02869468927383423\n",
      "Loss: 0.031478092074394226\n",
      "Loss: 0.01608564890921116\n",
      "Loss: 0.02982676960527897\n",
      "Loss: 0.021640341728925705\n",
      "Loss: 0.03161122649908066\n",
      "Loss: 0.035809699445962906\n",
      "Loss: 0.02775755524635315\n",
      "Loss: 0.022970637306571007\n",
      "Loss: 0.040765631943941116\n",
      "Loss: 0.03005867823958397\n",
      "Loss: 0.020359031856060028\n",
      "Loss: 0.022400129586458206\n",
      "Loss: 0.025237375870347023\n",
      "Loss: 0.03439715877175331\n",
      "Loss: 0.024580951780080795\n",
      "Loss: 0.026930678635835648\n",
      "Loss: 0.02058071829378605\n",
      "Loss: 0.038427118211984634\n",
      "Loss: 0.029515253379940987\n",
      "Loss: 0.015683745965361595\n",
      "Loss: 0.02352401427924633\n",
      "Loss: 0.030773727223277092\n",
      "Loss: 0.02027938701212406\n",
      "Loss: 0.028370719403028488\n",
      "Loss: 0.03404007479548454\n",
      "Loss: 0.025935295969247818\n",
      "Loss: 0.03178931027650833\n",
      "Loss: 0.0330541729927063\n",
      "Loss: 0.021905794739723206\n",
      "Loss: 0.027770331129431725\n",
      "Loss: 0.032576099038124084\n",
      "Loss: 0.023706182837486267\n",
      "Loss: 0.03141157329082489\n",
      "Loss: 0.02393537573516369\n",
      "Loss: 0.03007468208670616\n",
      "Loss: 0.019210422411561012\n",
      "Loss: 0.021825918927788734\n",
      "Loss: 0.03209598734974861\n",
      "Loss: 0.022494489327073097\n",
      "Loss: 0.023989783599972725\n",
      "Loss: 0.03954223543405533\n",
      "Loss: 0.03820740431547165\n",
      "Loss: 0.02385728992521763\n",
      "Loss: 0.023002566769719124\n",
      "Loss: 0.030770303681492805\n",
      "Loss: 0.03677358478307724\n",
      "Loss: 0.031532421708106995\n",
      "Loss: 0.030540086328983307\n",
      "Loss: 0.02779487892985344\n",
      "Loss: 0.03212498500943184\n",
      "Loss: 0.033028990030288696\n",
      "Loss: 0.03603885695338249\n",
      "Loss: 0.02583199180662632\n",
      "Loss: 0.024283921346068382\n",
      "Loss: 0.03452864661812782\n",
      "Loss: 0.023278359323740005\n",
      "Loss: 0.022931694984436035\n",
      "Loss: 0.021448280662298203\n",
      "Loss: 0.016679616644978523\n",
      "Loss: 0.04486793652176857\n",
      "Loss: 0.02852936089038849\n",
      "Loss: 0.03203154355287552\n",
      "Loss: 0.016114315018057823\n",
      "Loss: 0.04064394533634186\n",
      "Loss: 0.037156715989112854\n",
      "Loss: 0.022561021149158478\n",
      "Loss: 0.035423606634140015\n",
      "Loss: 0.02890552580356598\n",
      "Loss: 0.02708939090371132\n",
      "Loss: 0.03346554562449455\n",
      "Loss: 0.027026189491152763\n",
      "Loss: 0.027785660699009895\n",
      "Loss: 0.024083763360977173\n",
      "Loss: 0.029628563672304153\n",
      "Loss: 0.020757129415869713\n",
      "Loss: 0.0394337922334671\n",
      "Loss: 0.02394277974963188\n",
      "Loss: 0.02949564717710018\n",
      "Loss: 0.030369596555829048\n",
      "Loss: 0.02554316446185112\n",
      "Loss: 0.019169585779309273\n",
      "Loss: 0.028115278109908104\n",
      "Loss: 0.019208790734410286\n",
      "Loss: 0.03070751763880253\n",
      "Loss: 0.04865366220474243\n",
      "Loss: 0.03943365812301636\n",
      "Loss: 0.02961074374616146\n",
      "Loss: 0.021466458216309547\n",
      "Loss: 0.037205494940280914\n",
      "Loss: 0.021548807621002197\n",
      "Loss: 0.0229373499751091\n",
      "Loss: 0.03596552088856697\n",
      "Loss: 0.026233043521642685\n",
      "Loss: 0.02526719868183136\n",
      "Loss: 0.020533306524157524\n",
      "Loss: 0.032862916588783264\n",
      "Loss: 0.03735596314072609\n",
      "Loss: 0.02696552313864231\n",
      "Loss: 0.02219877764582634\n",
      "Loss: 0.01550182793289423\n",
      "Loss: 0.03135260194540024\n",
      "Loss: 0.022739998996257782\n",
      "Loss: 0.020498670637607574\n",
      "Loss: 0.030311426147818565\n",
      "Loss: 0.017060214653611183\n",
      "Loss: 0.03297248110175133\n",
      "Loss: 0.035629257559776306\n",
      "Loss: 0.018401825800538063\n",
      "Loss: 0.02239074371755123\n",
      "Loss: 0.03139892965555191\n",
      "Loss: 0.01838068850338459\n",
      "Loss: 0.020250357687473297\n",
      "Loss: 0.01927344873547554\n",
      "Loss: 0.026257500052452087\n",
      "Loss: 0.026969503611326218\n",
      "Loss: 0.028020963072776794\n",
      "Loss: 0.01651374064385891\n",
      "Loss: 0.02783539518713951\n",
      "Loss: 0.0364956259727478\n",
      "Loss: 0.04830554872751236\n",
      "Loss: 0.024782726541161537\n",
      "Loss: 0.029058627784252167\n",
      "Loss: 0.028509557247161865\n",
      "Loss: 0.040553413331508636\n",
      "Loss: 0.04022176191210747\n",
      "Loss: 0.027677636593580246\n",
      "Loss: 0.03306983411312103\n",
      "Loss: 0.024820320308208466\n",
      "Loss: 0.02786063216626644\n",
      "Loss: 0.025508560240268707\n",
      "Loss: 0.03365812823176384\n",
      "Loss: 0.018354851752519608\n",
      "Loss: 0.030493101105093956\n",
      "Loss: 0.02496412768959999\n",
      "Loss: 0.01589619368314743\n",
      "Loss: 0.04870035499334335\n",
      "Loss: 0.017774365842342377\n",
      "Loss: 0.07037307322025299\n",
      "Loss: 0.026599658653140068\n",
      "Loss: 0.023392328992486\n",
      "Loss: 0.028529252856969833\n",
      "Loss: 0.029641306027770042\n",
      "Loss: 0.02324894443154335\n",
      "Loss: 0.03192060813307762\n",
      "Loss: 0.027969444170594215\n",
      "Loss: 0.019412338733673096\n",
      "Loss: 0.03142610192298889\n",
      "Loss: 0.01831546239554882\n",
      "Loss: 0.02128015272319317\n",
      "Loss: 0.020799238234758377\n",
      "Loss: 0.04143788293004036\n",
      "Loss: 0.013909464702010155\n",
      "Loss: 0.028464779257774353\n",
      "Loss: 0.0270992498844862\n",
      "Loss: 0.028323190286755562\n",
      "Loss: 0.012642806395888329\n",
      "Loss: 0.027384070679545403\n",
      "Loss: 0.027939409017562866\n",
      "Loss: 0.0298032034188509\n",
      "Loss: 0.03026866912841797\n",
      "Loss: 0.04066270962357521\n",
      "Loss: 0.028144218027591705\n",
      "Loss: 0.02445482276380062\n",
      "Loss: 0.03064076416194439\n",
      "Loss: 0.022265933454036713\n",
      "Loss: 0.033989571034908295\n",
      "Loss: 0.03063492476940155\n",
      "Loss: 0.03477964550256729\n",
      "Loss: 0.02288050204515457\n",
      "Loss: 0.02362109161913395\n",
      "Loss: 0.021150413900613785\n",
      "Loss: 0.02777320332825184\n",
      "Loss: 0.028375335037708282\n",
      "Loss: 0.027599694207310677\n",
      "Loss: 0.041872307658195496\n",
      "Loss: 0.03354205563664436\n",
      "Loss: 0.03428179770708084\n",
      "Loss: 0.04057154804468155\n",
      "Loss: 0.040102675557136536\n",
      "Loss: 0.031241772696375847\n",
      "Loss: 0.025646332651376724\n",
      "Loss: 0.026268580928444862\n",
      "Loss: 0.029966948553919792\n",
      "Loss: 0.034969303756952286\n",
      "Loss: 0.024991577491164207\n",
      "Loss: 0.02028428018093109\n",
      "Loss: 0.02198513224720955\n",
      "Loss: 0.02369704283773899\n",
      "Loss: 0.026982218027114868\n",
      "Loss: 0.020164484158158302\n",
      "Loss: 0.029009941965341568\n",
      "Loss: 0.0323789156973362\n",
      "Loss: 0.03161339834332466\n",
      "Loss: 0.02096557803452015\n",
      "Loss: 0.01738675870001316\n",
      "Loss: 0.018920060247182846\n",
      "Loss: 0.020584698766469955\n",
      "Loss: 0.03272748738527298\n",
      "Loss: 0.02552998811006546\n",
      "Loss: 0.026256171986460686\n",
      "Loss: 0.041566528379917145\n",
      "Loss: 0.04116849973797798\n",
      "Loss: 0.03433253616094589\n",
      "Loss: 0.022572536021471024\n",
      "Loss: 0.02538025565445423\n",
      "Loss: 0.017112402245402336\n",
      "Loss: 0.03225092217326164\n",
      "Loss: 0.033415522426366806\n",
      "Loss: 0.027710817754268646\n",
      "Loss: 0.02378835715353489\n",
      "Loss: 0.029463952407240868\n",
      "Loss: 0.01834416575729847\n",
      "Loss: 0.030029643326997757\n",
      "Loss: 0.021729396656155586\n",
      "Loss: 0.020552203059196472\n",
      "Loss: 0.027470573782920837\n",
      "Loss: 0.02681135945022106\n",
      "Loss: 0.02351677231490612\n",
      "Loss: 0.01859940215945244\n",
      "Loss: 0.01663128472864628\n",
      "Loss: 0.030356111004948616\n",
      "Loss: 0.0197419673204422\n",
      "Loss: 0.04250849038362503\n",
      "Loss: 0.022407257929444313\n",
      "Loss: 0.0217281524091959\n",
      "Loss: 0.03003743104636669\n",
      "Loss: 0.025789929553866386\n",
      "Loss: 0.014461712911725044\n",
      "Loss: 0.0343630313873291\n",
      "Loss: 0.025888776406645775\n",
      "Loss: 0.043943874537944794\n",
      "Loss: 0.016695760190486908\n",
      "Loss: 0.01781349815428257\n",
      "Loss: 0.029794912785291672\n",
      "Loss: 0.017407525330781937\n",
      "Loss: 0.031440578401088715\n",
      "Loss: 0.041517890989780426\n",
      "Loss: 0.03174348920583725\n",
      "Loss: 0.02163579687476158\n",
      "Loss: 0.034744732081890106\n",
      "Loss: 0.027915196493268013\n",
      "Loss: 0.028179701417684555\n",
      "Loss: 0.036369599401950836\n",
      "Loss: 0.02870052307844162\n",
      "Loss: 0.021996306255459785\n",
      "Loss: 0.028567368164658546\n",
      "Loss: 0.022514766082167625\n",
      "Loss: 0.02401335909962654\n",
      "Loss: 0.04473331198096275\n",
      "Loss: 0.021174777299165726\n",
      "Loss: 0.019118329510092735\n",
      "Loss: 0.0164344422519207\n",
      "Loss: 0.0378047451376915\n",
      "Loss: 0.029792791232466698\n",
      "Loss: 0.01849137246608734\n",
      "Loss: 0.043239228427410126\n",
      "Loss: 0.031272295862436295\n",
      "Loss: 0.02350733056664467\n",
      "Loss: 0.02809247188270092\n",
      "Loss: 0.02728860266506672\n",
      "Loss: 0.02274370566010475\n",
      "Loss: 0.025533469393849373\n",
      "Loss: 0.03707899898290634\n",
      "Loss: 0.019237717613577843\n",
      "Loss: 0.03322133421897888\n",
      "Loss: 0.027017731219530106\n",
      "Loss: 0.020255815237760544\n",
      "Loss: 0.02342834882438183\n",
      "Loss: 0.024630142375826836\n",
      "Loss: 0.01897464320063591\n",
      "Loss: 0.03314712643623352\n",
      "Loss: 0.031175963580608368\n",
      "Loss: 0.025827396661043167\n",
      "Loss: 0.03588073328137398\n",
      "Loss: 0.018972713500261307\n",
      "Loss: 0.01679506152868271\n",
      "Loss: 0.046726327389478683\n",
      "Loss: 0.020320622250437737\n",
      "Loss: 0.02207210473716259\n",
      "Loss: 0.030555589124560356\n",
      "Loss: 0.020019903779029846\n",
      "Loss: 0.04035089537501335\n",
      "Loss: 0.019687755033373833\n",
      "Loss: 0.023571446537971497\n",
      "Loss: 0.028884558007121086\n",
      "Loss: 0.04727073013782501\n",
      "Loss: 0.03035815805196762\n",
      "Loss: 0.030184166505932808\n",
      "Loss: 0.028091318905353546\n",
      "Loss: 0.020156079903244972\n",
      "Loss: 0.035951875150203705\n",
      "Loss: 0.03080848790705204\n",
      "Loss: 0.021643860265612602\n",
      "Loss: 0.0231684111058712\n",
      "Loss: 0.018901240080595016\n",
      "Loss: 0.034578874707221985\n",
      "Loss: 0.02147255465388298\n",
      "Loss: 0.03397774323821068\n",
      "Loss: 0.04368101432919502\n",
      "Loss: 0.02428468130528927\n",
      "Loss: 0.03251596540212631\n",
      "Loss: 0.025848153978586197\n",
      "Loss: 0.024055905640125275\n",
      "Loss: 0.026093825697898865\n",
      "Loss: 0.022864453494548798\n",
      "Loss: 0.029323222115635872\n",
      "Loss: 0.02057303674519062\n",
      "Loss: 0.02655411697924137\n",
      "Loss: 0.022196602076292038\n",
      "Loss: 0.026514938101172447\n",
      "Loss: 0.04667694866657257\n",
      "Loss: 0.020023798570036888\n",
      "Loss: 0.03356894850730896\n",
      "Loss: 0.02866169437766075\n",
      "Loss: 0.02405339851975441\n",
      "Loss: 0.029278237372636795\n",
      "Loss: 0.01592077501118183\n",
      "Loss: 0.01514664851129055\n",
      "Loss: 0.01506069116294384\n",
      "Loss: 0.017781663686037064\n",
      "Loss: 0.03076491504907608\n",
      "Loss: 0.02918749488890171\n",
      "Loss: 0.03056824952363968\n",
      "Loss: 0.03755256161093712\n",
      "Loss: 0.0291738323867321\n",
      "Loss: 0.01757854036986828\n",
      "Loss: 0.024839047342538834\n",
      "Loss: 0.02096846140921116\n",
      "Loss: 0.015242631547152996\n",
      "Loss: 0.04675203561782837\n",
      "Loss: 0.033174723386764526\n",
      "Loss: 0.015864918008446693\n",
      "Loss: 0.026101931929588318\n",
      "Loss: 0.027056964114308357\n",
      "Loss: 0.01829955168068409\n",
      "Loss: 0.04065479710698128\n",
      "Loss: 0.02688358537852764\n",
      "Loss: 0.03833822160959244\n",
      "Loss: 0.02089054137468338\n",
      "Loss: 0.02752639539539814\n",
      "Loss: 0.030414465814828873\n",
      "Loss: 0.04222363606095314\n",
      "Loss: 0.02638605237007141\n",
      "Loss: 0.028171755373477936\n",
      "Loss: 0.020798981189727783\n",
      "Loss: 0.02494993433356285\n",
      "Loss: 0.024272410199046135\n",
      "Loss: 0.01962645724415779\n",
      "Loss: 0.025557201355695724\n",
      "Loss: 0.028923295438289642\n",
      "Loss: 0.02926909178495407\n",
      "Loss: 0.027612289413809776\n",
      "Loss: 0.025807341560721397\n",
      "Loss: 0.013021467253565788\n",
      "Loss: 0.028789972886443138\n",
      "Loss: 0.02987312152981758\n",
      "Loss: 0.03553769737482071\n",
      "Loss: 0.027955936267971992\n",
      "Loss: 0.018073108047246933\n",
      "Loss: 0.025482304394245148\n",
      "Loss: 0.02509211376309395\n",
      "Loss: 0.02835015580058098\n",
      "Loss: 0.041023749858140945\n",
      "Loss: 0.034402910619974136\n",
      "Loss: 0.04233379662036896\n",
      "Loss: 0.022439930588006973\n",
      "Loss: 0.030649807304143906\n",
      "Loss: 0.035955239087343216\n",
      "Loss: 0.030006317421793938\n",
      "Loss: 0.02975497581064701\n",
      "Loss: 0.02717479132115841\n",
      "Loss: 0.024747192859649658\n",
      "Loss: 0.03967227786779404\n",
      "Loss: 0.026078203693032265\n",
      "Loss: 0.022054247558116913\n",
      "Loss: 0.034638501703739166\n",
      "Loss: 0.03185289353132248\n",
      "Loss: 0.022997966036200523\n",
      "Loss: 0.02139037474989891\n",
      "Loss: 0.023418983444571495\n",
      "Loss: 0.026430433616042137\n",
      "Loss: 0.01879705674946308\n",
      "Loss: 0.03290265426039696\n",
      "Loss: 0.050375763326883316\n",
      "Loss: 0.03792791813611984\n",
      "Loss: 0.030745066702365875\n",
      "Loss: 0.023612242192029953\n",
      "Loss: 0.029603026807308197\n",
      "Loss: 0.022397607564926147\n",
      "Loss: 0.029696237295866013\n",
      "Loss: 0.033727001398801804\n",
      "Loss: 0.03677833080291748\n",
      "Loss: 0.025313954800367355\n",
      "Loss: 0.03204084560275078\n",
      "Loss: 0.040937501937150955\n",
      "Loss: 0.029467163607478142\n",
      "Loss: 0.022235823795199394\n",
      "Loss: 0.02423020638525486\n",
      "Loss: 0.029478419572114944\n",
      "Loss: 0.028104856610298157\n",
      "Loss: 0.027335550636053085\n",
      "Loss: 0.02461721934378147\n",
      "Loss: 0.04135282710194588\n",
      "Loss: 0.026327360421419144\n",
      "Loss: 0.033004242926836014\n",
      "Loss: 0.022503428161144257\n",
      "Loss: 0.01845427230000496\n",
      "Loss: 0.023097509518265724\n",
      "Loss: 0.027765359729528427\n",
      "Loss: 0.03481695055961609\n",
      "Loss: 0.027479877695441246\n",
      "Loss: 0.03871328383684158\n",
      "Loss: 0.01543046161532402\n",
      "Loss: 0.021625777706503868\n",
      "Loss: 0.01969270594418049\n",
      "Loss: 0.03483963385224342\n",
      "Loss: 0.021143311634659767\n",
      "Loss: 0.018772264942526817\n",
      "Loss: 0.020415786653757095\n",
      "Loss: 0.018967142328619957\n",
      "Loss: 0.022414395585656166\n",
      "Loss: 0.03931977599859238\n",
      "Loss: 0.021140240132808685\n",
      "Loss: 0.04099659249186516\n",
      "Loss: 0.013680791482329369\n",
      "Loss: 0.017971018329262733\n",
      "Loss: 0.04722937196493149\n",
      "Loss: 0.021878143772482872\n",
      "Loss: 0.032063379883766174\n",
      "Loss: 0.027724919840693474\n",
      "Loss: 0.03578249737620354\n",
      "Loss: 0.020905518904328346\n",
      "Loss: 0.025220774114131927\n",
      "Loss: 0.024019071832299232\n",
      "Loss: 0.03940402343869209\n",
      "Loss: 0.021093688905239105\n",
      "Loss: 0.03600051999092102\n",
      "Loss: 0.027270155027508736\n",
      "Loss: 0.04190833866596222\n",
      "Loss: 0.027301495894789696\n",
      "Loss: 0.025732167065143585\n",
      "Loss: 0.02623603865504265\n",
      "Loss: 0.03302806615829468\n",
      "Loss: 0.01899567060172558\n",
      "Loss: 0.04129834845662117\n",
      "Loss: 0.020015470683574677\n",
      "Loss: 0.03786107152700424\n",
      "Loss: 0.029963864013552666\n",
      "Loss: 0.027097627520561218\n",
      "Loss: 0.0355151891708374\n",
      "Loss: 0.03417089581489563\n",
      "Loss: 0.03182659298181534\n",
      "Loss: 0.026602759957313538\n",
      "Loss: 0.02500985562801361\n",
      "Loss: 0.025995366275310516\n",
      "Loss: 0.031195931136608124\n",
      "Loss: 0.03363325446844101\n",
      "Loss: 0.019738735631108284\n",
      "Loss: 0.022444674745202065\n",
      "Loss: 0.03157765045762062\n",
      "Loss: 0.020800147205591202\n",
      "Loss: 0.023317674174904823\n",
      "Loss: 0.032553017139434814\n",
      "Loss: 0.020767342299222946\n",
      "Loss: 0.018122311681509018\n",
      "Loss: 0.02811645343899727\n",
      "Loss: 0.04456757754087448\n",
      "Loss: 0.026206672191619873\n",
      "Loss: 0.020431678742170334\n",
      "Loss: 0.023330729454755783\n",
      "Loss: 0.030268166214227676\n",
      "Loss: 0.03970088064670563\n",
      "Loss: 0.026971986517310143\n",
      "Loss: 0.0326634906232357\n",
      "Loss: 0.027070503681898117\n",
      "Loss: 0.029326941817998886\n",
      "Loss: 0.030423440039157867\n",
      "Loss: 0.021293126046657562\n",
      "Loss: 0.02910580486059189\n",
      "Loss: 0.04157774895429611\n",
      "Loss: 0.029309162870049477\n",
      "Loss: 0.02273661270737648\n",
      "Loss: 0.028050409629940987\n",
      "Loss: 0.04043049365282059\n",
      "Loss: 0.0325043648481369\n",
      "Loss: 0.029484029859304428\n",
      "Loss: 0.02534675784409046\n",
      "Loss: 0.022616466507315636\n",
      "Loss: 0.02723262459039688\n",
      "Loss: 0.02947939559817314\n",
      "Loss: 0.020854555070400238\n",
      "Loss: 0.03304903209209442\n",
      "Loss: 0.026572246104478836\n",
      "Loss: 0.022089961916208267\n",
      "Loss: 0.026989435777068138\n",
      "Loss: 0.036123957484960556\n",
      "Loss: 0.038982659578323364\n",
      "Loss: 0.05021126568317413\n",
      "Loss: 0.028061924502253532\n",
      "Loss: 0.018817374482750893\n",
      "Loss: 0.03427773714065552\n",
      "Loss: 0.039687711745500565\n",
      "Loss: 0.03161377087235451\n",
      "Loss: 0.02648182213306427\n",
      "Loss: 0.029605232179164886\n",
      "Loss: 0.031843751668930054\n",
      "Loss: 0.02490971051156521\n",
      "Loss: 0.028900621458888054\n",
      "Loss: 0.029293349012732506\n",
      "Loss: 0.02693384699523449\n",
      "Loss: 0.01894908957183361\n",
      "Loss: 0.023867320269346237\n",
      "Loss: 0.02431475929915905\n",
      "Loss: 0.036116860806941986\n",
      "Loss: 0.017633290961384773\n",
      "Loss: 0.023715609684586525\n",
      "Loss: 0.02502608858048916\n",
      "Loss: 0.029877441003918648\n",
      "Loss: 0.01446619164198637\n",
      "Loss: 0.03223613649606705\n",
      "Loss: 0.014556434005498886\n",
      "Loss: 0.015053307637572289\n",
      "Loss: 0.03593749180436134\n",
      "Loss: 0.060015205293893814\n",
      "Loss: 0.021869704127311707\n",
      "Loss: 0.027008917182683945\n",
      "Loss: 0.0176810584962368\n",
      "Loss: 0.03265520930290222\n",
      "Loss: 0.031866587698459625\n",
      "Loss: 0.018847569823265076\n",
      "Loss: 0.0270109623670578\n",
      "Loss: 0.021640684455633163\n",
      "Loss: 0.020884372293949127\n",
      "Loss: 0.030822210013866425\n",
      "Loss: 0.02147240936756134\n",
      "Loss: 0.02464260160923004\n",
      "Loss: 0.02802307903766632\n",
      "Loss: 0.0218686293810606\n",
      "Loss: 0.03445613011717796\n",
      "Loss: 0.03018658049404621\n",
      "Loss: 0.02245178259909153\n",
      "Loss: 0.02560528740286827\n",
      "Loss: 0.020654136314988136\n",
      "Loss: 0.020653383806347847\n",
      "Loss: 0.017294203862547874\n",
      "Loss: 0.025144506245851517\n",
      "Loss: 0.01656249165534973\n",
      "Loss: 0.030602842569351196\n",
      "Loss: 0.04352089390158653\n",
      "Loss: 0.021392785012722015\n",
      "Loss: 0.032646093517541885\n",
      "Loss: 0.03185884654521942\n",
      "Loss: 0.028934966772794724\n",
      "Loss: 0.028121553361415863\n",
      "Loss: 0.019774381071329117\n",
      "Loss: 0.02370884083211422\n",
      "Loss: 0.024967558681964874\n",
      "Loss: 0.03823322802782059\n",
      "Loss: 0.03221224620938301\n",
      "Loss: 0.025441229343414307\n",
      "Loss: 0.0285037849098444\n",
      "Loss: 0.027861054986715317\n",
      "Loss: 0.03700166195631027\n",
      "Loss: 0.02344394661486149\n",
      "Loss: 0.036330871284008026\n",
      "Loss: 0.0316484272480011\n",
      "Loss: 0.020755231380462646\n",
      "Loss: 0.03797536343336105\n",
      "Loss: 0.03392154723405838\n",
      "Loss: 0.034511685371398926\n",
      "Loss: 0.022496767342090607\n",
      "Loss: 0.019581930711865425\n",
      "Loss: 0.01907571405172348\n",
      "Loss: 0.02780422754585743\n",
      "Loss: 0.027533911168575287\n",
      "Loss: 0.039584189653396606\n",
      "Loss: 0.023983636870980263\n",
      "Loss: 0.02161014825105667\n",
      "Loss: 0.043610524386167526\n",
      "Loss: 0.020768065005540848\n",
      "Loss: 0.0506809800863266\n",
      "Loss: 0.024543331936001778\n",
      "Loss: 0.02608651854097843\n",
      "Loss: 0.02675793506205082\n",
      "Loss: 0.03059658221900463\n",
      "Loss: 0.023473665118217468\n",
      "Loss: 0.031418509781360626\n",
      "Loss: 0.02014303393661976\n",
      "Loss: 0.025911420583724976\n",
      "Loss: 0.028683025389909744\n",
      "Loss: 0.026599455624818802\n",
      "Loss: 0.030934341251850128\n",
      "Loss: 0.023607682436704636\n",
      "Loss: 0.02331688068807125\n",
      "Loss: 0.03608651086688042\n",
      "Loss: 0.034282051026821136\n",
      "Loss: 0.035634759813547134\n",
      "Loss: 0.036935944110155106\n",
      "Loss: 0.02639131247997284\n",
      "Loss: 0.0359051413834095\n",
      "Loss: 0.027501149103045464\n",
      "Loss: 0.03449471294879913\n",
      "Loss: 0.028464986011385918\n",
      "Loss: 0.030376005917787552\n",
      "Loss: 0.029653208330273628\n",
      "Loss: 0.028384536504745483\n",
      "Loss: 0.027329150587320328\n",
      "Loss: 0.023606684058904648\n",
      "Loss: 0.027221471071243286\n",
      "Loss: 0.03182879835367203\n",
      "Loss: 0.03110160119831562\n",
      "Loss: 0.02799370139837265\n",
      "Loss: 0.016284730285406113\n",
      "Loss: 0.01949015073478222\n",
      "Loss: 0.038236603140830994\n",
      "Loss: 0.032028067857027054\n",
      "Loss: 0.02462868206202984\n",
      "Loss: 0.02588030695915222\n",
      "Loss: 0.02574126422405243\n",
      "Loss: 0.02302403934299946\n",
      "Loss: 0.04469476640224457\n",
      "Loss: 0.025146299973130226\n",
      "Loss: 0.01947597786784172\n",
      "Loss: 0.029898278415203094\n",
      "Loss: 0.034304533153772354\n",
      "Loss: 0.022816848009824753\n",
      "Loss: 0.025566911324858665\n",
      "Loss: 0.027687231078743935\n",
      "Loss: 0.02863769792020321\n",
      "Loss: 0.01570972241461277\n",
      "Loss: 0.016840502619743347\n",
      "Loss: 0.027539433911442757\n",
      "Loss: 0.03343482315540314\n",
      "Loss: 0.03773003816604614\n",
      "Loss: 0.021504752337932587\n",
      "Loss: 0.021180180832743645\n",
      "Loss: 0.02082075923681259\n",
      "Loss: 0.020176146179437637\n",
      "Loss: 0.020171621814370155\n",
      "Loss: 0.028016582131385803\n",
      "Loss: 0.03248516470193863\n",
      "Loss: 0.036720868200063705\n",
      "Loss: 0.027899257838726044\n",
      "Loss: 0.027677014470100403\n",
      "Loss: 0.02675187960267067\n",
      "Loss: 0.03682530298829079\n",
      "Loss: 0.033577535301446915\n",
      "Loss: 0.02662433311343193\n",
      "Loss: 0.024446580559015274\n",
      "Loss: 0.031342096626758575\n",
      "Loss: 0.022504357621073723\n",
      "Loss: 0.018070362508296967\n",
      "Loss: 0.018919525668025017\n",
      "Loss: 0.02788311056792736\n",
      "Loss: 0.0247210580855608\n",
      "Loss: 0.030276741832494736\n",
      "Loss: 0.029571784660220146\n",
      "Loss: 0.045796677470207214\n",
      "Loss: 0.017267413437366486\n",
      "Loss: 0.02236383594572544\n",
      "Loss: 0.025694753974676132\n",
      "Loss: 0.01828724704682827\n",
      "Loss: 0.026565147563815117\n",
      "Loss: 0.0169803649187088\n",
      "Loss: 0.034708522260189056\n",
      "Loss: 0.02891678735613823\n",
      "Loss: 0.023268310353159904\n",
      "Loss: 0.03443564847111702\n",
      "Loss: 0.025974076241254807\n",
      "Loss: 0.02190229669213295\n",
      "Loss: 0.027779191732406616\n",
      "Loss: 0.03696289286017418\n",
      "Loss: 0.019217655062675476\n",
      "Loss: 0.02250809781253338\n",
      "Loss: 0.022957539185881615\n",
      "Loss: 0.023089716210961342\n",
      "Loss: 0.027859728783369064\n",
      "Loss: 0.02015342377126217\n",
      "Loss: 0.035019535571336746\n",
      "Loss: 0.02862224169075489\n",
      "Loss: 0.0143500417470932\n",
      "Loss: 0.028331231325864792\n",
      "Loss: 0.018954560160636902\n",
      "Loss: 0.042708199471235275\n",
      "Loss: 0.03216637670993805\n",
      "Loss: 0.034400731325149536\n",
      "Loss: 0.02213362604379654\n",
      "Loss: 0.054038312286138535\n",
      "Loss: 0.03149357810616493\n",
      "Loss: 0.021392494440078735\n",
      "Loss: 0.02470521628856659\n",
      "Loss: 0.03180047497153282\n",
      "Loss: 0.031140033155679703\n",
      "Loss: 0.02249591425061226\n",
      "Loss: 0.02715221419930458\n",
      "Loss: 0.022114872932434082\n",
      "Loss: 0.03145977109670639\n",
      "Loss: 0.03166045621037483\n",
      "Loss: 0.038008738309144974\n",
      "Loss: 0.027478577569127083\n",
      "Loss: 0.018197260797023773\n",
      "Loss: 0.030831731855869293\n",
      "Loss: 0.03194425255060196\n",
      "Loss: 0.02203473635017872\n",
      "Loss: 0.024235151708126068\n",
      "Loss: 0.023642130196094513\n",
      "Loss: 0.020917393267154694\n",
      "Loss: 0.024075770750641823\n",
      "Loss: 0.03127233311533928\n",
      "Loss: 0.04081909358501434\n",
      "Loss: 0.020306464284658432\n",
      "Loss: 0.02774219587445259\n",
      "Loss: 0.025108695030212402\n",
      "Loss: 0.02059461548924446\n",
      "Loss: 0.023365773260593414\n",
      "Loss: 0.017607148736715317\n",
      "Loss: 0.023976987227797508\n",
      "Loss: 0.023846890777349472\n",
      "Loss: 0.02832767739892006\n",
      "Loss: 0.02469470724463463\n",
      "Loss: 0.02758551761507988\n",
      "Loss: 0.028942232951521873\n",
      "Loss: 0.013692428357899189\n",
      "Loss: 0.012175804935395718\n",
      "Loss: 0.025928033515810966\n",
      "Loss: 0.012586850672960281\n",
      "Loss: 0.018745459616184235\n",
      "Loss: 0.041606537997722626\n",
      "Loss: 0.015242775902152061\n",
      "Loss: 0.04384889826178551\n",
      "Loss: 0.03591272234916687\n",
      "Loss: 0.028913816437125206\n",
      "Loss: 0.023713067173957825\n",
      "Loss: 0.032096169888973236\n",
      "Loss: 0.020643511787056923\n",
      "Loss: 0.028819570317864418\n",
      "Loss: 0.03286063298583031\n",
      "Loss: 0.031908974051475525\n",
      "Loss: 0.02675500139594078\n",
      "Loss: 0.028950557112693787\n",
      "Loss: 0.027024615556001663\n",
      "Loss: 0.03054756298661232\n",
      "Loss: 0.022190619260072708\n",
      "Loss: 0.02301858551800251\n",
      "Loss: 0.020413490012288094\n",
      "Loss: 0.016805829480290413\n",
      "Loss: 0.030992861837148666\n",
      "Loss: 0.041902024298906326\n",
      "Loss: 0.016101554036140442\n",
      "Loss: 0.038672227412462234\n",
      "Loss: 0.023678412660956383\n",
      "Loss: 0.02736050635576248\n",
      "Loss: 0.01591985672712326\n",
      "Loss: 0.02716226503252983\n",
      "Loss: 0.028081050142645836\n",
      "Loss: 0.02249542996287346\n",
      "Loss: 0.025893965736031532\n",
      "Loss: 0.039998412132263184\n",
      "Loss: 0.02643265761435032\n",
      "Loss: 0.03378487005829811\n",
      "Loss: 0.02576104924082756\n",
      "Loss: 0.021897796541452408\n",
      "Loss: 0.028583789244294167\n",
      "Loss: 0.03432893007993698\n",
      "Loss: 0.01960475742816925\n",
      "Loss: 0.035135939717292786\n",
      "Loss: 0.0301495511084795\n",
      "Loss: 0.02604289911687374\n",
      "Loss: 0.024504024535417557\n",
      "Loss: 0.02362125739455223\n",
      "Loss: 0.02860306017100811\n",
      "Loss: 0.023370442911982536\n",
      "Loss: 0.026952749118208885\n",
      "Loss: 0.03327111899852753\n",
      "Loss: 0.026242706924676895\n",
      "Loss: 0.025380797684192657\n",
      "Loss: 0.029942866414785385\n",
      "Loss: 0.0296008363366127\n",
      "Loss: 0.018000086769461632\n",
      "Loss: 0.020936088636517525\n",
      "Loss: 0.020361827686429024\n",
      "Loss: 0.019053559750318527\n",
      "Loss: 0.012763679027557373\n",
      "Loss: 0.027062173932790756\n",
      "Loss: 0.02968163602054119\n",
      "Loss: 0.043445885181427\n",
      "Loss: 0.02036317251622677\n",
      "Loss: 0.03469937667250633\n",
      "Loss: 0.012838284485042095\n",
      "Loss: 0.011762571521103382\n",
      "Loss: 0.024909935891628265\n",
      "Loss: 0.01712673529982567\n",
      "Loss: 0.03361378610134125\n",
      "Loss: 0.03193939849734306\n",
      "Loss: 0.036848291754722595\n",
      "Loss: 0.03601868450641632\n",
      "Loss: 0.02949059195816517\n",
      "Loss: 0.02651049941778183\n",
      "Loss: 0.023513950407505035\n",
      "Loss: 0.03440899774432182\n",
      "Loss: 0.024306096136569977\n",
      "Loss: 0.027530422434210777\n",
      "Loss: 0.03055773302912712\n",
      "Loss: 0.02596592903137207\n",
      "Loss: 0.034742116928100586\n",
      "Loss: 0.023793160915374756\n",
      "Loss: 0.01625734753906727\n",
      "Loss: 0.015721501782536507\n",
      "Loss: 0.037595611065626144\n",
      "Loss: 0.03180873766541481\n",
      "Loss: 0.034885622560977936\n",
      "Loss: 0.04662157595157623\n",
      "Loss: 0.02911868318915367\n",
      "Loss: 0.039159420877695084\n",
      "Loss: 0.028030315414071083\n",
      "Loss: 0.027477622032165527\n",
      "Loss: 0.025754056870937347\n",
      "Loss: 0.025290489196777344\n",
      "Loss: 0.028432337567210197\n",
      "Loss: 0.024885131046175957\n",
      "Loss: 0.029097678139805794\n",
      "Loss: 0.034889522939920425\n",
      "Loss: 0.02146260254085064\n",
      "Loss: 0.02346705086529255\n",
      "Loss: 0.03934236988425255\n",
      "Loss: 0.02922223135828972\n",
      "Loss: 0.03467575088143349\n",
      "Loss: 0.030128611251711845\n",
      "Loss: 0.03329525515437126\n",
      "Loss: 0.048196710646152496\n",
      "Loss: 0.027976984158158302\n",
      "Loss: 0.02337055280804634\n",
      "Loss: 0.032576318830251694\n",
      "Loss: 0.02480969950556755\n",
      "Loss: 0.032631441950798035\n",
      "Loss: 0.031180500984191895\n",
      "Loss: 0.021108681336045265\n",
      "Loss: 0.03755800798535347\n",
      "Loss: 0.02531055361032486\n",
      "Loss: 0.0288549792021513\n",
      "Loss: 0.02500114031136036\n",
      "Loss: 0.018515989184379578\n",
      "Loss: 0.019106661900877953\n",
      "Loss: 0.02211524173617363\n",
      "Loss: 0.03020145557820797\n",
      "Loss: 0.05167713016271591\n",
      "Loss: 0.021638022735714912\n",
      "Loss: 0.025886310264468193\n",
      "Loss: 0.018090536817908287\n",
      "Loss: 0.030226796865463257\n",
      "Loss: 0.034429363906383514\n",
      "Loss: 0.019235095009207726\n",
      "Loss: 0.031675226986408234\n",
      "Loss: 0.03193033114075661\n",
      "Loss: 0.023902907967567444\n",
      "Loss: 0.024684522300958633\n",
      "Loss: 0.028506657108664513\n",
      "Loss: 0.033534251153469086\n",
      "Loss: 0.03010430373251438\n",
      "Loss: 0.02343996986746788\n",
      "Loss: 0.02812749147415161\n",
      "Loss: 0.03042001463472843\n",
      "Loss: 0.02510826662182808\n",
      "Loss: 0.026852475479245186\n",
      "Loss: 0.025726772844791412\n",
      "Loss: 0.030167987570166588\n",
      "Loss: 0.0264536514878273\n",
      "Loss: 0.022406261414289474\n",
      "Loss: 0.027554385364055634\n",
      "Loss: 0.021891582757234573\n",
      "Loss: 0.049650486558675766\n",
      "Loss: 0.02707614004611969\n",
      "Loss: 0.027847174555063248\n",
      "Loss: 0.02048363909125328\n",
      "Loss: 0.03424958512187004\n",
      "Loss: 0.030596038326621056\n",
      "Loss: 0.03428526967763901\n",
      "Loss: 0.023660022765398026\n",
      "Loss: 0.026506952941417694\n",
      "Loss: 0.027400275692343712\n",
      "Loss: 0.02628215029835701\n",
      "Loss: 0.0352352075278759\n",
      "Loss: 0.03389032185077667\n",
      "Loss: 0.03210502117872238\n",
      "Loss: 0.034396179020404816\n",
      "Loss: 0.01992427185177803\n",
      "Loss: 0.036042653024196625\n",
      "Loss: 0.038355570286512375\n",
      "Loss: 0.028513748198747635\n",
      "Loss: 0.024103960022330284\n",
      "Loss: 0.022797182202339172\n",
      "Loss: 0.0186409130692482\n",
      "Loss: 0.020659804344177246\n",
      "Loss: 0.0368657112121582\n",
      "Loss: 0.02095356583595276\n",
      "Loss: 0.03066154196858406\n",
      "Loss: 0.027078920975327492\n",
      "Loss: 0.022151438519358635\n",
      "Loss: 0.026256218552589417\n",
      "Loss: 0.02840527705848217\n",
      "Loss: 0.029483074322342873\n",
      "Loss: 0.03073861263692379\n",
      "Loss: 0.02299974113702774\n",
      "Loss: 0.020313546061515808\n",
      "Loss: 0.020016450434923172\n",
      "Loss: 0.024052442982792854\n",
      "Loss: 0.032232850790023804\n",
      "Loss: 0.02941254712641239\n",
      "Loss: 0.02225901372730732\n",
      "Loss: 0.024760261178016663\n",
      "Loss: 0.02220933884382248\n",
      "Loss: 0.027421627193689346\n",
      "Loss: 0.025707509368658066\n",
      "Loss: 0.037567898631095886\n",
      "Loss: 0.020565759390592575\n",
      "Loss: 0.03470419719815254\n",
      "Loss: 0.034085988998413086\n",
      "Loss: 0.02647051215171814\n",
      "Loss: 0.02223658189177513\n",
      "Loss: 0.03414130210876465\n",
      "Loss: 0.018701564520597458\n",
      "Loss: 0.03454501926898956\n",
      "Loss: 0.034547850489616394\n",
      "Loss: 0.022826844826340675\n",
      "Loss: 0.04328601062297821\n",
      "Loss: 0.028989311307668686\n",
      "Loss: 0.03502851352095604\n",
      "Loss: 0.02241179160773754\n",
      "Loss: 0.030669599771499634\n",
      "Loss: 0.03519734740257263\n",
      "Loss: 0.03295554220676422\n",
      "Loss: 0.02344535104930401\n",
      "Loss: 0.03081931173801422\n",
      "Loss: 0.02931721694767475\n",
      "Loss: 0.023185942322015762\n",
      "Loss: 0.028473084792494774\n",
      "Loss: 0.044269829988479614\n",
      "Loss: 0.02080693654716015\n",
      "Loss: 0.030124003067612648\n",
      "Loss: 0.022809825837612152\n",
      "Loss: 0.02985033206641674\n",
      "Loss: 0.021356848999857903\n",
      "Loss: 0.04125599190592766\n",
      "Loss: 0.034914225339889526\n",
      "Loss: 0.026241624727845192\n",
      "Loss: 0.028111688792705536\n",
      "Loss: 0.01870846189558506\n",
      "Loss: 0.034433476626873016\n",
      "Loss: 0.02499760314822197\n",
      "Loss: 0.028563065454363823\n",
      "Loss: 0.019564734771847725\n",
      "Loss: 0.030788201838731766\n",
      "Loss: 0.01997065171599388\n",
      "Loss: 0.026595376431941986\n",
      "Loss: 0.028957029804587364\n",
      "Loss: 0.03391551226377487\n",
      "Loss: 0.034336429089307785\n",
      "Loss: 0.022869061678647995\n",
      "Loss: 0.0400032140314579\n",
      "Loss: 0.019735589623451233\n",
      "Loss: 0.030328750610351562\n",
      "Loss: 0.025255709886550903\n",
      "Loss: 0.024634478613734245\n",
      "Loss: 0.02315928414463997\n",
      "Loss: 0.028493545949459076\n",
      "Loss: 0.022969113662838936\n",
      "Loss: 0.029477735981345177\n",
      "Loss: 0.0350828543305397\n",
      "Loss: 0.021185433492064476\n",
      "Loss: 0.0295502208173275\n",
      "Loss: 0.027211518958210945\n",
      "Loss: 0.021474136039614677\n",
      "Loss: 0.029714541509747505\n",
      "Loss: 0.026071682572364807\n",
      "Loss: 0.02550244890153408\n",
      "Loss: 0.01495545543730259\n",
      "Loss: 0.015220495872199535\n",
      "Loss: 0.03593064099550247\n",
      "Loss: 0.041524939239025116\n",
      "Loss: 0.032786641269922256\n",
      "Loss: 0.019669463858008385\n",
      "Loss: 0.031239774078130722\n",
      "Loss: 0.027714502066373825\n",
      "Loss: 0.02798617258667946\n",
      "Loss: 0.029143642634153366\n",
      "Loss: 0.021089553833007812\n",
      "Loss: 0.019911285489797592\n",
      "Loss: 0.04179111123085022\n",
      "Loss: 0.034750185906887054\n",
      "Loss: 0.028988081961870193\n",
      "Loss: 0.03290407732129097\n",
      "Loss: 0.020682187750935555\n",
      "Loss: 0.027921009808778763\n",
      "Loss: 0.024474821984767914\n",
      "Loss: 0.026010755449533463\n",
      "Loss: 0.02070479653775692\n",
      "Loss: 0.030338192358613014\n",
      "Loss: 0.028037944808602333\n",
      "Loss: 0.02456481382250786\n",
      "Loss: 0.026928681880235672\n",
      "Loss: 0.04861711338162422\n",
      "Loss: 0.029086433351039886\n",
      "Loss: 0.02200923301279545\n",
      "Loss: 0.03644176572561264\n",
      "Loss: 0.042268574237823486\n",
      "Loss: 0.026202276349067688\n",
      "Loss: 0.025547079741954803\n",
      "Loss: 0.03531768172979355\n",
      "Loss: 0.02725842595100403\n",
      "Loss: 0.02755192667245865\n",
      "Loss: 0.02925563044846058\n",
      "Loss: 0.026383167132735252\n",
      "Loss: 0.02900981530547142\n",
      "Loss: 0.021678784862160683\n",
      "Loss: 0.02257184498012066\n",
      "Loss: 0.032455384731292725\n",
      "Loss: 0.027366453781723976\n",
      "Loss: 0.034577127546072006\n",
      "Loss: 0.023898635059595108\n",
      "Loss: 0.027629252523183823\n",
      "Loss: 0.026649823412299156\n",
      "Loss: 0.02483964152634144\n",
      "Loss: 0.0508570596575737\n",
      "Loss: 0.0227636881172657\n",
      "Loss: 0.03312201797962189\n",
      "Loss: 0.0223553404211998\n",
      "Loss: 0.024621078744530678\n",
      "Loss: 0.019278569146990776\n",
      "Loss: 0.028102045878767967\n",
      "Loss: 0.026237864047288895\n",
      "Loss: 0.03133350610733032\n",
      "Loss: 0.02518383413553238\n",
      "Loss: 0.03409808874130249\n",
      "Loss: 0.03822191059589386\n",
      "Loss: 0.027477474883198738\n",
      "Loss: 0.020684855058789253\n",
      "Loss: 0.02367575280368328\n",
      "Loss: 0.029351288452744484\n",
      "Loss: 0.023052744567394257\n",
      "Loss: 0.0345490463078022\n",
      "Loss: 0.01929950714111328\n",
      "Loss: 0.016132697463035583\n",
      "Loss: 0.022687576711177826\n",
      "Loss: 0.01797499880194664\n",
      "Loss: 0.022451305761933327\n",
      "Loss: 0.017913131043314934\n",
      "Loss: 0.017020441591739655\n",
      "Loss: 0.03237656503915787\n",
      "Loss: 0.023565372452139854\n",
      "Loss: 0.011770152486860752\n",
      "Loss: 0.025682536885142326\n",
      "Loss: 0.021310213953256607\n",
      "Loss: 0.031603407114744186\n",
      "Loss: 0.03170287236571312\n",
      "Loss: 0.02723059058189392\n",
      "Loss: 0.029234176501631737\n",
      "Loss: 0.02539835311472416\n",
      "Loss: 0.023355087265372276\n",
      "Loss: 0.015692463144659996\n",
      "Loss: 0.0333118662238121\n",
      "Loss: 0.029345953837037086\n",
      "Loss: 0.026730550453066826\n",
      "Loss: 0.024313626810908318\n",
      "Loss: 0.02433076500892639\n",
      "Loss: 0.024817243218421936\n",
      "Loss: 0.02143065817654133\n",
      "Loss: 0.027217015624046326\n",
      "Loss: 0.044772256165742874\n",
      "Loss: 0.041623931378126144\n",
      "Loss: 0.023127658292651176\n",
      "Loss: 0.030119428411126137\n",
      "Loss: 0.024956777691841125\n",
      "Loss: 0.034568414092063904\n",
      "Loss: 0.03555162250995636\n",
      "Loss: 0.027919314801692963\n",
      "Loss: 0.028935497626662254\n",
      "Loss: 0.0255951639264822\n",
      "Loss: 0.0373552031815052\n",
      "Loss: 0.038798313587903976\n",
      "Loss: 0.027079548686742783\n",
      "Loss: 0.02362072840332985\n",
      "Loss: 0.034977659583091736\n",
      "Loss: 0.03095882758498192\n",
      "Loss: 0.024263614788651466\n",
      "Loss: 0.023839538916945457\n",
      "Loss: 0.02807369828224182\n",
      "Loss: 0.026882503181695938\n",
      "Loss: 0.030693676322698593\n",
      "Loss: 0.03033534623682499\n",
      "Loss: 0.020630940794944763\n",
      "Loss: 0.02239871211349964\n",
      "Loss: 0.023391854017972946\n",
      "Loss: 0.03140585124492645\n",
      "Loss: 0.03144135698676109\n",
      "Loss: 0.02614247426390648\n",
      "Loss: 0.016199229285120964\n",
      "Loss: 0.02002498134970665\n",
      "Loss: 0.037945181131362915\n",
      "Loss: 0.018582189455628395\n",
      "Loss: 0.029211655259132385\n",
      "Loss: 0.014995811507105827\n",
      "Loss: 0.02528933808207512\n",
      "Loss: 0.03590798377990723\n",
      "Loss: 0.02203953266143799\n",
      "Loss: 0.0496356226503849\n",
      "Loss: 0.025756925344467163\n",
      "Loss: 0.017560726031661034\n",
      "Loss: 0.02136235311627388\n",
      "Loss: 0.034176912158727646\n",
      "Loss: 0.029158202931284904\n",
      "Loss: 0.03848018869757652\n",
      "Loss: 0.028340868651866913\n",
      "Loss: 0.029340624809265137\n",
      "Loss: 0.019755108281970024\n",
      "Loss: 0.023824261501431465\n",
      "Loss: 0.02738848514854908\n",
      "Loss: 0.025292716920375824\n",
      "Loss: 0.025989070534706116\n",
      "Loss: 0.033420536667108536\n",
      "Loss: 0.019913895055651665\n",
      "Loss: 0.017173150554299355\n",
      "Loss: 0.01802849769592285\n",
      "Loss: 0.03394283354282379\n",
      "Loss: 0.02809114195406437\n",
      "Loss: 0.03094463050365448\n",
      "Loss: 0.035030756145715714\n",
      "Loss: 0.027031853795051575\n",
      "Loss: 0.025936244055628777\n",
      "Loss: 0.02008971944451332\n",
      "Loss: 0.027198266237974167\n",
      "Loss: 0.02608538791537285\n",
      "Loss: 0.023570872843265533\n",
      "Loss: 0.02222749963402748\n",
      "Loss: 0.038468651473522186\n",
      "Loss: 0.04570604860782623\n",
      "Loss: 0.036296334117650986\n",
      "Loss: 0.02263176254928112\n",
      "Loss: 0.026428721845149994\n",
      "Loss: 0.02260998822748661\n",
      "Loss: 0.031697724014520645\n",
      "Loss: 0.02938348799943924\n",
      "Loss: 0.03473864495754242\n",
      "Loss: 0.02102930285036564\n",
      "Loss: 0.0187094584107399\n",
      "Loss: 0.022664135321974754\n",
      "Loss: 0.01901949942111969\n",
      "Loss: 0.016253454610705376\n",
      "Loss: 0.023586979135870934\n",
      "Loss: 0.02049873024225235\n",
      "Loss: 0.022048622369766235\n",
      "Loss: 0.0212540403008461\n",
      "Loss: 0.020466776564717293\n",
      "Loss: 0.040709786117076874\n",
      "Loss: 0.050786275416612625\n",
      "Loss: 0.027443675324320793\n",
      "Loss: 0.01507016271352768\n",
      "Loss: 0.03769596293568611\n",
      "Loss: 0.036272358149290085\n",
      "Loss: 0.022340521216392517\n",
      "Loss: 0.03161322697997093\n",
      "Loss: 0.023406680673360825\n",
      "Loss: 0.033832114189863205\n",
      "Loss: 0.03358086198568344\n",
      "Loss: 0.029229190200567245\n",
      "Loss: 0.030244268476963043\n",
      "Loss: 0.01991100236773491\n",
      "Loss: 0.031123805791139603\n",
      "Loss: 0.02297605574131012\n",
      "Loss: 0.016816124320030212\n",
      "Loss: 0.023020783439278603\n",
      "Loss: 0.024904828518629074\n",
      "Loss: 0.06907182931900024\n",
      "Loss: 0.036280639469623566\n",
      "Loss: 0.0348065048456192\n",
      "Loss: 0.02154211513698101\n",
      "Loss: 0.015174399130046368\n",
      "Loss: 0.01797756366431713\n",
      "Loss: 0.018052512779831886\n",
      "Loss: 0.03584764897823334\n",
      "Loss: 0.020700231194496155\n",
      "Loss: 0.03448662906885147\n",
      "Loss: 0.016910996288061142\n",
      "Loss: 0.03193304315209389\n",
      "Loss: 0.018102478235960007\n",
      "Loss: 0.025672007352113724\n",
      "Loss: 0.02439899556338787\n",
      "Loss: 0.022031761705875397\n",
      "Loss: 0.014723381958901882\n",
      "Loss: 0.018559543415904045\n",
      "Loss: 0.019642667844891548\n",
      "Loss: 0.029355214908719063\n",
      "Loss: 0.03996209427714348\n",
      "Loss: 0.05119193345308304\n",
      "Loss: 0.04106910154223442\n",
      "Loss: 0.020878475159406662\n",
      "Loss: 0.020849833264946938\n",
      "Loss: 0.01676960475742817\n",
      "Loss: 0.02780846133828163\n",
      "Loss: 0.02130097523331642\n",
      "Loss: 0.038751378655433655\n",
      "Loss: 0.02968749962747097\n",
      "Loss: 0.030683111399412155\n",
      "Loss: 0.030074898153543472\n",
      "Loss: 0.026625193655490875\n",
      "Loss: 0.021237604320049286\n",
      "Loss: 0.03201022371649742\n",
      "Loss: 0.02545822039246559\n",
      "Loss: 0.025785129517316818\n",
      "Loss: 0.027802875265479088\n",
      "Loss: 0.02120782621204853\n",
      "Loss: 0.045372601598501205\n",
      "Loss: 0.015272030606865883\n",
      "Loss: 0.022594155743718147\n",
      "Loss: 0.03286944702267647\n",
      "Loss: 0.02965712547302246\n",
      "Loss: 0.05940285325050354\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# define loss function\n",
    "loss_fn = nn.BCEWithLogitsLoss(reduction='none')\n",
    "# loss_fn = nn.BCELoss()\n",
    "# optimizer Adam optimizer\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "num_epoch = 5\n",
    "\n",
    "# Training RNN loop\n",
    "for epoch in range(num_epoch):  # Loop over the dataset multiple times\n",
    "    for sequences, labels, lengths in patient_dataloader:\n",
    "        # print('sequence shape: ', sequences.shape)\n",
    "        # print('label shape: ', labels.shape)\n",
    "        # print('lengths: ', lengths)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        # During training every mini-batch, explicitly set all gradients to zero before start doing backpropagation (i.e., updating Weights and biases)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(sequences, lengths)\n",
    "        # print('output shape: ', outputs.shape)\n",
    "        mask = (labels != -1).float()\n",
    "        # print('mask shape: ', mask.shape)\n",
    "\n",
    "        loss = loss_fn(outputs.squeeze(), labels.float())\n",
    "        # Penalizing weight values of majority classes during back-propagation\n",
    "        weights = torch.ones(labels.shape)\n",
    "        weights[labels == 0] = pos_weight\n",
    "        weights = torch.Tensor(np.array(weights))\n",
    "        loss = loss * weights\n",
    "        # print('loss shape: ', loss.shape)\n",
    "        loss = (loss * mask).sum(axis=1) / mask.sum(axis=1)\n",
    "        # print('after mask, loss shape: ', loss.shape)\n",
    "        loss = loss.mean()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f'Loss: {loss.item()}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "train_result = defaultdict(list)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for train_seq, train_labels, train_lengths in patient_dataloader:\n",
    "\n",
    "        model_predict = model(train_seq, train_lengths)\n",
    "        m = nn.Sigmoid()\n",
    "        predict_prob = m(model_predict)\n",
    "\n",
    "        # mask = (labels != -1).float()\n",
    "        # ypredict = ypredict*mask\n",
    "\n",
    "        predict_label = (predict_prob >= 0.5).int()\n",
    "\n",
    "        mask = (train_labels != -1)\n",
    "        train_label = train_labels[mask]\n",
    "\n",
    "        predict_label = predict_label[:,:,0]\n",
    "        predict_label = predict_label[mask]\n",
    "\n",
    "        predict_prob = predict_prob[:,:,0]\n",
    "        predict_prob = predict_prob[mask]\n",
    "        predict_prob = predict_prob.flatten()\n",
    "\n",
    "        train_result['prediction_label'].extend(predict_label.squeeze().tolist())\n",
    "        train_result['true_label'].extend(train_label.tolist())\n",
    "        train_result['predict_prob'].extend(predict_prob.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.57      0.72   1218544\n",
      "           1       0.03      0.70      0.06     22669\n",
      "\n",
      "    accuracy                           0.57   1241213\n",
      "   macro avg       0.51      0.63      0.39   1241213\n",
      "weighted avg       0.97      0.57      0.71   1241213\n",
      "\n",
      "Train: auroc  0.692751296767565\n"
     ]
    }
   ],
   "source": [
    "y_train = train_result['true_label']\n",
    "ytrain_pred = train_result['prediction_label']\n",
    "train_prob = train_result['predict_prob']\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_train, ytrain_pred))\n",
    "# ROC curve\n",
    "fpr_train,tpr_train,threshold=roc_curve(y_train,train_prob)\n",
    "auroc_train=roc_auc_score(y_train,train_prob)\n",
    "print('Train: auroc ',auroc_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction of evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation dataloader\n",
    "eval_df = Patient_dataset(x_eval,y_eval)\n",
    "eval_dataloader = DataLoader(eval_df, batch_size=64, collate_fn=collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = defaultdict(list)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for eval_seq, eval_labels, eval_lengths in eval_dataloader:\n",
    "\n",
    "        model_predict = model(eval_seq, eval_lengths)\n",
    "        # print(model_predict)\n",
    "        # print(' ')\n",
    "        m = nn.Sigmoid()\n",
    "        predict_prob = m(model_predict)\n",
    "\n",
    "        # mask = (labels != -1).float()\n",
    "        # ypredict = ypredict*mask\n",
    "\n",
    "        predict_label = (predict_prob >= 0.5).int()\n",
    "\n",
    "        mask = (eval_labels != -1)\n",
    "        eval_label = eval_labels[mask]\n",
    "\n",
    "        predict_label = predict_label[:,:,0]\n",
    "        predict_label = predict_label[mask]\n",
    "\n",
    "        predict_prob = predict_prob[:,:,0]\n",
    "        predict_prob = predict_prob[mask]\n",
    "        predict_prob = predict_prob.flatten()\n",
    "\n",
    "        eval_result['prediction_label'].extend(predict_label.squeeze().tolist())\n",
    "        eval_result['true_label'].extend(eval_label.tolist())\n",
    "        eval_result['predict_prob'].extend(predict_prob.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.58      0.73    305750\n",
      "           1       0.03      0.70      0.05      5247\n",
      "\n",
      "    accuracy                           0.58    310997\n",
      "   macro avg       0.51      0.64      0.39    310997\n",
      "weighted avg       0.97      0.58      0.72    310997\n",
      "\n",
      "Test: auroc  0.6985315631826994\n"
     ]
    }
   ],
   "source": [
    "y_eval = eval_result['true_label']\n",
    "yeval_pred = eval_result['prediction_label']\n",
    "eval_prob = eval_result['predict_prob']\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_eval, yeval_pred))\n",
    "# ROC curve\n",
    "fpr_eval,tpr_eval,threshold=roc_curve(y_eval,eval_prob)\n",
    "auroc=roc_auc_score(y_eval,eval_prob)\n",
    "print('Test: auroc ',auroc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkIAAAHHCAYAAABTMjf2AAAAPHRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMHJjMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy9ytYEsAAAACXBIWXMAAA9hAAAPYQGoP6dpAACnF0lEQVR4nOzdeVhN2xsH8O9pOs3zqNKAzKWSIWWMzMIlChkvrnm65nBN1zxf4zWEzCGzyzXLFHUzFZJCRZrTeM76/dHPznEqRbUb3s/z9Gi/e+193nNK5z1rr72WgDHGQAghhBBSBcnwnQAhhBBCCF+oECKEEEJIlUWFECGEEEKqLCqECCGEEFJlUSFECCGEkCqLCiFCCCGEVFlUCBFCCCGkyqJCiBBCCCFVFhVChBBCCKmyqBAipISYm5tj8ODBfKdR5bRu3RqtW7fmO43vmj9/PgQCAeLi4vhOpdwRCASYP39+iZwrIiICAoEAu3fvLpHzkcqPCiFSIezevRsCgYD7kpOTg7GxMQYPHox3797xnV65lpaWhoULF8La2hrKysrQ0NCAs7MzfHx8UFFW2Hn69Cnmz5+PiIgIvlORIhKJsGvXLrRu3Rra2toQCoUwNzfHkCFD8ODBA77TKxG+vr5Yu3Yt32lIKI85kYpJju8ECCmOP/74AxYWFsjIyMCdO3ewe/du3Lx5E48fP4aioiKvuYWGhkJGpnx9toiNjUW7du3w7Nkz9OvXD2PHjkVGRgaOHTsGLy8vnD17Fvv374esrCzfqRbq6dOnWLBgAVq3bg1zc3OJfRcvXuQnKQDp6eno1asXzp8/j5YtW2LWrFnQ1tZGREQEDh8+jD179iAyMhImJia85VgSfH198fjxY0ycOLFUzp+eng45ueK9HRWUk5mZGdLT0yEvL1+CGZLKjAohUqF06tQJjRs3BgAMHz4curq6WLZsGfz9/dG3b19ecxMKhWX+mBkZGVBQUCiwAPPy8sKzZ89w/PhxdO/enYuPHz8e06ZNw8qVK2Fra4vp06eXVcoAcnupVFRUSuRcCgoKJXKeHzFt2jScP38ea9askXpDnjdvHtasWVOm+TDGkJGRASUlpTJ93B8hFouRlZUFRUXFEv0QIxAIeP9QRCoYRkgFsGvXLgaA3b9/XyJ++vRpBoAtWbJEIv7s2TPWu3dvpqWlxYRCIbO3t2cnT56UOm9CQgKbOHEiMzMzYwoKCszY2JgNHDiQffz4kWuTkZHBvL29WY0aNZiCggIzMTFh06ZNYxkZGRLnMjMzY15eXowxxu7fv88AsN27d0s95vnz5xkAdurUKS729u1bNmTIEKavr88UFBRYvXr12N9//y1x3JUrVxgAduDAATZ79mxWrVo1JhAIWEJCQr6vWUBAAAPAhg4dmu/+7OxsVqtWLaalpcU+f/7MGGPs9evXDABbsWIFW716NatevTpTVFRkLVu2ZCEhIVLnKMrr/OVnd/XqVTZ69Gimp6fHNDU1GWOMRUREsNGjRzMrKyumqKjItLW12S+//MJev34tdfy3X1euXGGMMdaqVSvWqlUrqdfp0KFDbNGiRczY2JgJhULWtm1b9uLFC6nnsHHjRmZhYcEUFRWZg4MDu379utQ58xMVFcXk5ORY+/btC233xbx58xgA9uLFC+bl5cU0NDSYuro6Gzx4MEtLS5Nou3PnTtamTRump6fHFBQUWN26ddlff/0ldU4zMzPWpUsXdv78eWZvb8+EQiFbs2ZNsc7BGGNnz55lLVu2ZKqqqkxNTY01btyY7d+/nzGW+/p++9qbmZlxxxb1/wcANmbMGLZv3z5Wr149Jicnx44fP87tmzdvHtc2OTmZTZgwgft/qaenx1xcXFhgYOB3c/ryO7xr1y6Jx3/27Bnr06cP09XVZYqKiszKyorNmjWrsB8ZqSKoR4hUaF/GjGhpaXGxJ0+eoEWLFjA2NsaMGTOgoqKCw4cPw83NDceOHUPPnj0BAKmpqXB2dsazZ88wdOhQ2NnZIS4uDv7+/nj79i10dXUhFovRvXt33Lx5E7/++ivq1q2LkJAQrFmzBmFhYThx4kS+eTVu3BiWlpY4fPgwvLy8JPYdOnQIWlpacHV1BZB7+apZs2YQCAQYO3Ys9PT0cO7cOQwbNgzJyclSPQ0LFy6EgoICpk6diszMzAJ7RE6dOgUAGDRoUL775eTk4OHhgQULFuDWrVtwcXHh9vn4+CAlJQVjxoxBRkYG1q1bh7Zt2yIkJAQGBgbFep2/+O2336Cnpwdvb2+kpaUBAO7fv4/bt2+jX79+MDExQUREBDZv3ozWrVvj6dOnUFZWRsuWLTF+/HisX78es2bNQt26dQGA+7cgf/75J2RkZDB16lQkJSVh+fLl8PT0xN27d7k2mzdvxtixY+Hs7IxJkyYhIiICbm5u0NLS+u7lrHPnziEnJwcDBw4stN23+vbtCwsLCyxduhQPHz7Ejh07oK+vj2XLlknkVb9+fXTv3h1ycnI4deoUfvvtN4jFYowZM0bifKGhoejfvz9GjhyJESNGoHbt2sU6x+7duzF06FDUr18fM2fOhKamJh49eoTz58/Dw8MDs2fPRlJSEt6+fcv1cKmqqgJAsf9//Pvvvzh8+DDGjh0LXV1dqcucX4waNQpHjx7F2LFjUa9ePXz69Ak3b97Es2fPYGdnV2hO+fnvv//g7OwMeXl5/PrrrzA3N8erV69w6tQpLF68uGg/OFJ58V2JEVIUX3oFLl26xD5+/MiioqLY0aNHmZ6eHhMKhSwqKopr265dO9awYUOJT6RisZg5OjqyWrVqcTFvb28GgPn5+Uk9nlgsZowxtnfvXiYjI8Nu3LghsX/Lli0MALt16xYX+7pHiDHGZs6cyeTl5Vl8fDwXy8zMZJqamhK9NMOGDWNGRkYsLi5O4jH69evHNDQ0uN6aLz0dlpaWXKwwbm5uDECBPUaMMebn58cAsPXr1zPG8j5NKykpsbdv33Lt7t69ywCwSZMmcbGivs5ffnZOTk4sJydH4vHzex5ferJ8fHy42JEjRyR6gb5WUI9Q3bp1WWZmJhdft24dA8D1bGVmZjIdHR3m4ODAsrOzuXa7d+9mAL7bIzRp0iQGgD169KjQdl986RH6toeuZ8+eTEdHRyKW3+vi6urKLC0tJWJmZmYMADt//rxU+6KcIzExkampqbGmTZuy9PR0ibZf/g8wxliXLl0keoG+KM7/DwBMRkaGPXnyROo8+KZHSENDg40ZM0aq3dcKyim/HqGWLVsyNTU19ubNmwKfI6m6ytfITkK+w8XFBXp6ejA1NcUvv/wCFRUV+Pv7c5/e4+Pj8e+//6Jv375ISUlBXFwc4uLi8OnTJ7i6uuLFixfcXWbHjh2DjY2NVM8FkDvOAACOHDmCunXrok6dOty54uLi0LZtWwDAlStXCszV3d0d2dnZ8PPz42IXL15EYmIi3N3dAeSO6Th27Bi6desGxpjEY7i6uiIpKQkPHz6UOK+Xl1eRxoCkpKQAANTU1Aps82VfcnKyRNzNzQ3GxsbcdpMmTdC0aVOcPXsWQPFe5y9GjBghNSj76+eRnZ2NT58+oWbNmtDU1JR63sU1ZMgQid4yZ2dnAEB4eDgA4MGDB/j06RNGjBghMVDX09NTooexIF9es8Je3/yMGjVKYtvZ2RmfPn2S+Bl8/bokJSUhLi4OrVq1Qnh4OJKSkiSOt7Cw4HoXv1aUc/zzzz9ISUnBjBkzpMbVfPk/UJji/v9o1aoV6tWr993zampq4u7du3j//v13237Px48fcf36dQwdOhTVq1eX2FeU50gqP7o0RiqUTZs2wcrKCklJSdi5cyeuX78uMUj55cuXYIxh7ty5mDt3br7n+PDhA4yNjfHq1Sv07t270Md78eIFnj17Bj09vQLPVRAbGxvUqVMHhw4dwrBhwwDkXhbT1dXl3ig+fvyIxMREbNu2Ddu2bSvSY1hYWBSa8xdf3qBTUlKgqamZb5uCiqVatWpJtbWyssLhw4cBFO91Lizv9PR0LF26FLt27cK7d+8kbuf/9g2/uL590/tS3CQkJAAA3rx5AwCoWbOmRDs5ObkCL9l8TV1dHUDea1gSeX05561btzBv3jwEBATg8+fPEu2TkpKgoaHBbRf0+1CUc7x69QoA0KBBg2I9hy+K+/+jqL+7y5cvh5eXF0xNTWFvb4/OnTtj0KBBsLS0LHaOXwrfH32OpPKjQohUKE2aNOHuGnNzc4OTkxM8PDwQGhoKVVVViMViAMDUqVPz/ZQMSL/xFUYsFqNhw4ZYvXp1vvtNTU0LPd7d3R2LFy9GXFwc1NTU4O/vj/79+3M9EF/yHTBggNRYoi+sra0ltot6R1DdunVx4sQJ/Pfff2jZsmW+bf777z8AKNKn9K/9yOucX97jxo3Drl27MHHiRDRv3hwaGhoQCATo168f9xg/qqApAVgJzZ1Up04dAEBISAgaNWpU5OO+l9erV6/Qrl071KlTB6tXr4apqSkUFBRw9uxZrFmzRup1ye91Le45flRx/38U9Xe3b9++cHZ2xvHjx3Hx4kWsWLECy5Ytg5+fHzp16vTTeRPyNSqESIUlKyuLpUuXok2bNti4cSNmzJjBfWKUl5eXGPybnxo1auDx48ffbRMcHIx27dr9UDe6u7s7FixYgGPHjsHAwADJycno168ft19PTw9qamoQiUTfzbe4unbtiqVLl8LHxyffQkgkEsHX1xdaWlpo0aKFxL4XL15ItQ8LC+N6SorzOhfm6NGj8PLywqpVq7hYRkYGEhMTJdqVxiUMMzMzALm9W23atOHiOTk5iIiIkCpAv9WpUyfIyspi3759xR4wXZhTp04hMzMT/v7+Er1HhV2G/dFz1KhRAwDw+PHjQj8gFPT6/+z/j8IYGRnht99+w2+//YYPHz7Azs4Oixcv5gqhoj7el9/V7/1fJ1UXjREiFVrr1q3RpEkTrF27FhkZGdDX10fr1q2xdetWREdHS7X/+PEj933v3r0RHByM48ePS7X78um8b9++ePfuHbZv3y7VJj09nbv7qSB169ZFw4YNcejQIRw6dAhGRkYSRYmsrCx69+6NY8eO5fuH+ut8i8vR0REuLi7YtWsXTp8+LbV/9uzZCAsLw++//y71Sf3EiRMSY3zu3buHu3fvcm9CxXmdCyMrKyvVQ7NhwwaIRCKJ2Jc5h74tkH5G48aNoaOjg+3btyMnJ4eL79+/n7t8VhhTU1OMGDECFy9exIYNG6T2i8VirFq1Cm/fvi1WXl96jL69TLhr164SP0eHDh2gpqaGpUuXIiMjQ2Lf18eqqKjke6nyZ/9/5EckEkk9lr6+PqpVq4bMzMzv5vQtPT09tGzZEjt37kRkZKTEvpLqHSQVG/UIkQpv2rRp6NOnD3bv3o1Ro0Zh06ZNcHJyQsOGDTFixAhYWloiNjYWAQEBePv2LYKDg7njjh49ij59+mDo0KGwt7dHfHw8/P39sWXLFtjY2GDgwIE4fPgwRo0ahStXrqBFixYQiUR4/vw5Dh8+jAsXLnCX6gri7u4Ob29vKCoqYtiwYVKTH/7555+4cuUKmjZtihEjRqBevXqIj4/Hw4cPcenSJcTHx//wa+Pj44N27dqhR48e8PDwgLOzMzIzM+Hn54erV6/C3d0d06ZNkzquZs2acHJywujRo5GZmYm1a9dCR0cHv//+O9emqK9zYbp27Yq9e/dCQ0MD9erVQ0BAAC5dugQdHR2Jdo0aNYKsrCyWLVuGpKQkCIVCtG3bFvr6+j/82igoKGD+/PkYN24c2rZti759+yIiIgK7d+9GjRo1itTjsGrVKrx69Qrjx4+Hn58funbtCi0tLURGRuLIkSN4/vy5RA9gUXTo0AEKCgro1q0bRo4cidTUVGzfvh36+vr5Fp0/cw51dXWsWbMGw4cPh4ODAzw8PKClpYXg4GB8/vwZe/bsAQDY29vj0KFDmDx5MhwcHKCqqopu3bqVyP+Pb6WkpMDExAS//PILbGxsoKqqikuXLuH+/fsSPYcF5ZSf9evXw8nJCXZ2dvj1119hYWGBiIgInDlzBkFBQcXKj1RCvNyrRkgxFTShImOMiUQiVqNGDVajRg3u9uxXr16xQYMGMUNDQyYvL8+MjY1Z165d2dGjRyWO/fTpExs7diwzNjbmJoPz8vKSuJU9KyuLLVu2jNWvX58JhUKmpaXF7O3t2YIFC1hSUhLX7tvb57948eIFN+nbzZs3831+sbGxbMyYMczU1JTJy8szQ0ND1q5dO7Zt2zauzZfbwo8cOVKs1y4lJYXNnz+f1a9fnykpKTE1NTXWokULtnv3bqnbh7+eUHHVqlXM1NSUCYVC5uzszIKDg6XOXZTXubCfXUJCAhsyZAjT1dVlqqqqzNXVlT1//jzf13L79u3M0tKSycrKFmlCxW9fp4Im2lu/fj0zMzNjQqGQNWnShN26dYvZ29uzjh07FuHVZSwnJ4ft2LGDOTs7Mw0NDSYvL8/MzMzYkCFDJG6t/3L7/NeTdX79+nw9iaS/vz+ztrZmioqKzNzcnC1btozt3LlTqt2XCRXzU9RzfGnr6OjIlJSUmLq6OmvSpAk7cOAAtz81NZV5eHgwTU1NqQkVi/r/A/+fUDE/+Or2+czMTDZt2jRmY2PD1NTUmIqKCrOxsZGaDLKgnAr6OT9+/Jj17NmTaWpqMkVFRVa7dm02d+7cfPMhVYuAMeobJITkioiIgIWFBVasWIGpU6fynQ4vxGIx9PT00KtXr3wv+RBCKhcaI0QIqbIyMjKkxon4+PggPj4erVu35icpQkiZojFChJAq686dO5g0aRL69OkDHR0dPHz4EH///TcaNGiAPn368J0eIaQMUCFECKmyzM3NYWpqivXr1yM+Ph7a2toYNGgQ/vzzT15XtSeElB0aI0QIIYSQKovGCBFCCCGkyqJCiBBCCCFVVpUbIyQWi/H+/XuoqanRysOEEEJIBcEYQ0pKCqpVqyY1Me3PqHKF0Pv377+7UCYhhBBCyqeoqCiYmJiU2PmqXCGkpqYGIPeFVFdX5zkbQgghhBRFcnIyTE1NuffxklLlCqEvl8PU1dWpECKEEEIqmJIe1kKDpQkhhBBSZVEhRAghhJAqiwohQgghhFRZVAgRQgghpMqiQogQQgghVRYVQoQQQgipsqgQIoQQQkiVRYUQIYQQQqosKoQIIYQQUmVRIUQIIYSQKovXQuj69evo1q0bqlWrBoFAgBMnTnz3mKtXr8LOzg5CoRA1a9bE7t27Sz1PQgghhFROvBZCaWlpsLGxwaZNm4rU/vXr1+jSpQvatGmDoKAgTJw4EcOHD8eFCxdKOVNCCCGEVEa8LrraqVMndOrUqcjtt2zZAgsLC6xatQoAULduXdy8eRNr1qyBq6traaVJCCGEkEqqQq0+HxAQABcXF4mYq6srJk6cyE9ChBBCCPk5jAGf44GPz4HI28hmMkhNTYXsm+tIUjZHyucMJKdlICyqdC5iVahCKCYmBgYGBhIxAwMDJCcnIz09HUpKSlLHZGZmIjMzk9tOTk4u9TwJIYQQ8n9iEZD6AchMAT7HQZQci/SwKxC+OA35jE9SzeUBaP3/e3U8QHSKCiafdMW1CN1SSa9CFUI/YunSpViwYAHfaRBCCCGVX04mPkeHIuX1fWjdWgSFzHipJrIAVIt4upPPa2D4qfaI+6wMIKMkM+VUqELI0NAQsbGxErHY2Fioq6vn2xsEADNnzsTkyZO57eTkZJiampZqnoQQQkilxRgyMjPw7vl9JEYEwTj8CAyT/+N2K///q7ieiM0gAHBJbIuXCvUhYIrYeiIRmZkMAKCtLUS8dF310ypUIdS8eXOcPXtWIvbPP/+gefPmBR4jFAohFApLOzVCCCGkcshIAj48A9I+Qpz0DtmRD4DIAAhT3wIAxBBAEQw1inHKp2IzqCAdd9EQ1fEekYp1oKRjiljj9lDQqQ4DdUWY6Sijhp4q6snmjQVqaPwQI0acgptbHaxe3QqWliV/hYfXQig1NRUvX77ktl+/fo2goCBoa2ujevXqmDlzJt69ewcfHx8AwKhRo7Bx40b8/vvvGDp0KP79918cPnwYZ86c4espEEIIIRWHWATEPgE+vQAibuWO20mNBctIgiA6SKq5DIBvuxJkwAo8fQaTh6IgG0GojQT1Ooiv1hrq1l1gqK4ILV1l9FWUBwA0y+dYkUiMnBwx8FUhNGyYLUxN1dGhQw2kpKQU//kWAa+F0IMHD9CmTRtu+8slLC8vL+zevRvR0dGIjIzk9ltYWODMmTOYNGkS1q1bBxMTE+zYsYNunSeEEEK+lfQWiLwDvL0PBO4GcgoeYyMo5qmfydSCtkwaPsuq4Z1BO1SvYwfTZr2hKJNbxDQq5vmiopIwaNAJNGighw0bOuflJRDA1bVmMc9WPALGWMGlXSWUnJwMDQ0NJCUlQV1dne90CCGEkB/DGJAaC2SmAh+eANnpQOxj4PaGEjn9dVFDhAnrgynrISRTD9b2TqhnUR22ZtpQUpAtkccAgMOHn2DkyNNITMwt1M6c8UDnzrWk2pXW+3eFGiNECCGEVDnZGUBcGBBxE3hxEYgOAgQywGfpW8+L4prIGpaCaOwXtUMSVBDNtCHSrgUDPT0YGFaDpa4q9NWFaGKujZbyJVfwfCs5ORPjx5/Dnj3BXMzUVB1qagql9pj5oUKIEEIIKQ/S4oD3j4CsNOC/w8CLC4A456dPezinFR4wKzxTtEN1yzqw1FNBor4qWqoJYWWgBh0VBQgExb049nMCAqIwYMBxhIcncDF39/rYvLkLtLTyvwu8tFAhRAghhJQ1sQh4+wC4vgJ4+c8PnyaTySFAXB/KggwkMRVcEdtCVsBwKccWdtYNYKWvBseaOuhspA5VIf9v+Tk5YixefB0LF16HSJQ7MkdNTQGbNnXGgAHWZV6QAVQIEUIIIaUrJwu4uxl47Jd7WesHPBObwljwCX/ldMcjcS08YjWRhdw7sLrZVEM1TUU41dTFtGoa0FJRwMISTL+kfPr0Gd26HUBAwFsu5uhoin37esLCQquQI0sXFUKEEEJISWAMeP8QeHIcSIwEMpKB8CvFPs3GnB5IY0r4BDUcE7WECHnjdPo3MUVnQ3X8bqyOmvpq0FCSL8lnUKo0NRUhJ5d7V5msrADe3q0wa5YzF+MLFUKEEEJIcWWnA0nvgHeBwMM9wJtbxTo8jqlDV5CMZ+LqWJvTCxfFjcGQVxBoKstDJGYY6WwG51p6qG+sDnXFilP05EdWVgZ79/ZEr16HsWlTZzRrZsJ3SgCoECKEEEIKl/weiP4PiPkPuLIYkFUARFnFPs1TsRlGZ0/AG2YotU9PTQjHGjpobqmDFjV1YaKlxMt4mZJ07VoElJTk0aSJMRczM9PEgwcjytVzo0KIEEII+VpKDBB+FXh9AwjaJ73/O0VQgrI5Tny2walMW0QyAyRCBTn5vN3WM1KHnpoQY9rUhIO5VrkqDn5GVpYI8+ZdwbJlt2BhoYWgoJFQU8ubn7q8PU8qhAghhFRtSe9yBzM/9MldZ6uoVPSQaWiH92kCXMpuiD/fWeeO5ylkkfSRrSzRqpYe7M21IJQrvTl6+BIaGgcPDz88fBgNAAgPT8DmzQ/w++8teM6sYFQIEUIIqVo+hgG31gJxL4C394p+XKsZECvr4D+ZujgZo42LT2Lx7kl6oYdoKsujj70JPJqawUJX5efyLscYY9i+/SEmTjyP9PTcuY/k5WWweHFbTJniyHN2haNCiBBCSOX36RVwfGTuultFoVsbqGYLkUVrvNJpiWfxwL/PP+Bk0HsAyf//kqahJI96RuqY2bkOrE00Syj58u3jxzSMGHEKJ0+GcrHatXXg69sbdnZGPGZWNFQIEUIIqXw+xwNB+4F/FwM5hffafCFuNx+vjLrC76UId8M/IfhBEkT3GICgAo+xq66JbBHDL/YmcHcwhWIpLklRHl248BKDB59ETEwqFxs1yh6rVrlCWbli3OVGhRAhhJDKI+4FcHgQ8OHp99t2/BNh5p64/Pwj7oR/wr2L8UjPDvvuYTYmGrAyUMPcbvUq/C3tPyM2NhVuboeQkZF7KUxXVxk7d3ZHt261ec6seKgQIoQQUnFlJAHPzwB3/gJiQgpvq6CGlPbLcSLHEc9iUvA0MBlBJ2589yF0VYXo09gEplrK6OdgChmZ8nXXE18MDFTx55/tMHHiBbi61sDu3W4wNFTlO61io0KIEEJIxRN0ADgx6rvNxHZeuKXdC9cS9XHr1SeE+iVDzJ4U2L5dHX00tdRGA2MN2JpqQUmhal3qKoxYzCASiSH/1eW/ceOawsREHT171q2wBSIVQoQQQiqG5PfAxTnA42PfbXrdehn2JjdCYHAK4tPSALzOt525jjLa1zNAQxNNdLM2Kndz3JQX0dEpGDz4JBo1MsCyZe25uIyMAL171+Mxs59HhRAhhJDyJzs9d16ffxcDQlUg+V2hzV/rtMJBjeE4/V4V7xLTgXsA8EmqXR1DNTiYa6N7o2qopa8KTWWF0sm/Ejl58jmGDfPHp0/p+OefV3B1rYm2bS34TqvEUCFECCGk/MhOBw56AK/+zYtl5j/J4Qe1eujzaRTeiHWBd8j9guQdYqpCOTSvoQPX+oZoaaULfTXFUku9sklLy8KUKRexdWsgFzMwqHhjgL6HCiFCCCHlw8U5wO0N3222EMPhm+GI9Iz8ixobU02Y6yhjmJMFGhpr0OWuHxAY+B4eHn4IC8vrVevRozZ27OgOXV1lHjMreVQIEUII4U9WGvBgZ24R9A2xjAJuV/PCpk/2CEhQL/AUaopyqKmvihkd68DGVLPKzeVTkkQiMVauvI05c64gJ0cMAFBWlsfata4YPtyuUhaVVAgRQggpezlZwLbWwIf87+CalT0MvqJ2wMv8D3eqqYtmltpwrqUHaxPq9SkJcXGf0afPEVy9GsHF7O2N4OvbG1ZWOvwlVsqoECKEEFI2GAOi7gFnpxQ4589jsTm6Zi0GIFnYWOqqgAHwbFodnk3N6Lb2UqChIURqahYAQCAAZsxwwvz5raFQyV9rKoQIIYSUrpwsYG1DIDWmwCYHc1pjRY47PkGDizUwVodbI2N0bGAIE63KNS6lPJKXl8X+/b3g5nYQmzd3QatW5nynVCaoECKEEFLyGMsd+HxzNZCeUGCzP7IHYqeoE7dtbaKB7jbVMLC5GYRylbsngm8BAVFQVpaHjY0hF7Oy0sHjx79V2MkRfwQVQoQQQkqOWARsdADiXxXarHfmPASy2qimoYjx9ibo09gUptrU61MWcnLEWLz4OhYuvA4rKx08ePCrxAKpVakIAqgQIoQQUhLE4tzLX8lvC2wyL9sLe0SuqKGngpo1VHG8VQ3YVtcqwyRJeHgCBgzwQ0BA7s/p2bM4/PXXfUyd6shzZvyhQogQQsiPS4lF9r6+kI8Nynf3K7ERumUtRqaMEmZ1rouHtsbQVqHZnMsaYwx79/6HsWPPIiUld0C0rKwA8+a1wsSJzXjOjl9UCBFCCCmerM8Q+fSA7Nt7AAD5fJocFbXE1OxR6NzQEJsdqqOVlV7Z5kg4CQnpGDXqDA4fzpuqoEYNLezb1wvNmpnwmFn5QIUQIYSQ72MMn5+chfjCXKimvEJBw5jX5fTCZ8ffwQAEt6kJDaX8yiRSVq5ejcDAgcfx9m0yFxsypBHWresINTUhj5mVH1QIEUIIKVBaeiZSdveBYew1FDSUWcwE8NcfCX3XaZhQU7dM8yMFi45OgavrPmRliQAAWlqK2Lq1K/r0qc9zZuULFUKEEEIkZGSLcOPeA+jcmAu7jLtQKaBdkEoLvOuwA50aGsGtit1pVBEYGalh3rxWmD37X7RpYw4fn54wMSl4qZKqigohQgghAID7rz/h2sUTaPfuL7SXyX9ti9eK9RBnNx42bd3RSE4Gjco2RVIIxhjEYgZZWRkuNn16C5iaqsPT07rK3RZfVALGGOM7ibKUnJwMDQ0NJCUlQV2dKmNCSNX2NuEzLj6JhdKtP9E//WCB7eINWkBrxHEI5GhcSXn08WMaRow4BVtbQ8yb15rvdEpFab1/U48QIYRUQR9TMrH03DM4h8zCUNlbBbbL+WUP5Or3gDYtalpuXbjwEoMHn0RMTCpOnw5Dhw410Ly5Kd9pVRhUCBFCSBWRIxLj+ouPOBr4Fs2e/4nVsheR3+1fTEUPgi6rgLrdIUcFULmVkZGDmTMvYe3au1xMS0uJmyeIFA0VQoQQUsklpGVhx81w/PMkBg6fTuIv+Z35F0BCdQhmREJAxU+5FxISC09PP4SEfOBirq41sHu3GwwNVXnMrOKhQogQQiqp+LQsHLv2EOJ7W2Ereo1pso/yn/1QUQOYFg6BLL0llHdiMcOGDXcxffolZGbm3hYvFMpi+fL2GDu2CQ2I/gH0W08IIZXM4zcf8O7wFLim+WPEl2B+MyDW7gy47wNkaJX3iuDTp8/w9PTDhQt5C9o2bKgPX9/eaNBAn8fMKjYqhAghpBJIzsjGiXsvYHRjNtpn/4sGhTVW1gVG3QDUq5VVeqQEqKgo4N27FG570qRmWLKkHRQV6a38Z9CrRwghFVhGtgg7rzxB71vdMUiQUHDD2l0A10WARnWALoFVSIqKcvD17YUePQ5iy5au6NChBt8pVQo0jxAhhFRAcSkZOHHmFNo/nwMzxOTbJqf+L5DrtZUKnwoqMPA9VFQUUKeO5LIlOTliyMnJFHBU5UXzCBFCCEHGpzeIPjQZFh8uYXgBbViTXyHosBhycgplmhspGSKRGCtX3sacOVfQoIE+7twZBqEw7+26KhZBpYkKIUIIKe9SP0J0aQFkg/ZCEYBFQe1sPICem0H3DVVcUVFJGDjwOK5dewMACAqKwV9/3cekSc15zqzyokKIEELKo5wsIGAj2O0NEKTH53vTFwCIFNQh298XsHAu0/RIyTt8+AlGjjyNxMQMAIBAAMyY4YQxY5rwnFnlRoUQIYSUJ5mpwInRwDN/ACiwd+dN0wUwcx0PWRm6TFLRJSdnYvz4c9izJ5iLmZqqY+/enmjVypy/xKoIKoQIIaS8uDAbCNiY765XYiO8kTVDfKct+KWJBczKODVSOgICojBgwHGEh+fd8efuXh+bN3eBlpYSj5lVHVQIEUIInxgDTo4Bgvbnu/u8yAHL5H9DtxYNMMmlFi1/UYm8e5eM1q33ICsrd4ZoNTUFbNrUGQMGWNPPuQxRIUQIIXx5fR3Y0y3fXYdyWmN6zq/4o0d9XGluXrZ5kTJhbKyOqVObY8mSm3B0NMW+fT1hYaHFd1pVDhVChBBSlnIygbNTgYc++e6OEBugbdYqTO9UD48am0JLhW6Bryy+TNv3dW/P/PmtUb26BoYNs6Pb4nlChRAhhJS2zBTg5Fjg6YkCm7xluhiq8Tfcm5jhhaM5ZGnxzEolISEdo0adgYNDNUyd6sjF5eVlMXJkYx4zI1QIEUJIaUmJAXZ1BuJfFdpssd5KOLd3w0UrvTJKjJSlq1cjMHDgcbx9m4zjx5+hXTsL2Noa8Z0W+T8qhAghpCQxBkTcKHDszxc+Oe1xyXwKZndtgNmGamWUHClLWVkieHtfwfLlt/BlMStVVQXExKTymxiRQIUQIYSUhOx0IOQI4D+uwCbbczpjRY47LAy0saqvDQYZa5RhgqQshYbGwcPDDw8fRnOxNm3M4ePTEyYmtM5leUKFECGE/IzbG4GLswtt8mvWJFwUOwAAxrWtifHtakFelgbGVkaMMWzbFohJky4gPT0HACAvL4PFi9tiyhRHyNDYr3KHCiFCCCmurDTg/g7gH+8Cm+QwGfTPmoP7rA4AwEhDESfHtoC+mmJZZUnKWHx8OoYMOQl//1AuVru2Dnx9e8POjsYElVdUCBFCSHEcHwUEHyhw9z1xbfye/SsiWN4b36Ffm6GppU5ZZEd4JBTK4vnzOG579OjGWLmyA5SV5XnMinwPFUKEEPI9jAFBvsDJ3/LdnS2Qh3X6VqQjr7dHRgAs622NX+xNaJbgKkJFRQH79/dCjx4HsWVLF3TrVpvvlEgRUCFECCGFSXgD/NUMyP4stStSvw06Rg7E568KIAVZGYxtWxNj29Sk8SCVXEhILFRUFGBpmTcbdOPG1RAePh5CIb29VhT0kyKEkPxkJAE7XIC4sHx399I8jIeRORKxYU4WmNqhNpQUZMsiQ8ITsZhhw4a7mD79EmxtjXDjxhCJWaGpCKpY6KdFCCHfenYaOOSZ765fdX1w8a0cEJNXBDnW0MEa90YwUKeB0JVddHQKBg8+iYsXcyfJvHPnLTZvvo9x45rynBn5Ubzfv7lp0yaYm5tDUVERTZs2xb179wptv3btWtSuXRtKSkowNTXFpEmTkJGRUUbZEkIqteRo4LBXvkXQbJUFMM/wzS2C/s9SVwV7hjaB74hmVARVASdPPkfDhpu5IggAJk1qhhEj7HnMivwsXnuEDh06hMmTJ2PLli1o2rQp1q5dC1dXV4SGhkJfX1+qva+vL2bMmIGdO3fC0dERYWFhGDx4MAQCAVavXs3DMyCEVAqM5S6Cemq81K4rFlMw5Jk98NXnLV1VITyaVsfEdrVoHFAVkJaWhSlTLmLr1kAuZmSkit273dChQw0eMyMlQcC+LIfLg6ZNm8LBwQEbN24EAIjFYpiammLcuHGYMWOGVPuxY8fi2bNnuHz5MhebMmUK7t69i5s3bxbpMZOTk6GhoYGkpCSoq9PsnoRUeVmfgSXSc7yIZYUYrbwKFz5qczETLSUscmuAVlZ6dCdYFREY+B4eHn4IC/vExdzc6mD79m7Q1VXmMbOqp7Tev3m7NJaVlYXAwEC4uLjkJSMjAxcXFwQEBOR7jKOjIwIDA7nLZ+Hh4Th79iw6d+5c4ONkZmYiOTlZ4osQQgAAH57nWwQdr7UUDbP3SBRBHesb4srU1mhdW5+KoCoiKioJjo47uSJIWVke27d3g59fXyqCKhHeLo3FxcVBJBLBwMBAIm5gYIDnz5/ne4yHhwfi4uLg5OQExhhycnIwatQozJo1q8DHWbp0KRYsWFCiuRNCKoHP8cBf0gNc+2kfxp2QHABiLjanS10Md7Ysw+RIeWBqqoHffmuMtWvvwt7eCL6+vWFlRRNjVja8D5YujqtXr2LJkiX466+/8PDhQ/j5+eHMmTNYuHBhgcfMnDkTSUlJ3FdUVFQZZkwIKXcYA/5dBCy3kAi/0XFC7ZyDuPM+724wSz0VPJrbnoqgKuTb0SJLl7pg9eoOuH17GBVBlRRvPUK6urqQlZVFbGysRDw2NhaGhob5HjN37lwMHDgQw4cPBwA0bNgQaWlp+PXXXzF79mzIyEjXdUKhEEKhsOSfACGkYhFlA5cXALc3SO06K98Bv70bjK97gbYPaoz29Qyk2pLKKTk5E+PHn0OTJsb47TcHLq6oKIdJk5rzmBkpbbz1CCkoKMDe3l5i4LNYLMbly5fRvHn+v3SfP3+WKnZkZXMnLuNxzDchpDzLSgP+XQws1M23CDolaobfUgZz280stXF3VjsqgqqQgIAoNGq0BXv2BGPKlIt49uwj3ymRMsTr7fOTJ0+Gl5cXGjdujCZNmmDt2rVIS0vDkCFDAACDBg2CsbExli5dCgDo1q0bVq9eDVtbWzRt2hQvX77E3Llz0a1bN64gIoQQAEDye+DyHwUukHoLNhidMRbJUAEAqArlsLKPDTo2yL9HmlQ+OTliLFp0HYsWXYdIlPthWl5eBq9eJaBuXT2esyNlhddCyN3dHR8/foS3tzdiYmLQqFEjnD9/nhtAHRkZKdEDNGfOHAgEAsyZMwfv3r2Dnp4eunXrhsWLF/P1FAgh5Y0oB9jsCMSF5rv7sUxtdP3sDSDvzq+etsZY/os15GUr1LBJ8hPCwxMwYIAfAgLecjFHR1Ps29cTFhZahRxJKhte5xHiA80jREglJhYBf5oBWSlSu64KW2NCkgeSoMrFahuoYXIHK7jWp16gqoIxBh+fYIwdew6pqVkAAFlZAby9W2HWLGeJNcNI+VJa79+01hghpHLISgOWVJMKX7SYjnFh1sjMyOsB0lcTYveQJqhXjT4MVSWJiRkYOfI0Dh9+wsUsLbWwf38vNGtmwmNmhE9UCBFCKr4Ls4GAjRKhRI16cE1fiNhnmRLxLtZG2NDPlpbGqIIEAuDu3bxLYYMHN8L69R2hpkZ3FldlVAgRQiouUQ6wuwsQdUdqV6PYOQDyiqCWVnpY3tsahhq0OGpVpaGhiL17e6JXr8P466/O6NOnPt8pkXKACiFCSMV0ewNwcY5UeHl2X/wlcuO2G5tpwbtbPVibaJZdbqRcCA2Ng4qKAkxM8i6BOjubISJiAlRUFHjMjJQnVAgRQiqev5oDH55KhS0z9kH8/+nR5GUFWNrLGr/Y09iPqoYxhm3bAjFp0gU0a2aCS5cGSVwKpSKIfI2GxxNCKg5RNrBAW6oIOi9ygHmGL1cEudTVx5MFHakIqoI+fkyDm9shjBp1BunpObhyJQLbtgXynRYpx6hHiBBSMWSnA4ulb3O3y9iCeORe+uhmUw3TOtRGdR1aGbwqunDhJQYPPomYmFQuNmqUPQYNsuExK1LeUSFECCn/Li0Abq6WCltk7AODDFQUZHF+YkuYalMBVBVlZORg5sxLWLv2LhfT1VXGzp3d0a1bbR4zIxUBFUKEkPLr3nbg7FSp8D8iO4zIzo33tjPBqr70ib+qCgmJhaenH0JCPnAxV9ca2L3bDYaGqoUcSUguKoQIIeVPTAiwxSnfXbOzh2K/yAVqinJY2KMB3GyNyzg5Ul68eZMIB4ftyMwUAQCEQlksX94eY8c2oXmiSJFRIUQIKV9SYoAdLvnuqpOxCxkQormlDjZ62EJHlSbCq8rMzDQxaJANtm9/iIYN9eHr2xsNGujznRapYKgQIoSUD+kJwGYnIPmtRPiRuCYGZM1EGpQAALuHOKB1bXqzI7nWrHGFmZkGpkxxhKIivaWR4qPfGkII/94GAjvaSoUHZU3HdXHe+J/7s12gR8shVElpaVmYMuUimjUzweDBjbi4iooCZs9uyV9ipMKjQogQwq83AcCujlLhedleXBHUqYEhNnrYQZbGfVRJgYHv4enph9DQT9i/PwTOztVRo4Y232mRSoIKIUIIP3IygYOewMt/JMLrc9ywOqcvAEBdUQ4r+tjAtb70/EGk8hOJxFi58jbmzLmCnBwxAEAsZnj8+AMVQqTEUCFECCl7wYeA479KhSdk/YaT4ry7xZb1tqYiqIqKikrCwIHHce3aGy5mb28EX9/esLLS4TEzUtlQIUQIKTvxr4F9vYD4cKldrTNXIYIZAQB0VYW4PLkVNJTlyzpDUg4cPvwEI0eeRmJiBgBAIABmzHDC/PmtoaAgy3N2pLKhQogQUvoYA06NBx76SO0KV6iNtsneAHLH/3g1N8OCHg3KOEFSHqSkZGLcuHPYsyeYi5maqmPv3p5o1cqcv8RIpUaFECGkdAUdAE6MyndXL4UteJiszm0PoiKoSsvMFOHixVfctrt7fWze3AVaWko8ZkUqO1p9nhBSOsRi4ED/fIugPYazYJ6xnyuClORlsa5fI/xBRVCVpqurjD173KCuLoSPjxsOHOhNRRApddQjRAgpeY+PAUeHSoVjzXug7/v+eBMh5mI2JhrYPMAe1TTpDa+qCQ9PgIqKPAwM8tYEa9++Bt68mQhNTUUeMyNVCfUIEUJK1o1V+RZBSxpdQvNQd7xJziuCvJqb4cSYFlQEVTGMMezZEwQbmy0YOtQfjDGJ/VQEkbJEPUKEkJLx6l9gb0+pcLa8Oprk7EDCnbzVwc10lLF9UGNYGaiVZYakHEhISMeoUWdw+PATAMDZsy+wa1cQhg615TkzUlVRIUQI+TliMbDYABBlSe2aY3UK+0NSwFgOAEBBTgb9HUzh3a0+zRJdBV29GoGBA4/j7dtkLjZ4cCP06VOPx6xIVUeFECHkx70LBLZLrxEGAJNqX8bx4Fhu21hTCb4jmsJMR6WssiPlRFaWCN7eV7B8+S18uQqmpaWIrVu7ok+f+vwmR6o8KoQIIT/mfVC+RdD7Ma/Qe0cwor8qgvo3McXCHg0gJ0vDEqua58/j4Onph4cPo7lYmzbm8PHpCRMT9UKOJKRsUCFECCm+l5eAfb0lQszKFbvNlmH5+gdIzxZx8eW9rdHXwbSsMyTlQHh4AuzstiI9PffSqLy8DBYvbospUxwhQ5dGSTlBhRAhpOiy04GNTYCkSInwe/cL+P2WADdPPeViGkry8B3RFPWraZR1lqScsLTUQq9edbF/fwhq19aBr29v2NkZ8Z0WIRKoECKEfB9jwIOdwJnJUrsuOh3F1ENJSM7I4WK9bI0xp2s9aKsolGWWpBzatKkzzMw0MHt2SyjT2nGkHBKwbydwKIaMjAwoKlas+R6Sk5OhoaGBpKQkqKvT9WlCimSzExAbIhX+VWcXLr4TSsQ2etiiq3W1ssqMlBMZGTmYOfMSHB1NaQA0KRWl9f5d7JGLYrEYCxcuhLGxMVRVVREenruK9Ny5c/H333+XWGKEkHLg0ytgvoZUEZSlUxctlU9IFEG1DdRwe0ZbKoKqoJCQWDRpsh1r197Fr7+eRlRUEt8pEVJkxS6EFi1ahN27d2P58uVQUMjr9m7QoAF27NhRoskRQnjyNjC3ANpgJ7VrjuURWL2bi8j4zwAAbRUFLHRrgAuTWtIM0VWMWMywbt0dODhsR0hI7oSZ6enZePDgPc+ZEVJ0xR4j5OPjg23btqFdu3YYNSpvMUUbGxs8f/68RJMjhPDgylLg2p/57mosexRxT/MmTqxjqIbdQ5rAUKNiXSInPy86OgVDhpzEhQt5q8U3bKgPX9/eaNBAn8fMCCmeYhdC7969Q82aNaXiYrEY2dnZJZIUIYQH7x4C29vku+uE5XxMeVYTIpZXBHWzqYYVv1hDUV62jBIk5cXJk88xfPgpxMV95mKTJjXDkiXtoKhI9+CQiqXYv7H16tXDjRs3YGZmJhE/evQobG1prRhCKqRry4Eri6XCUd0Oou9FBUQ/zeBiSvKy2D3EAU0tdcoyQ1IOpKVlYcqUi9i6NZCLGRmpYvduN3ToUIPHzAj5ccUuhLy9veHl5YV3795BLBbDz88PoaGh8PHxwenTp0sjR0JIabq3XboIatgXS+RGY7dfLLJEeUWQV3MzzO5SDwpyNEN0VZScnIljx55x225udbB9ezfo6irzmBUhP+eHbp+/ceMG/vjjDwQHByM1NRV2dnbw9vZGhw4dSiPHEkW3zxPyf4wBO1yAdw8kw6NvY9YtEQ7ci+Ji6opy2DLQHo41dMs6S1LOnDz5HB4efli3riOGDbOFQEAzRJOyUVrv3z81j1BFRIUQIQCurQCuLJIKvxjyBFNPRSD4bd7tz33sTbDQrQGNBaqCoqKSoKKiAG1tybsBP3xIg74+LZ5Lyla5mUfI0tISnz59koonJibC0tKyRJIihJSSpHe5t8XnUwQdcDiKPnueShRBE11qYUUfGyqCqqDDh5/A2noLRo48jW8/L1MRRCqTYhdCEREREIlEUvHMzEy8e/euRJIihJSChDfAmnpSYbFQAx01T2HmjSwkfs6989NYUwlbBthhootVWWdJeJacnInBg0/A3f0oEhMzcPToU/j6Ss8qTkhlUeTB0v7+/tz3Fy5cgIZG3kKKIpEIly9fhrm5eYkmRwgpIZf/AG6skgoHDX2N8QeDERmTwsW6NDTC8l+soSKk26CrmoCAKHh6+uH160Qu5u5eH5071+IvKUJKWZH/0rm5uQEABAIBvLy8JPbJy8vD3Nwcq1ZJ/6ElhPAoOhjY2lIqzJqPx0p4YsuWOxCJ8y577BrigDa1aTK8qiYnR4zFi69j4cLrEIlyfx/U1BSwaVNnDBhgTQOiSaVW5EJILBYDACwsLHD//n3o6tLdI4SUay8uAft7S4U/ddiI0SE1ce913ozA+mpCHPy1GSz1VMsyQ1IOhIcnYMAAPwQEvOVijo6m2LevJywstHjMjJCyUey+79evX5dGHoSQknR6MvBAehHkQ07nMd0/HkA8F3OupYutA+2hrECXwqqaly/jYWe3FSkpuTOGy8oK4O3dCrNmOUOO5ooiVcQP/eVLS0vDtWvXEBkZiaysLIl948ePL5HECCE/QCwGFuoCTPKGhv86HsHEWwoIvxQvEV/+izX6NjYtywxJOVKjhhbatbPEiRPPYWmphf37e6FZMxO+0yKkTBW7EHr06BE6d+6Mz58/Iy0tDdra2oiLi4OysjL09fWpECKEL+FXAZ8eUuG99Xdgvn8OROK8tQAbGKvjwIhmUFOUL8MESXkjEAiwfXs3mJlpYOHCNlBTE/KdEiFlrth9n5MmTUK3bt2QkJAAJSUl3LlzB2/evIG9vT1WrlxZGjkSQr7n0b58i6CxNS9hbqAyNyDaUlcFp8c54fQ4ZyqCqpisLBFmzLiEM2fCJOK6uspYu7YjFUGkyip2IRQUFIQpU6ZARkYGsrKyyMzMhKmpKZYvX45Zs2aVRo6EkIIwBhwfDZwcIxEWmTTDlHrXcfrxBy7W1doIFye1RANjjW/PQiq50NA4NG/+N5Ytu4WhQ/0RG5vKd0qElBvFLoTk5eUhI5N7mL6+PiIjIwEAGhoaiIqKKuxQQkhJ+vAMWKAJBPtKhGMGB6BpzFQce5h3F1BPW2Ns9LCDnCwNgK1KGGPYuvUBbG234uHDaABAQkI6bt2iv9WEfFHsMUK2tra4f/8+atWqhVatWsHb2xtxcXHYu3cvGjRoUBo5EkK+9Wg/cPI3qbB/h5tY7Psecan/vwtIRoD53etjYDOzss6Q8OzjxzQMH34K/v6hXKx2bR34+vaGnZ0Rj5kRUr4UuxBasmQJUlJyZ6FdvHgxBg0ahNGjR6NWrVr4+2/p23UJISVsa8vciRK/wpR1sKDOKez2j5CIbxlgj/b1DMowOVIeXLjwEoMHn0RMTN4lsNGjG2Plyg5QVqaxYYR8jVafJ6Si+BwPLLeQCqd0/xtD7lbDgzcJXKyOoRp2eDWGiZZyWWZIeJaRkYOZMy9h7dq7XExXVxk7d3ZHt261ecyMkJ9XblafL8jDhw/RtWvXkjodIeRr8eH5FkEB7U+g4WEliSKoobEGzk1wpiKoCvrwIQ27dgVx2x071kRIyGgqgggpRLEKoQsXLmDq1KmYNWsWwsPDAQDPnz+Hm5sbHBwcuGU4CCEl6MlxYL2tRIjJKmBZ0zvwPP1ZIr6ge32cGudEa0NVUdWra2Dz5i4QCmWxfn1HnD3rAUNDWjaFkMIU+dLY33//jREjRkBbWxsJCQnQ0dHB6tWrMW7cOLi7u2PChAmoW7duaef70+jSGKkwGAO2tZIaD/TJoAX6pP2O8Lg0Lla/mjq2DrSnXqAqJjo6BSoqClBXl5wDKCoqCaamNE0CqVx4vzS2bt06LFu2DHFxcTh8+DDi4uLw119/ISQkBFu2bKkQRRAhFUb4tdxb478pgs7X9Ib9mzESRdDQFhY4NtqRiqAq5uTJ57C23oLx489J7aMiiJCiK3KPkIqKCp48eQJzc3MwxiAUCnHlyhW0aNGitHMsUdQjRMq9+zuAM1OkwuMN98M/QvKS195hTeBcS6+sMiPlQFpaFqZMuYitWwO52NGjfdC7dz0esyKk9JXW+3eRb59PT0+HsnLuJ06BQAChUAgjI5qLgpASc3crcO53qfCnhsPRNawzoiMyuJhzLV3sHOwAeZogsUoJDHwPDw8/hIV94mJubnXQqpU5f0kRUsEVax6hHTt2QFU1d+BdTk4Odu/eDV1dXYk2tOgqIcXEGLC/D/DyH6ldgV3Pof/xJGSJ8oog7671MNRJ+g4yUnmJRGKsXHkbc+ZcQU5O7k0pysryWLeuI4YNs6XB8YT8hCJfGjM3N//ufzaBQMDdTVZUmzZtwooVKxATEwMbGxts2LABTZo0KbB9YmIiZs+eDT8/P8THx8PMzAxr165F586di/R4dGmMlCsJb4B11vnumlLjNI49Sea2VYVy2OHVGM0sdcoqO1IOREUlYeDA47h27Q0Xs7c3gq9vb1hZ0e8CqTp4vzQWERFRYg/6xaFDhzB58mRs2bIFTZs2xdq1a+Hq6orQ0FDo6+tLtc/KykL79u2hr6+Po0ePwtjYGG/evIGmpmaJ50ZIqWMs3yIobXIEBvs+w/0neXMDqSjI4t+praCvpliWGRKehYV9QtOmO5CYmNsjKBAAM2Y4Yf781lBQkOU5O0IqB15nlm7atCkcHBywceNGAIBYLIapqSnGjRuHGTNmSLXfsmULVqxYgefPn0Ne/semiaceIVIupH0CVlhKxixaIrPvAdgtvYm0LBEXdq6liz1DmkBGhi5/VDViMUPnzvtx4cIrmJqqY+/enjQeiFRZvN8+X9KysrIQGBgIFxeXvGRkZODi4oKAgIB8j/H390fz5s0xZswYGBgYoEGDBliyZAlEIlG+7Qkpl8Ri6SKoQW+IBvpjuO8TiSJoagcr7B3WlIqgKkpGRoBdu3rg11/tEBw8ioogQkpBsRddLSlxcXEQiUQwMJBcENLAwADPnz/P95jw8HD8+++/8PT0xNmzZ/Hy5Uv89ttvyM7Oxrx58/I9JjMzE5mZmdx2cnJyvu0IKRPPTgGHBkjGrDohvds2DNwawC2VISsjwLp+jdDVuhoPSRI+5OSIsXjxdTg7m6Ft27zB8EZGati6tRuPmRFSufFWCP0IsVgMfX19bNu2DbKysrC3t8e7d++wYsWKAguhpUuXYsGCBWWcKSHfEOXkzhId+1gyrm6Mx622ouu8CxLhaa61qQiqQsLDEzBggB8CAt7C2FgN//03GtraSnynRUiVwNulMV1dXcjKyiI2NlYiHhsbC0NDw3yPMTIygpWVFWRl8wYJ1q1bFzExMcjKysr3mJkzZyIpKYn7ioqKKrknQUhRRN4FFupIF0FWnXCt6zV4bL8jEV7d1wajWtUowwQJXxhj8PEJRqNGWxAQ8BYAEBOTiitXXvOcGSFVxw8VQq9evcKcOXPQv39/fPjwAQBw7tw5PHnypMjnUFBQgL29PS5fvszFxGIxLl++jObNm+d7TIsWLfDy5UuJxV3DwsJgZGQEBQWFfI8RCoVQV1eX+CKkzIiygZ0dpMLJvz7ASNFUeO28h+SMHC6+Z2gT9LIzKcsMCU8SEtLRr98xeHmdQEpK7gc5S0st3Lw5lGaJJqQMFbsQunbtGho2bIi7d+/Cz88PqampAIDg4OACL08VZPLkydi+fTv27NmDZ8+eYfTo0UhLS8OQIUMAAIMGDcLMmTO59qNHj0Z8fDwmTJiAsLAwnDlzBkuWLMGYMWOK+zQIKV2MAbfWAwslJxxFmzl4+dtbtN35Bhee5PWGWhmo4sbvbdDKipbLqAquXo2AtfUWHD6c9+Fx8OBGCAoaiWbNqBAmpCwVe4zQjBkzsGjRIkyePBlqampcvG3bttxt8EXl7u6Ojx8/wtvbGzExMWjUqBHOnz/PDaCOjIyEjExerWZqaooLFy5g0qRJsLa2hrGxMSZMmIDp06cX92kQUno+hgGbHKTjXdfgilpXTNl6B/FpeZdyBzuaY163ejQ7cBWQlSXCvHlXsGzZLXyZuERTUxHbtnVFnz71+U2OkCqq2PMIqaqqIiQkBBYWFlBTU0NwcDAsLS0RERGBOnXqICMj4/sn4RHNI0RK1fOzwMH+UmGRRWtMVJiPU8HvuZiGkjz+8rRDi5q6Uu1J5RQengBr681IS8sGALRubQ4fHzdaLZ6QIig38whpamoiOjpaKv7o0SMYGxuXSFKEVEjBh6SLIG1LvB35HF0Tp0oUQcaaSjg3wZmKoCrG0lIL69Z1hLy8DJYvd8Hly4OoCCKEZ8W+NNavXz9Mnz4dR44cgUAggFgsxq1btzB16lQMGjSoNHIkpHxLeANsdABEmZJxz2MIkLHF6O2BSPyczYV/bWmJ6R3rQJYmSaz04uI+Q1lZHsrKeTPhDx1qi1atzFGzpjaPmRFCvih2j9CSJUtQp04dmJqaIjU1FfXq1UPLli3h6OiIOXPmlEaOhJRPYjGws2PuemHfFEGiyWHwfmqI/tvvSBRBa9xtMKtzXSqCqoALF16iYcPNmDbtokRcIBBQEURIOfLDa41FRkbi8ePHSE1Nha2tLWrVqlXSuZUKGiNESkTqB2Bl/r/zUX0vYPot4ParTxLx0+Oc0MCYLoNUdhkZOZg58xLWrr3LxU6f7o8uXax4zIqQio/31ee/uHnzJpycnFC9enVUr169xBIhpMJ4dho45Ckd77UdhzKaYubexxB/9fFisKM5ZnepC3lZ3uYvJWUkJCQWnp5+CAn5wMU6dqwJe3uaJZyQ8qrYf5nbtm0LCwsLzJo1C0+fPi2NnAgpv66vzLcISp7+EatjG2HOySdcEaSrKsT6/raY370+FUGVnFjMsG7dHTg4bOeKIKFQFuvXd8TZsx4wNFTlOUNCSEGK3SP0/v17HDx4EAcOHMCff/4Ja2treHp6on///jAxoYnASCUlFgN/aAP45krywON4rtIYv226jfC4NC7c284EC3rUh6qwQi3nR35AdHQKhgw5iQsXXnGxhg314evbGw0a6POYGSGkKH54jBAAvH79Gr6+vjhw4ACeP3+Oli1b4t9//y3J/EocjREiP+TCbCDgmwlDR9/Go8xq6PnXbYnwYEdzzO9Ok+NVBaGhcXBy2oW4uM9cbNKkZliypB0UFakIJqQkldb7908VQgAgEolw7tw5zJ07F//99x9EIlFJ5VYqqBAixba5heSCqXW7A+57cSX0A37b9xDp2Xm/87uGOKBNbeoFqCpEIjHatvXB9etvYGSkit273dChAy2YS0hpKDeDpb+4desW9u/fj6NHjyIjIwM9evTA0qVLSywxQniXmQos/WaSUAVVZP+yBxP2B+JsSIzEroCZbWGkoVSGCRK+ycrKYO/enpgz51+sXu0KXV1lvlMihBRTsXuEZs6ciYMHD+L9+/do3749PD090aNHDygrV4w/ANQjRIpElC29YCqAD+PDMXDPE4TGpnAxaxMN7B3aFBpfTZpHKh+RSIyVK2/D2dkMjo6mfKdDSJVTbnqErl+/jmnTpqFv377Q1aXlAUgltbenVChq3Dt47LiHqPh0LjbJxQrj2taEDE2QWKlFRSVh4MDjuHbtDSwsNBEUNArq6kK+0yKElIBiF0K3bt0qjTwIKT92dwUibuRtt5iAp/Wnou/6W0jNzOHCPkOboKWVHg8JkrJ0+PATjBx5GomJuQtKR0Qk4uLFV/jll3o8Z0YIKQlFKoT8/f3RqVMnyMvLw9/fv9C23bt3L5HECClzjAH7ekkWQXW64rbFeAz+6xaycsQAAC1leWweYI9mljo8JUrKQnJyJsaPP4c9e4K5mKmpOvbu7YlWrcz5S4wQUqKKNEZIRkYGMTEx0NfXh4xMwRPDCQQCumuMVEyMAavrAinREuH9ro/gfeo5RP+fJVFDSR7nJzrToOhKLiAgCgMGHEd4eAIXc3evj82bu0BLi372hPCB1zFCYrE43+8JqRQ+PAf+aioVntngKg6cfMZtK8jJ4Nq01tBUVijL7EgZyskRY/Hi61i48DpEotziV01NAZs2dcaAAdYQCGgsGCGVTbHn/ffx8UFmZqZUPCsrCz4+PiWSFCFl5unJfIugGQ2u48CD99x2T1tjPFngSkVQJffqVTyWLr3JFUGOjqYIDh6FgQNtqAgipJIqdiE0ZMgQJCUlScVTUlIwZMiQEkmKkDLx9CRweJBEiJk2xdhal3DwwVsAgKyMABPa1cIa90a0XlgVULu2LpYvbw9ZWQEWLGiNa9cGw8JCi++0CCGlqNh3jTHG8v1k9PbtW2hoaJRIUoSUurQ4qSIosacvxgfq4fpXK4f3tDXGpPZWZZ0dKSMJCelQVpaH8Ks14caNa4K2bS1onTBCqogiF0K2trYQCAQQCARo164d5OTyDhWJRHj9+jU6duxYKkkSUqI+vQI22EmEdjicxoojqcjM+cjFZneuixEtLcs6O1JGrl6NwMCBx9GvX32sWNGBiwsEAiqCCKlCilwIubm5AQCCgoLg6uoKVVVVbp+CggLMzc3Ru3fvEk+QkBLDGOA3Agg5IhHepj0NS24kS8RojqDKKytLhHnzrmDZsltgDFi5MgAdO9ZEu3ZU9BJSFRW5EJo3bx4AwNzcHO7u7lBUVCy1pAgpFQc9gdAzEqFNsgOw4r0tt22uo4wdXo1RU1+trLMjZSA0NA4eHn54+DBvmoQ2bcxRuzbNkk9IVVXsMUJeXl6lkQchpSvsolQRNE08DkcymgMAZARApwZG2ORpl9/RpIJjjGHbtkBMmnQB6em5s4PLy8tg8eK2mDLFkZZIIaQKK1IhpK2tjbCwMOjq6kJLS6vQ20jj4+NLLDlCSsSRwcCT4xKh+qL9SMvO/T02UBdi//Cm1AtUSX38mIbhw0/B3z+Ui9WurQNf396wszPiMTNCSHlQpEJozZo1UFNT476n+TRIhZCTCSySHvRqmbEPYuT9Du8e0oSKoEoqNDQOrVvvQUxMKhcbPboxVq7sAGVleR4zI4SUF0VaYqMyoSU2qgjGgMVGQE66RNgpcy3estziqJmlNnZ4OUBVWOwrxKSCyM4WoUWLnbh//z10dZWxc2d3dOtWm++0CCE/oLTev4s9Q9zDhw8REhLCbZ88eRJubm6YNWsWsrKySiwxQn5YZiqwpJpUEWSRsY8rgkY4W8B3eDMqgio5eXlZ7N/fC7161UVIyGgqggghUopdCI0cORJhYWEAgPDwcLi7u0NZWRlHjhzB77//XuIJElIsaZ+ApcZA9mcu9EpgCvMMX7D//7ofGNEMs7vUowGylYxYzLB+/V08eiS5cG6tWjo4dqwvDA1VCziSEFKVFbsQCgsLQ6NGjQAAR44cQatWreDr64vdu3fj2LFjJZ0fIUWX+hFYIT0XTLv0P7nvj//miOY1dMoyK1IGoqNT0LnzfkyYcB4eHn74/Dmb75QIIRVEsQshxhi3Av2lS5fQuXNnAICpqSni4uJKNjtCioIxYE93YGVNifChnNYwz/AFIICJlhLOjHeCbXVaN6qyOXnyOaytt+DChVcAgOfP43Du3AuesyKEVBTFHiDRuHFjLFq0CC4uLrh27Ro2b94MAHj9+jUMDAxKPEFCCiUWA2vqASmSl0N8ctrDOyd3EeCpHawwunVNyNKlsEolLS0LU6ZcxNatgVzMyEgVu3e7oUOHGjxmRgipSIpdCK1duxaenp44ceIEZs+ejZo1cz+FHz16FI6OjiWeICEFykzNHQ/0jYFZM3BDbA0A2D6oMdrXowK9sgkMfA8PDz+EhX3iYm5udbB9ezfo6irzmBkhpKIpsdvnMzIyICsrC3n58j03B90+XwmIxcCuTkDUHYnwY7E5umYtBiCAgqwMzk90hqUeDZCtTEQiMVasuI25c68gJyf3Er2ysjzWrnXF8OF2NMcZIZVYab1///C9w4GBgXj27BkAoF69erCzo6UJSBlgDPi7PfDugUQ4jqmja9YSbvvsBCcqgiqh58/jJIoge3sj+Pr2hpUVDYAnhPyYYhdCHz58gLu7O65duwZNTU0AQGJiItq0aYODBw9CT49W7Cal5HM8sNxCKrw8uy/+ErkBAHRVFXDj97ZQUpAt4+RIWahfXx8LF7bBrFmXMWOGE+bPbw0F+lkTQn5Cse8aGzduHFJTU/HkyRPEx8cjPj4ejx8/RnJyMsaPH18aORIChF+TKoIiZUxgnuHLFUHKCrK4Oq0NFUGVSEpKJtf788W0aY64d28ElixpR0UQIeSnFXuMkIaGBi5dugQHBweJ+L1799ChQwckJiaWZH4ljsYIVUAX5wK310uEEgQasE3fzG33sjXGqr42NEakEgkIiMKAAccxcKA15s9vzXc6hBCelZslNsRicb4DouXl5bn5hQgpEYwBPm5SRZCvsK9EETSypSVWuzeiIqiSyMkRY8GCq3B23oXw8AQsXHgdt29H8Z0WIaSSKvYYobZt22LChAk4cOAAqlWrBgB49+4dJk2ahHbt2pV4gqSKYgxYoCkVHq++Fv4f8laU/8vTDp0bGpVhYqQ0hYcnYMAAPwQEvOVizZqZwMiIBr4TQkpHsXuENm7ciOTkZJibm6NGjRqoUaMGLCwskJycjA0bNpRGjqSqyc7ItwjqpesvUQQt7FGfiqBKgjEGH59gNGq0hSuCZGUFWLCgNa5dGwwLC5oRnBBSOordI2RqaoqHDx/i8uXL3O3zdevWhYuLS4knR6ogxoAtTlJh8wxf4G0qt738F2v0bWxalpmRUpKQkI7Ro8/g0KEnXMzSUgv79/dCs2YmPGZGCKkKilUIHTp0CP7+/sjKykK7du0wbty40sqLVFX/eAOf8taJEimoo7uqL/A+mYtNc61NRVAlERoah/bt9yIqKu/nO3hwI6xf3xFqakIeMyOEVBVFLoQ2b96MMWPGoFatWlBSUoKfnx9evXqFFStWlGZ+pCr5x1tiYHSqrg0avJ0OJOe9Sa7qY4Pe9tRLUFmYmWlCU1MRUVHJ0NJSxNatXdGnT32+0yKEVCFFvn2+fv366Nu3L+bNmwcA2LdvH0aOHIm0tLRSTbCk0e3z5VBOFrBIciLOdBUT1P20DEDenWCbPe3QicYEVTqPH3/A9OmXsHVrV5iY0P9JQkj+Suv9u8iFkJKSEp49ewZzc3MAubfRKykpISIiAkZGFefNiQqhciYxCljbQCpsnbUTyWJFbvvK1Naw0FUpy8xICWOMYfv2h3Byqo569WgGekJI8fC+1lhmZiZUVPLeiGRkZKCgoID09PQSS4ZUMVmfpYqgNIEK6qdv57a1lOURMLMdFOVpBuGK7OPHNAwffgr+/qGwsTHA3bvDIRT+8FKHhBBSYor1l2ju3LlQVlbmtrOysrB48WJoaGhwsdWrV5dcdqRyW1FDYtNfYyDGx3bitt0bm2KhWwMoyBV7lgdSjly48BKDB59ETEzuXX/BwbE4fToMvXvX4zkzQggpRiHUsmVLhIaGSsQcHR0RHh7ObdPMvqTIrq8Esj9zmweVPTDjqyJohLMFZnehN8qKLCMjBzNmXMK6dXe5mK6uMnbu7I5u3WrzmBkhhOQpciF09erVUkyDVCkfw4B/F3KbyXI6mBHfldte426DnrZ0Z1hFFhISCw8PPzx+/IGLubrWwO7dbjA0pFmiCSHlB12kJ2Ur5jGwpYVEqHnqcu77oS0sqAiqwMRihg0b7mL69EvIzBQBAIRCWSxf3h5jxzaBjAz1GhNCyhcqhEjZiX0qVQS5Zf6BNCgBAOZ2rYdhThZ8ZEZKSEhILCZPvgixOPdm1IYN9eHr2xsNGuh/50hCCOEHjUIlZePJCWBzc4nQtOxfEcRqAgC62VSjIqgSsLExxKxZuUukTJrUDPfujaAiiBBSrlGPECl9d7YA56dLhIZkTcMVsW3u9y3MMa8bzSZcEX3+nA1FRTmJS17e3q3QoUMNODub8ZgZIYQUDfUIkdJ1aoJUEeSZNZMrgmZ0qkNFUAUVGPgetrZbsWrVbYm4vLwsFUGEkArjhwqhGzduYMCAAWjevDnevXsHANi7dy9u3rxZosmRCkwsAhZXAwJ3S4S7ZC7BLXFDAMCeoU0wqlWNfA4m5ZlIJMayZTfRrNnfCAv7hNmz/8XDh9F8p0UIIT+k2IXQsWPH4OrqCiUlJTx69AiZmZkAgKSkJCxZsqTEEyQVUEwI8Ic2kJ23Dt1TsRkaZuzAE2YOALg0uRVaWdEyCxVNVFQS2rXzwYwZl5GTIwYAWFsbQFVVgefMCCHkxxS7EFq0aBG2bNmC7du3Q15enou3aNECDx8+LNHkSAWUngBscZIIPRGboXPWUqRAGTammni+sCNq6tNcMhXN4cNPYG29BdeuvQEACATAzJlOuH17GKysdHjOjhBCfkyxB0uHhoaiZcuWUnENDQ0kJiaWRE6kohKLgbU2EqG9OS6YmzMUQO6gaO+u9WgG8gomOTkT48efw549wVzM1FQde/f2RKtW5vwlRgghJaDYhZChoSFevnzJrUL/xc2bN2FpaVlSeZGKhjFgqbHEshnjs8bAX5w7b9ClyS1RU1+Nr+zIDwoNjUPnzr4ID0/gYu7u9bFlS1doairymBkhhJSMYl8aGzFiBCZMmIC7d+9CIBDg/fv32L9/P6ZOnYrRo0eXRo6kvMv6DKyoKVEEPRObwl/cArqqCvh3SisqgiooExN1yP1/0Vs1NQX4+LjhwIHeVAQRQiqNYhdCM2bMgIeHB9q1a4fU1FS0bNkSw4cPx8iRIzFu3LgfSmLTpk0wNzeHoqIimjZtinv37hXpuIMHD0IgEMDNze2HHpeUgM/xwBIj4HOcRLhT1jK0r2eAS5NbwVKPxgNVVCoqCvD17YXWrc0RHDwKAwfa0KVNQkilImCMsR85MCsrCy9fvkRqairq1asHVdUfe7M7dOgQBg0ahC1btqBp06ZYu3Ytjhw5gtDQUOjrFzwjbUREBJycnGBpaQltbW2cOHGiSI+XnJwMDQ0NJCUlQV1d/YdyJv/HGLBAUyKUxWRhlemD6R3rYnRrujW+ImGMYe/e/9CihSlq1NCW2kcFECGET6X1/v3DEyoqKCigXr16aNKkyQ8XQQCwevVqjBgxAkOGDEG9evWwZcsWKCsrY+fOnQUeIxKJ4OnpiQULFtC4JD7t/0Vi01/UHPWy98F3RDMqgiqYhIR09Ot3DF5eJ+Dp6YfsbJHEfiqCCCGVVbEHS7dp06bQP4r//vtvkc+VlZWFwMBAzJw5k4vJyMjAxcUFAQEBBR73xx9/QF9fH8OGDcONGzcKfYzMzExuriMgt6IkJSD0PPDykkRopmAiToxpjgbGGjwlRX7E1asRGDjwON6+zf2/cffuO5w+HYaePevynBkhhJS+YhdCjRo1ktjOzs5GUFAQHj9+DC8vr2KdKy4uDiKRCAYGBhJxAwMDPH/+PN9jbt68ib///htBQUFFeoylS5diwYIFxcqLfMfneOCAu0TIPMMXp8dREVSRZGWJ4O19BcuX38KXC+RaWorYtq0bFUGEkCqj2IXQmjVr8o3Pnz8fqampP51QYVJSUjBw4EBs374durq6RTpm5syZmDx5MrednJwMU1PT0kqxalguuUq8fcZmnB3vjHrVaMxVRREaGgcPDz+JpTHatDGHj09PmJjQz5EQUnWU2OrzAwYMQJMmTbBy5coiH6OrqwtZWVnExsZKxGNjY2FoaCjV/tWrV4iIiEC3bt24mFicO82/nJwcQkNDUaOG5NgUoVAIoVBYnKdCCsG2tcHXF0YnZP2GPzzaUBFUQTDGsG1bICZNuoD09BwAgLy8DBYvbospUxwlVpEnhJCqoMQKoYCAACgqFm9uEQUFBdjb2+Py5cvcLfBisRiXL1/G2LFjpdrXqVMHISEhErE5c+YgJSUF69ato56eUsZ2doTgfd4yKvfFVjBwGoQu1kY8ZkWK49GjGIwadYbbrl1bB76+vWFnRz9DQkjVVOxCqFevXhLbjDFER0fjwYMHmDt3brETmDx5Mry8vNC4cWM0adIEa9euRVpaGoYMGQIAGDRoEIyNjbF06VIoKiqiQYMGEsdramoCgFSclCx2ZQkEkZID2O+22o9Z7ax4yoj8CDs7I0ye3AyrV9/B6NGNsXJlBygry3//QEIIqaSKXQhpaEgOhpWRkUHt2rXxxx9/oEOHDsVOwN3dHR8/foS3tzdiYmLQqFEjnD9/nhtAHRkZCRmZH77Ln5QAdn42BHc2SsT+anUfY9tQEVTeZWbmQEFBVuJOzyVL2qFjx5po356mOCCEkGJNqCgSiXDr1i00bNgQWlpapZlXqaEJFYvp8THg6FBu8wPTxPE2lzGydU0ekyJFERISCw8PP4we3Ri//ebAdzqEEPJTysWEirKysujQoQOtMl9VZKcj5+R4idAh54tUBJVzYjHDunV34OCwHY8ff8CUKRfx9OlHvtMihJByqdiXxho0aIDw8HBYWFh8vzGpuHIygcWGEr8gk839sNqlNm8pke+Ljk7BkCEnceHCKy5Wq5Z2IUcQQkjVVuzBN4sWLcLUqVNx+vRpREdHIzk5WeKLVAI5mcAiyXXeVqhMwZ8D2vCUECmKkyefw9p6i0QRNGlSM9y7NwL16unxmBkhhJRfRR4j9Mcff2DKlClQU1PLO/irAZhfFmUUiUT5HV5u0Bih7xNtag7Zj0+57Vimic/jnsJCV4XHrEhB0tKyMGXKRWzdGsjFjIxUsXu3Gzp0oAHRhJDKobTev4tcCMnKyiI6OhrPnj0rtF2rVq1KJLHSQoVQ4XJOT4Pcg23c9r9iO9Sbcg6GGsWbI4qUjbCwT+jW7QDCwj5xMTe3Oti+vRt0dZV5zIwQQkpWab1/F3mM0Jd6qbwXOuTHpVzfBLWviqAosR5Seu2jIqgcMzBQQVZWbi+ssrI81q3riGHDbGm1eEIIKaJijRGiP66Vl+jTa6j9O0siFuh2FT0aGfOUESkKDQ1F7NvXE02bGuPRo5EYPtyO/p8SQkgxFPnSmIyMDDQ0NL77RzY+Pr5EEistdGksH4wBCzQlQlf6PkGbeib85EMKdOTIEzRrZgJTU8mJTb+M0SOEkMqK90tjALBgwQKpmaVJxRe1uSe+XqXtqos/FUHlTHJyJsaPP4c9e4LRurU5Ll0aCFnZvA5dKoIIIeTHFKsQ6tevH/T19b/fkFQYaTvdYPrhCrf9UtUerZ1oHFh5EhAQhQEDjiM8PAEAcPVqBE6fDkOPHnV4zowQQiq+Io8Rok+clU/K/kFQibwiEas55TJP2ZBv5eSIsWDBVTg77+KKIDU1Bfj4uKF7d5rYkhBCSkKx7xojlUNy6HWovzgpEUucFgtNKnjLhfDwBAwY4IeAgLdczNHRFPv29YSFRcVc548QQsqjIhdCYrG4NPMgZSgtMwfqB7pJxCKGBMNchW6T5xtjDHv3/oexY88iJSULACArK4C3dyvMmuUMObliTwZPCCGkEMVea4xUfCnL6uLrOaIf9r0POzNzvtIhX3nw4D28vE5w25aWWti/vxeaNaPB64QQUhro42UVc/X4dhiKP3Db6ZpWsKtnxWNG5GsODsYYOdIeADB4cCMEBY2kIogQQkoR9QhVIXfu30Pr4KkSMaXxd3nKhgBAdrYIcnIyEjcjrFrVAZ0716IB0YQQUgaoR6iKSExNR7Mz7SVi4lmxgAz9CvAlNDQOzZr9jT17giXiKioKVAQRQkgZoXfBKuLGtokS25lt/4CMAg2O5gNjDFu3PoCt7VY8fBiNcePO4eXL8j0jOyGEVFZ0aawKWORzCnOSD3LbKTW6Qq3lBB4zqro+fkzD8OGn4O8fysWMjdWQnp7NY1aEEFJ1USFUyb2MTcKc8AESMTXPvTxlU7VduPASgwefRExMKhcbNcoeq1a5QllZnsfMCCGk6qJCqBLLzBHh8cZ+qCn7VbDHJhoXVMYyMnIwc+YlrF2bNzBdV1cZO3d2R7duNBaIEEL4RIVQJfbHrpNYLHub2842bgJ52wGFHEFK2suX8ejV6xBCQvKmLOjYsSZ27eoBQ0NVHjMjhBAC0GDpSuvWi4+Y/nYMtx2nXh/yI/7hMaOqSUtLEZ8+pQMAhEJZrF/fEWfPelARRAgh5QQVQpXQh+QMBO2ZDHXBZy6mO+I4jxlVXTo6yti9uwdsbAzw4MGvGDeuKS1gTAgh5QhdGqtkxGIGr41ncU7On4vlNP4VcmoGPGZVdZw6FQoHB2OJHp/27WsgMNACsrL0uYMQQsob+stcyWy5/grT09dy22I5Jch1Wc5fQlVEWloWRo06je7dD2Lo0JNgjEnspyKIEELKJ/rrXIncehmHexcPorVs3kzFMkPPAXQpplQFBr6Hnd02bN0aCAA4d+4lTp8O4zkrQgghRUGXxiqJ5IxsTNtxBrcVV+QFjRoB1Wx5y6myE4nEWLnyNubMuYKcHDEAQFlZHuvWdUTXrrSQLSGEVARUCFUCjDHM2ncNtxXHS+7wOMxPQlVAVFQSBg48jmvX3nAxe3sj+Pr2hpWVDo+ZEUIIKQ4qhCqBiQcfYVWUO/D1FbBfrwI0QLpUHDr0GKNGnUFiYgaA3CuPM2Y4Yf781lBQkP3O0YQQQsoTKoQquJNB76Dw+ACE8jl5wW7r6JJYKblz5y369TvGbZuaqmPv3p5o1cqcv6QIIYT8MBosXYG9jkvDjCOPME3uq0tgdboC9oN5y6mya9bMBAMHWgMA3N3rIzh4FBVBhBBSgVGPUAX1OSsHw/fcx2HZ2dAXJObtcN/HW06VkVjMICMjedfdxo2d0aVLLfTtW58mRySEkAqOeoQqIMYYJh4MwuCEDWgoE5G3o+dWulW+BIWHJ8DJaScOH34iEVdXF8LdvQEVQYQQUglQIVQB7b4dgQdPX2Cg3CXJHTb9+EmokmGMwccnGI0abUFAwFuMHHkaUVFJfKdFCCGkFFAhVME8ikzAunNBuCmcILlj7id+EqpkEhLS0a/fMXh5nUBKShYAQFtbiVs4lRBCSOVCY4QqkIxsEQbtvIdjMrOhLMjM29H7b0CWfpQ/6+rVCAwceBxv3yZzscGDG2H9+o5QUxPymBkhhJDSQu+eFUjXDTehmBEHK8V3eUGDBkDDX/hLqhLIyhLB2/sKli+/hS9LhGlqKmLbtq7o06c+v8kRQggpVVQIVRCXn8Xi5YdUXFeYJ7lj9C1+EqokwsMT0KfPETx8GM3FWrc2h4+PG0xNNXjMjBBCSFmgMUIVAGMMEw4G4RfZa6gu8zFvx4BjBR9EikRJSQ6RkbkDoeXlZbB8uQsuXx5ERRAhhFQRVAhVAKf/i4ZjdgBWym/NC9ZzA2q68JZTZWFkpIa//+6OOnV0cefOcEyb1kJq3iBCCCGVF10aK+cyskV4engBtikclNzRcws/CVVwly6Fw9bWEDo6ylyse/fa6NSpJuTlaZ0wQgipaqhHqJybs/0Ypst/UwSNDwLklXjJp6LKyMjBpEnn0b79XowceRrsy6jo/6MiiBBCqiYqhMqxU8Hv0Tt6jWRw8jNA24KfhCqokJBYNGmyHWvX3gUAHDv2DOfPv+Q5K0IIIeUBFULlVFxqJq4e2Yjmsk/zguODAPVqvOVU0YjFDOvW3YGDw3aEhHwAAAiFsli/viM6dqzJc3aEEELKAxojVA7liMQYsPkqzstt4mLMzgsC6gkqsujoFAwZchIXLrziYg0b6sPXtzcaNNDnMTNCCCHlCRVC5dDmq6+wNGWWRH+doNs6/hKqYPz9QzFsmD/i4j5zsUmTmmHJknZQVKRfeUIIIXnoXaGcCf+YClxZBFu5r8aw0KryRXbrViR69MgbXG5oqIo9e9zQoUMNHrMihBBSXtEYoXImeNd4jJM7kReo05VWlS8GR0dT9OxZBwDQo0dthISMpiKIEEJIgahHqBw5df8Fen7+ZrZo9338JFNBMMYg+Kq3TCAQYPv2bujevTa8vGwk9hFCCCHfoh6hcoIxhucnlksG53ykS2KFiIpKQtu2Pjh9OkwirqOjjMGDG1ERRAgh5LuoR6ic2PlvCKbJH+a2xZ1WQkZOgceMyrfDh59g5MjTSEzMwJMnH/Dff6NhaKjKd1qEEEIqGOoRKgcO3ouEzJVFEjGZJsN5yqZ8S07OxODBJ+DufhSJiRkAAEVFObx/n8JzZoQQQioi6hHiWUJaFjYev4ybwgt5wSHn6JJYPgICouDp6YfXrxO5mLt7fWze3AVaWrTkCCGEkOKjQohn4w48wkb5Ddy22NwZMmaOPGZU/uTkiLFo0XUsWnQdIlHuGmFqagrYtKkzBgywprFAhBBCfhgVQjx6+j4ZrhHL0Ugub/Zjma5rCjmi6omISISHxzEEBLzlYo6Opti3rycsLLR4zIwQQkhlQGOEeMIYw1+nb2Gg3KW8oE1/QLcWf0mVQzIyAjx9+hEAICsrwIIFrXHt2mAqggghhJQIKoR4supiGNpHrZcMdt+Qf+MqrHp1DWzZ0hWWllq4eXMovL1bQU6Ofm0JIYSUDHpH4cG7xHT8c/Vf9JC9nRf89RogK89fUuXEjRtvkJycKRHr168Bnjz5Dc2amfCUFSGEkMqqXBRCmzZtgrm5ORQVFdG0aVPcu3evwLbbt2+Hs7MztLS0oKWlBRcXl0Lbl0czfO/ggnCGZLBaI15yKS+yskSYMeMSWrXajXHjzkntp8VSCSGElAbeC6FDhw5h8uTJmDdvHh4+fAgbGxu4urriw4cP+ba/evUq+vfvjytXriAgIACmpqbo0KED3r17V8aZ/5g74Z8wNWayZHDqC36SKSdCQ+PQvPnfWLbsFhgDfHyCcfHiq+8fSAghhPwkAWOM8ZlA06ZN4eDggI0bNwIAxGIxTE1NMW7cOMyYMeM7RwMikQhaWlrYuHEjBg0a9N32ycnJ0NDQQFJSEtTV1X86/+IQiRl+W7IeW3O884L2Q4Bua8s0j/KCMYZt2wIxadIFpKfnAADk5WWweHFbTJniCBkZui2eEEJIrtJ6/+b1ekNWVhYCAwMxc+ZMLiYjIwMXFxcEBAQU6RyfP39GdnY2tLW1892fmZmJzMy8MSfJyck/l/RPmOBzU6IIYqpGEFTRIujjxzQMH34K/v6hXKx2bR34+vaGnZ0Rj5kRQgipSni9NBYXFweRSAQDAwOJuIGBAWJiYop0junTp6NatWpwcXHJd//SpUuhoaHBfZmamv503j8i6XM2fnk1WyIm8PLnJRe+XbjwEtbWWySKoNGjG+Phw5FUBBFCCClTvI8R+hl//vknDh48iOPHj0NRUTHfNjNnzkRSUhL3FRUVVcZZ5vpj7Tq0lg3OC7SZA+hZ8ZILn27ceIOOHfcjJiYVAKCrqwx//374668uUFamu+YIIYSULV4vjenq6kJWVhaxsbES8djYWBgaGhZ67MqVK/Hnn3/i0qVLsLa2LrCdUCiEUCgskXx/1P3Xn+CRcZArOzO0rKDYahqvOfHFyak6OnasifPnX6Jjx5rYtasHrRpPCCGEN7z2CCkoKMDe3h6XL1/mYmKxGJcvX0bz5s0LPG758uVYuHAhzp8/j8aNG5dFqj+MMYaP+4bBXibvzjDF327wmBG/BAIBdu3qgb/+6oyzZz2oCCKEEMIr3i+NTZ48Gdu3b8eePXvw7NkzjB49GmlpaRgyZAgAYNCgQRKDqZctW4a5c+di586dMDc3R0xMDGJiYpCamsrXUyjU7QN/orPoCrctbr8QkM//Ml5lExOTii5dfHH5crhE3NBQFaNHO9BiqYQQQnjH+yx17u7u+PjxI7y9vRETE4NGjRrh/Pnz3ADqyMhIyMjk1WubN29GVlYWfvnlF4nzzJs3D/Pnzy/L1L/rzYd4tAj7UyIm02I8T9mULX//UAwb5o+4uM8IDo5BcPAo6Ogo850WIYQQIoH3eYTKWlnOI3R4/e/oG781LzDnAyDH73il0paWloUpUy5i69ZALmZkpIpTp/rD3r4aj5kRQgipyCrlPEKV2bmQaIkiKKXRCKhV8iIoMPA9PD39EBr6iYu5udXB9u3doKtLvUGEEELKH97HCFVGKRnZuHtys0RMrftynrIpfSKRGMuW3USzZn9zRZCysjy2b+8GP7++VAQRQggpt6hHqBRsOf8Q83PWcdviGu0kxjlVJm/fJmPgwOO4ejWCi9nbG8HXtzesrHT4S4wQQggpgsr57syjtIxsTAvqIBGT8TjMUzalLz09G/fv5y54KxAAM2c64fbtYVQEEUIIqRCoECphH/Z4SQa6bwBkK2/HW61aOli/vhNMTdVx5YoXlixpBwUFWb7TIoQQQoqECqES9Dn+HSyiz3Dbibp2gN0gHjMqeffuvcPnz9kSsSFDGuHp0zFo1cqcn6QIIYSQH0SFUAkK3DmF+/6FnBU0xvzLYzYlKydHjAULrsLR8W9MnXpRYp9AIICqqgJPmRFCCCE/jgqhEhL97A6cU89x27J9/q40MyeHhyegZctdmD//GkQihs2bH+DKldd8p0UIIYT8NCqESsjno6O570+zFrCsXfBCsBUFYww+PsFo1GgLAgLeAgBkZQVYsKA1nJ3N+E2OEEIIKQGVdxRvGUp4dR81RHnraTUcspHHbEpGQkI6Ro8+g0OHnnAxS0st7N/fC82amfCYGSGEEFJyqBAqAanHp0Dr/9/fF1vBwdyS13x+1rVrERg48DiiopK52ODBjbB+fUeoqVXu2bEJ/0QiEbKzs7/fkBBS6SgoKJT5vHtUCP2kmLAHME0N5rb1hh7iMZufd+1aBNq02YMvK9BpaSli69au6NOnPr+JkUqPMYaYmBgkJibynQohhCcyMjKwsLCAgkLZ3YBDhdBPSjg2GYb//z5MowWsKnhvkJNTdbRsaYZr196gTRtz+Pj0hIlJ6S5OSwgArgjS19eHsrJypbnZgBBSNGKxGO/fv0d0dDSqV69eZn8DqBD6CYGPn8E+M683SN9jC4/ZlAxZWRns3dsTR448xcSJzSAjQ29GpPSJRCKuCNLRoVnJCamq9PT08P79e+Tk5EBeXr5MHpPuGvtBTCyG+pHe3HaSnC40DarzmFHxffyYht69D+PWrUiJuKmpBiZPbk5FECkzX8YEKSvTAr2EVGVfLomJRKIye0zqEfpBEX5zUUvwjtsWDjrKYzbFd+HCSwwefBIxMal4+DAawcGjoK5OA6EJv+hyGCFVGx9/A6hH6AeIszNh8TjvFvm3SrWhWN2Wx4yKLiMjBxMnnkfHjvsRE5MKAEhNzUJY2CeeMyOEEELKHhVCP+CN73ju+2jownBKAI/ZFF1ISCwcHLZj3bq7XKxjx5oICRmNxo2r8ZgZIeRr5ubmWLt2Ld9pVFmXL19G3bp1y/TyTFWQlZUFc3NzPHjwgO9UJFAhVEzinBwYhx/htl9Y/Qo5ufK92rpYzLBu3R04OGzH48cfAABCoSzWr++Is2c9YGioynOGhFRMAoGg0K/58+f/0Hnv37+PX3/99adya926NZeHoqIirKyssHTpUrAvc2N8Zc+ePXBwcICysjLU1NTQqlUrnD59WqodYwzbtm1D06ZNoaqqCk1NTTRu3Bhr167F58+ffyrf8uT333/HnDlzICtbvv+2/yjGGLy9vWFkZAQlJSW4uLjgxYsX3z3u3bt3GDBgAHR0dKCkpISGDRtKFDWxsbEYPHgwqlWrBmVlZXTs2FHivAoKCpg6dSqmT59eKs/rR1EhVEzP/h4OBUHup4Rn4upo4T6N54wKFx2dgs6d92PixAvIzMzNu2FDfTx48CvGjWtKYzII+QnR0dHc19q1a6Guri4Rmzp1KteWMYacnJwinVdPT69EBo6PGDEC0dHRCA0NxcyZM+Ht7Y0tWyTvbp06dSpGjhwJd3d3/Pfff7h37x6cnJzQo0cPbNwoOUv+wIEDMXHiRPTo0QNXrlxBUFAQ5s6di5MnT+LiRcnFmEtTVlZWqZ375s2bePXqFXr37v39xoUozRx/1vLly7F+/Xps2bIFd+/ehYqKClxdXZGRkVHgMQkJCWjRogXk5eVx7tw5PH36FKtWrYKWVu50wowxuLm5ITw8HCdPnsSjR49gZmYGFxcXpKWlcefx9PTEzZs38eTJk4IequyxKiYpKYkBYElJScU+NiU+lrF56tzX7RNbSiHDkvX4cSwTChcyYD4D5rNJk86z9PRsvtMiREJ6ejp7+vQpS09P5zuVH7Zr1y6moaHBbV+5coUBYGfPnmV2dnZMXl6eXblyhb18+ZJ1796d6evrMxUVFda4cWP2zz//SJzLzMyMrVmzhtsGwLZv387c3NyYkpISq1mzJjt58mSh+bRq1YpNmDBBImZnZ8d69uzJbQcEBDAAbP369VLHT548mcnLy7PIyEjGGGOHDh1iANiJEyek2orFYpaYmFhgLo8fP2ZdunRhampqTFVVlTk5ObGXL18WmGePHj2Yl5cXt21mZsb++OMPNnDgQKampsa8vLxY8+bN2e+//y5x3IcPH5icnBy7du0aY4yxjIwMNmXKFFatWjWmrKzMmjRpwq5cuVJgnowxNmbMGPbLL79IxIr6M/s2R8YYu3HjBnNycmKKiorMxMSEjRs3jqWmpnLH+fj4MHt7e6aqqsoMDAxY//79WWxsbKE5/gyxWMwMDQ3ZihUruFhiYiITCoXswIEDBR43ffp05uTkVOD+0NBQBoA9fvyYi4lEIqanp8e2b98u0bZNmzZszpw5+Z6nsL8FP/P+XRi6a6wYVNfVkthu3mMkT5kUXf36+lixoj2WLLmJPXvc0KFDDb5TIqTIum24iY8pmWX+uHpqQpwa51Qi55oxYwZWrlwJS0tLaGlpISoqCp07d8bixYshFArh4+ODbt26ITQ0FNWrFzwFx4IFC7B8+XKsWLECGzZsgKenJ968eQNtbe3v5sAYw82bN/H8+XPUqpX3d+zAgQNQVVXFyJHSf8umTJmC1atX49ixY5g4cSL279+P2rVro0ePHlJtBQIBNDQ08n3sd+/eoWXLlmjdujX+/fdfqKur49atW0XuHfti5cqV8Pb2xrx58wAA58+fx/Lly/Hnn39yPduHDh1CtWrV4OzsDAAYO3Ysnj59ioMHD6JatWo4fvw4OnbsiJCQEInX4Ws3btyAh4eHRCw1NbVIP7Nvc3z16hU6duyIRYsWYefOnfj48SPGjh2LsWPHYteuXQByp45YuHAhateujQ8fPmDy5MkYPHgwzp49W+BrMWrUKOzbt6/Q1ys1NTXf+OvXrxETEwMXFxcupqGhgaZNmyIgIAD9+vXL9zh/f3+4urqiT58+uHbtGoyNjfHbb79hxIgRAIDMzNz/p4qKitwxMjIyEAqFuHnzJoYPH87FmzRpghs3bhSaf1miQqiIPoQGQP+r7aA2PmjEVzKFCA6OQZ06uhAK8360Y8c2wYAB1tDSUuIxM0KK72NKJmKSC+6urwj++OMPtG/fntvW1taGjY0Nt71w4UIcP34c/v7+GDt2bIHnGTx4MPr37w8AWLJkCdavX4979+6hY8eOBR7z119/YceOHcjKykJ2djYUFRUxfnzezR5hYWGoUaNGvssZVKtWDerq6ggLCwMAvHjxArVr1y76E/+/TZs2QUNDAwcPHuQmyLOysir2edq2bYspU6Zw23379sXEiRNx8+ZNrvDx9fVF//79IRAIEBkZiV27diEyMhLVquXeDDJ16lScP38eu3btwpIlS/J9nDdv3nDtv7CxsSnSz+zbHIcPHw5PT09MnDgRAFCrVi2sX78erVq1wubNm6GoqIihQ4dy7S0tLbF+/Xo4ODggNTUVqqr5j9/8448/JC67FkdMTAwAwMDAQCJuYGDA7ctPeHg4Nm/ejMmTJ2PWrFm4f/8+xo8fDwUFBXh5eaFOnTqoXr06Zs6cia1bt0JFRQVr1qzB27dvER0dLXGuatWq4c2bNz+Uf2mgQqiI4s4tkyiEGrWS/lTEJ5FIjJUrb2POnCuYMKEpVq7swO0TCARUBJEKSY+nRX5L8nEbN24ssZ2amor58+fjzJkziI6ORk5ODtLT0xEZGVnAGXJZW1tz36uoqEBdXR0fPnwo9BhPT0/Mnj0bCQkJmDdvHhwdHeHo6CjRhuUzeDo/RW33raCgIDg7O//0LMHfvo56enro0KED9u/fD2dnZ7x+/RoBAQHYunUrACAkJAQikUiq6MrMzCx09vL09HSJXg2g6D+zb3MMDg7Gf//9h/3793MxxhjEYjFev36NunXrIjAwEPPnz0dwcDASEhIgFosBAJGRkahXr16+Oerr60NfXz/ffaVFLBajcePGXAFpa2uLx48fY8uWLfDy8oK8vDz8/PwwbNgwaGtrQ1ZWFi4uLujUqZPU746SklK5GlxPhVAR5CREoV7iFW47ZuQTbn2x8iAqKgkDBx7HtWu5FfaqVQFwc6sDJ6eKNdM1Id8qqctTfFJRUZHYnjp1Kv755x+sXLkSNWvWhJKSEn755ZfvDq79tpAQCATcm2ZBNDQ0ULNmTQDA4cOHUbNmTTRr1oy7LGJlZYWbN28iKytLqlfo/fv3SE5O5goJKysrPH/+/PtP+BtKSoV/CJORkZF6o/wy0/jXvn0dgdxCb/z48diwYQN8fX3RsGFDNGzYEEBu8SIrK4vAwECpu78K6mkBAF1dXSQkJEjEivoz+zbH1NRUjBw5UqIX7ovq1asjLS0Nrq6ucHV1xf79+6Gnp4fIyEi4uroW+vvwM5fGDA1z371iY2NhZGTExWNjY9GoUaMCz2dkZCRVmNWtWxfHjh3jtu3t7REUFISkpCRkZWVBT08PTZs2lSoQ4+PjoaenV2j+ZYnuGiuCT/tHcN/7K/aAoZEJj9lIOnz4Caytt3BFkEAAzJzphCZNjHnOjBCSn1u3bmHw4MHo2bMnGjZsCENDQ0RERJT646qqqmLChAmYOnUqV3j069cPqampXC/K11auXAl5eXnu7ikPDw+EhYXh5MmTUm0ZY0hKSsr3ca2trXHjxo18ixsgt2fn60snIpEIjx8/LtJz6tGjBzIyMnD+/Hn4+vrC09OT22drawuRSIQPHz6gZs2aEl9fioH82Nra4unTpxKxH/2Z2dnZ4enTp1KPX7NmTSgoKOD58+f49OkT/vzzTzg7O6NOnTrf7eUDci+NBQUFFfpVEAsLCxgaGuLy5ctcLDk5GXfv3kXz5s0LPK5FixYIDQ2ViIWFhcHMzEyqrYaGBvT09PDixQs8ePBAalzZ48ePYWtbfiYhpkLoO9iHZzCIy5sw0ajdaB6zyZOcnInBg0/A3f0oEhNzx1CYmqrjyhUvLFnSDgoKlXP+C0Iqulq1asHPzw9BQUEIDg6Gh4fHd3t2SsrIkSMRFhbGfYpv3rw5JkyYgGnTpmHVqlV49eoVnj9/jjlz5mDdunVYtWoVTE1NAeSOyXF3d0f//v2xZMkSPHjwAG/evMHp06fh4uKCK1eu5PuYY8eORXJyMvr164cHDx7gxYsX2Lt3L/em2rZtW5w5cwZnzpzB8+fPMXr0aCQmJhbp+aioqMDNzQ1z587Fs2fPuDFUQG4PlqenJwYNGgQ/Pz+8fv0a9+7dw9KlS3HmzJkCz+nq6oqbN29KxH70ZzZ9+nTcvn0bY8eORVBQEF68eIGTJ09y44qqV68OBQUFbNiwAeHh4fD398fChQu/e159ff18i6uvvwoiEAgwceJELFq0CP7+/ggJCcGgQYNQrVo1uLm5ce3atWsnMX3CpEmTcOfOHSxZsgQvX76Er68vtm3bhjFjxnBtjhw5gqtXr3K30Ldv3x5ubm7o0KHD1yngxo0bUjFeleg9aBVAsW6/E4slbpf3m9Op9BMsgtu3I5ml5TrulnhgPnN3P8Li4z/znRohP6Qy3z6fkJAg0e7169esTZs2TElJiZmamrKNGzdK3UKe3+3zx48flziPhoYG27VrV4H55HdbOmOMjRw5ktWvX5+JRCIu9vfffzN7e3umqKjIVFRUmLOzM/P395c6ViQSsc2bNzMHBwemrKzM1NXVmb29PVu3bh37/Lngvz/BwcGsQ4cOTFlZmampqTFnZ2f26tUrxhhjWVlZbPTo0UxbW5vp6+uzpUuX5nv7/Nevx9fOnj3LALCWLVtK7cvKymLe3t7M3NycycvLMyMjI9azZ0/233//FZjrp0+fmKKiInv+/DkX+5Gf2Rf37t1j7du3Z6qqqkxFRYVZW1uzxYsXc/t9fX2Zubk5EwqFrHnz5szf358BYI8ePSowx58lFovZ3LlzmYGBARMKhaxdu3YsNDRUoo2ZmRmbN2+eROzUqVOsQYMGTCgUsjp16rBt27ZJ7F+3bh0zMTFh8vLyrHr16mzOnDksMzNTos3t27eZpqZmgb8vfNw+L2DsB0fAVVDJycnQ0NBAUlIS1NXVC28cuBs4NYHbPNn+Knq04Lc77+rVCLi4+EAkyv2xqakpYNOmzhgwwJomRyQVVkZGBl6/fg0LCwupgaqElLVp06YhOTk530uG5Oe4u7vDxsYGs2bNynd/YX8LivX+XQx0aawQOTfXcd/fEtVHu8YNecwmV4sWprC3z72109HRFMHBozBwoA0VQYQQUkJmz54NMzOzMrtkWVVkZWWhYcOGmDRpEt+pSKC7xgqSkwW5hHBu85ztX2gh5P/lkpeXxf79vXDo0GNMn+4EOTmqZQkhpCRpamoW2GNBfpyCggLmzJnDdxpS6F20AOLAPdz375k2BjsVPPistCQkpMPT0w+Bge8l4jVramP27JZUBBFCCCE/id5JC5Bx8y/u+0XZA1BTv2xXaL96NQLW1lvg6xsCT08/fP6c/62nhBBCCPlxVAjl5+0DKKfkXhYLFluiW//fyuyhs7JEmDHjEtq23YO3b5MBAB8+pOHJk+/PLUEIIYSQ4uF/0Es5lHVvJ77MsfpKYIbOdcpmKvPQ0Dh4ePjh4cO8ycXatDGHj09PmJiU3Ah5QgghhOSiHqF8xEbkzSoapt4civKlOzkhYwxbtz6Are1WrgiSl5fB8uUuuHRpEBVBhBBCSCmhHqFvpcTANPkRt9mmx+BSfbiPH9MwfPgp+PvnTV1eu7YOfH17w87OqJAjCSGEEPKzqEfoG6m3tnPfnxA5okmN0r0sFhWVjLNnX3Dbo0c3xsOHI6kIIoQQQsoAFULfSHtyjvs+p4ZrqU9UaGdnhEWL2kBXVxn+/v3w119doKws//0DCSGE/JS5c+fi119/5TuNSmfLli3o1q0b32kUGRVCX/vwHHopeeODLFoPKvGHeP48DtnZIonY1KmOePLkN3TrVrvEH48QUnoEAkGhX/Pnz/+pc584caJYOairq8PBwSHfFeLT09Mxb948WFlZQSgUQldXF3369MGTJ0+k2iYnJ2P27NmoU6cOFBUVYWhoCBcXF/j5+aGyrMoUExODdevWYfbs2XynUmri4+Ph6ekJdXV1aGpqYtiwYUhNTS2wfURERIG/y0eOHOHaRUZGokuXLlBWVoa+vj6mTZuGnJwcbv/QoUPx8OFD3Lhxo1SfX0mhQugrsde2Qga5/8n/FTSFvbl2iZ1bLGZYt+4OGjXagkWLrkvsk5WVgb6+Sok9FiGkbERHR3Nfa9euhbq6ukRs6tSpZZLHrl27EB0djQcPHqBFixb45ZdfEBISwu3PzMyEi4sLdu7ciUWLFiEsLAxnz55FTk4OmjZtijt37nBtExMT4ejoCB8fH8ycORMPHz7E9evX4e7ujt9//x1JSUll8pwAIDu79OZP27FjBxwdHWFmZvZT5ynNHH+Wp6cnnjx5gn/++QenT5/G9evXC+0BMzU1lfj9jY6OxoIFC6CqqopOnToBAEQiEbp06YKsrCzcvn0be/bswe7du+Ht7c2dR0FBAR4eHli/fn2pP8cSUaJLuFYABa5em50hsdL8uqOXSuwx379PZq6ue7mV4mVkFrC7d9+W2PkJqegq4+rzjDG2fft2VqdOHSYUClnt2rXZpk2buH2ZmZlszJgxzNDQkAmFQla9enW2ZMkSxljuyt8AuC8zM7MCHxffrEyfnJzMALB169ZxsT///JMJBAIWFBQkcaxIJGKNGzdm9erVY2KxmDHG2OjRo5mKigp79+6d1GOlpKSw7OzsAnPx9/dnjRs3ZkKhkOno6DA3N7cC82SMMQ0NDbZr1y7GWO4K7wDYwYMHWcuWLZlQKGTr1q1jioqK7OzZsxLH+fn5MVVVVZaWlsYYYywyMpL16dOHaWhoMC0tLda9e3f2+vXrAvNkjLH69euzjRs3SsTOnTvHWrRowTQ0NJi2tjbr0qULe/nyJbc/vxy/5F/Yz5oxxn7//XdWq1YtpqSkxCwsLNicOXNYVlZWoTn+jKdPnzIA7P79+xLPTyAQ5PuzLUijRo3Y0KFDue2zZ88yGRkZFhMTw8U2b97M1NXVJVaav3btGlNQUChwlfmC8LH6PN019n/s0gJ8GQ0kYgL0a9+iRM578uRzDB9+CnFxn7nY+PFNYG1tUCLnJ6RS29oKSOVhMlFVfWDktZ86xf79++Ht7Y2NGzfC1tYWjx49wogRI6CiogIvLy+sX78e/v7+OHz4MKpXr46oqChERUUBAO7fvw99fX3s2rULHTt2hKxs0abwyMnJwd9//w0g91P5F76+vmjfvj1sbGwk2svIyGDSpEnw9PREcHAwrK2tcfDgQXh6eqJatWpS51dVLXiG/TNnzqBnz56YPXs2fHx8kJWVhbNnzxYp76/NmDEDq1atgq2tLRQVFXHjxg34+vpyPRJA7mvr5uYGZWVlZGdnw9XVFc2bN8eNGzcgJyeHRYsWoWPHjvjvv/8kXocv4uPj8fTpUzRu3FginpaWhsmTJ8Pa2hqpqanw9vZGz549ERQUBBmZvAso3+b4vZ81AKipqWH37t2oVq0aQkJCMGLECKipqeH3338v8LWoX78+3rx5U+B+Z2dnnDt3Lt99AQEB0NTUlHiOLi4ukJGRwd27d9GzZ88Cz/tFYGAggoKCsGnTJonzNmzYEAYGee9hrq6uGD16NJ48eQJbW1sAQOPGjZGTk4O7d++idevW330sPlEh9H/Jr+5B4//f7xO5wEtd8afOl5aWhSlTLmLr1kAuZmioij173NChQ42fOjchVUbqByDl/ffblUPz5s3DqlWr0KtXLwCAhYUFnj59iq1bt8LLywuRkZGoVasWnJycIBAIJC7R6Onp4X/t3X1cjff/B/BXd+d0Sqck3XHcpBvMSFErM2NRM5a5qWjuFvZFbMy25i5tC7vB8LWbQplFYYzfJF+ix5S+WKRNKd0N31VGkVLSOe/fHx5dc3RKJ93YOe/n49Hj4fpcn8/nel/nI+ftuj6f6wIevvzT2tr6iceaPHky9PT0UFVVBYVCgR49esDPz0/Yn5OTg+HDh6ts26dPH6GOra0tysrK0Lt3b7XPNzw8HAEBAQgLCxPKHk+8muLdd98VPjPg4e2dqVOn4t69ezAyMkJ5eTkOHz6MAwcOAADi4uKgUCiwdetWYXFLVFQUzMzMkJSUhFGjRtU7xtWrV0FE9ZK9CRMmKG1v374dnTt3RmZmJvr169dgjE8aawBKLxvt0aMHlixZgtjY2EYTofj4+EZvvUkkkgb3FRcXw9JSedWzvr4+zM3NUVxc3GC7R23btg19+vSBp6enUr+PJkEAhO1H+zUyMoKpqWmjidyzghMhALhbAtO/zgEASqkDbr0U/lTdpaX9iSlT9iMn55ZQ5uvrhK1bX4eFhdFT9c2YVunQNk91b+njVlZWIi8vD0FBQZg9e7ZQXltbC1PTh//lmjFjBkaOHAknJyf4+PhgzJgxKr+0m2LDhg3w8vJCfn4+Fi1ahE2bNsHcXHmOIzVhknNT6jQkPT1d6Vyb6/GrNKNHj4aBgQEOHTqEgIAA/Pjjj5BKpfDy8gIAXLx4Ebm5uTAxMVFqV11djby8PJXHqKqqAgAYGir/h/fKlStYuXIlzpw5g5s3b0KhUAB4mDg9mgg9GmNTxhp4mLBt2rQJeXl5qKioQG1tLaTSxh+W+7Tzl55GVVUVdu3ahRUrVjS7D4lEgnv37j25YjvjRAjAgzNbUbdgPVY+AkFD7Zrd14kTBfD2/gG1tQ9/gYyMDPDVV96YNcul1ZfiM6ZxnvL2VHupW5kTGRkJd3d3pX11t7lcXFxQUFCAI0eO4Pjx4/Dz84OXlxf27dun9vGsra1hb28Pe3t7REVFYfTo0cjMzBSuCDg6OiIrK0tl27pyR0dHdO7cGWZmZrh8+bLaMTR2dQJ4uLrt8URL1dUOY2PlhSMikQgTJ07Erl27EBAQgF27dsHf3x/6+g+/vioqKuDq6oqYmJh6fdVdWXuchYUFAKCsrEypztixY9G9e3dERkbC1tYWCoUC/fr1Q01NTYMxNmWsU1NTERgYiLCwMHh7e8PU1BSxsbFYt26dyvjqPM2tMWtra9y4oXxbuba2FqWlpU26yrhv3z7cu3cP06Ypr562trbG2bNnlcpKSkqEfY8qLS1tcAyeJZwIlRXCIPlzYfNshxGYJ2n+c3yGDJGhb9/OyMgogaurDXbtmgBHx04tESlj7B/CysoKtra2yM/PR2BgYIP1pFIp/P394e/vj4kTJ8LHxwelpaUwNzeHgYEB5HJ5g20b4ubmBldXV4SHh2Pjxo0AgICAACxbtgwXL15Uul2lUCiwYcMG9O3bFwMGDICOjg4CAgKwc+dOhIaG1rt1VFFRAUNDQyEJeVT//v2RmJiImTNnqoyrc+fOKCr6+z2KV65cafLVgsDAQIwcORKXLl3CiRMn8Omnnwr7XFxcEBcXB0tLyydeYanTq1cvSKVSZGZmwtHREQBw69YtZGdnIzIyEkOHDgUAJCcnP7Gvpoz16dOn0b17d6Wl+k25ZfQ0t8Y8PDxw+/ZtpKWlwdXVFQBw4sQJKBSKegmbKtu2bcPrr79eL5Hx8PBAeHg4bty4ISTax44dg1QqRd++fYV6eXl5qK6uFuYMPdNadOr1P0C9WecnVgsrxU4t96DDGX8+9TF+/72Eli1LpPv3a5+6L8a0gSauGouMjCSJREIbN26k7OxsysjIoO3bt9O6deuIiGjdunW0a9cuysrKouzsbAoKCiJra2uSy+VEROTg4EBz586loqIiKi0tbfC4ULEaKz4+nsRiMV2//nB1alVVFbm7u5NMJqM9e/bQH3/8QWfPnqVx48aRsbExpaamCm1v3bpFvXv3pq5du9KOHTvo0qVLlJOTQ9u2bSN7e3sqKytTGcfJkydJV1eXVq5cSZmZmZSRkUFr164V9gcEBFCfPn3o/PnzdO7cORoxYgQZGBjUWzV24cKFen0rFAqSyWQ0YMAA6tWrl9K+yspKcnBwoJdffpl++eUXys/Pp5MnT9KCBQvo2rVrDX5u48ePp/fee0/Ylsvl1KlTJ3rzzTfpypUrlJiYSIMHD1b6fBuK8UljffDgQdLX16fdu3dTbm4ubdy4kczNzeutMmxpPj4+NHDgQDpz5gwlJyeTg4MDTZ48Wdh//fp1cnJyojNnzii1u3LlCuno6NCRI0fq9VlbW0v9+vWjUaNGUXp6OiUkJFDnzp3po48+UqoXFRVFdnZ2asfcHqvGtD4Rqto2RkiEZn70MT2olavRVzXNmnWQfv+9pLXCZUwraGIiREQUExNDzs7OJBKJqGPHjvTSSy/R/v37iYgoIiKCnJ2dydjYmKRSKb3yyit0/vx5oe2hQ4fI3t6e9PX11Vo+T/QwcejduzfNnTtXKKusrKRly5aRvb09GRgYkLm5OU2YMIF+++23en3evn2bQkJCyMHBgUQiEVlZWZGXlxcdOHBAWGavyo8//iicr4WFBY0fP17Y97///Y9GjRpFxsbG5ODgQPHx8SqXz6tKhIgeLj8HQCtXrqy3r6ioiKZNm0YWFhYkFovJzs6OZs+e3egXZnx8PHXp0kVIPImIjh07Rn369CGxWEz9+/enpKSkJiVCRI2PNRHR+++/T506daIOHTqQv78/bdiwodUToVu3btHkyZOpQ4cOJJVKaebMmXT37l1hf935nDx5UqndRx99RDKZTOmzeVRhYSG9+uqrJJFIyMLCgt577716j1UYNWoUrVmzRu2Y2yMR0iHSkMeENlF5eTlMTU1x584dSDt0AD7uCAC4SxIssTuE76a7Namf1NRrePPNA8jPL0P//lY4e3YWxGK+08hYc1RXV6OgoAA9e/asN4GVsdZARHB3d8eiRYswefLk9g5Ho1y6dAkjRoxATk6O0oTxpmjs3wKl7+8m3gZtCu1+svSl/cIf/yArfDC6byOVH6qtVSAsLAlDh0YhP78MAFBQUIaMjJJWC5MxxljL0tHRQUREhNKrIVjLKCoqwvfff692EtRetPoSRnnKVtTllGcVvfFW54YfFgYA+fllePPN/UhNvS6UeXrK8MMPb6Bnz46tGCljjLGW5uzsDGdn5/YOQ+PUPdrgn0J7EyH5A4hupAubxQPfabAqEWHnzgwEB8fj7t2Hyyj19HSwcuUwLF06FPr62n1hjTHGGPun0tpE6F7uKUgVDx+q9Yv8ecx/dbDKemVlVZg79zDi4v5+Q7OdXUfExIzHCy90bZNYGWOMMdY6tPZSxq1ze4U/F1qNhKmR6mcHZWXdxN69mcL2jBnOSE9/m5MgxlqBlq3dYIw9pj3+DdDaREh27f+EPzu8OKHBep6eMixbNhRmZobYs2cioqJ8YWIibosQGdMaBgYP/yPyT3gcP2Os9dQ9xbupLxpuCVp7a6xOETph4HN9hO2CgjJ062YKPb2/c8QVK17C22+7okuXlluuxxj7m56eHszMzIRXAhgZGfEraRjTMgqFAn/99ReMjIxUPr28tWh9IpRjMRLDDPRARIiISMOiRUcRGjoMH374olDHwECPkyDGWlnde4oefz8SY0x76Orqolu3bm36HyGtT4Ro0Cz89VclZs36Pxw6lA0AWL78JEaN6oWBA23aOTrGtIeOjg5sbGxgaWnZ6PuVGGOaSyQSQVe3bWftPBOJ0JYtW/DFF1+guLgYAwYMwObNm+Hm1vATnvfu3YsVK1agsLAQDg4O+OyzzzB69Gi1j1tCHXH7phT93/gWxcUVQvmsWQPh5GTRrHNhjD0dPT29Np0fwBjTbu0+WTouLg6LFy9GaGgozp8/jwEDBsDb27vBy+OnT5/G5MmTERQUhAsXLmDcuHEYN24cfv/9d7WOW12rhyXHRuCNsbFCEmRhYYRDhwLwzTdjYNTAKjLGGGOMaY52f9eYu7s7Bg8ejH//+98AHk6WkslkWLBgAUJCQurV9/f3R2VlJX7++Weh7IUXXoCzszO+/fbbJx6v7l0lfSzmIOumrVDu42OPqChfWFs3/nRpxhhjjLU9jXzXWE1NDdLS0pQex62rqwsvLy+kpqaqbJOamlrv8d3e3t4N1m9I1k1zAIBYrIdNm3wQHz+FkyDGGGNMy7TrHKGbN29CLpfDyspKqdzKygqXL19W2aa4uFhl/eLiYpX179+/j/v37wvbd+7cqduDvn07Y9s2X/Tt2xl3795t/okwxhhjrFWVl5cDaPmHLj4Tk6Vb05o1axAWFqZizwZkZgIeHu+1eUyMMcYYa55bt2616Jvt2zURsrCwgJ6eHkpKSpTKS0pKhGeKPM7a2lqt+h999BEWL14sbN++fRvdu3fH1atXW/SDZOorLy+HTCbDtWvXWvR+L2seHo9nB4/Fs4PH4tlx584ddOvWDebm5i3ab7smQiKRCK6urkhMTMS4ceMAPJwsnZiYiODgYJVtPDw8kJiYiHfffVcoO3bsGDw8PFTWF4vFEIvrvxLD1NSU/1I/I6RSKY/FM4TH49nBY/Hs4LF4drT0c4ba/dbY4sWLMX36dAwaNAhubm746quvUFlZiZkzZwIApk2bhi5dumDNmjUAgHfeeQfDhg3DunXr8NprryE2Nha//vorIiIi2vM0GGOMMfYP1O6JkL+/P/766y+sXLkSxcXFcHZ2RkJCgjAh+urVq0rZn6enJ3bt2oXly5dj6dKlcHBwwE8//YR+/fq11ykwxhhj7B+q3RMhAAgODm7wVlhSUlK9skmTJmHSpEnNOpZYLEZoaKjK22WsbfFYPFt4PJ4dPBbPDh6LZ0drjUW7P1CRMcYYY6y9tPsrNhhjjDHG2gsnQowxxhjTWpwIMcYYY0xrcSLEGGOMMa2lkYnQli1b0KNHDxgaGsLd3R1nz55ttP7evXvRu3dvGBoa4vnnn0d8fHwbRar51BmLyMhIDB06FB07dkTHjh3h5eX1xLFj6lH3d6NObGwsdHR0hAefsqen7ljcvn0b8+fPh42NDcRiMRwdHfnfqhai7lh89dVXcHJygkQigUwmw6JFi1BdXd1G0WquX375BWPHjoWtrS10dHTw008/PbFNUlISXFxcIBaLYW9vj+joaPUPTBomNjaWRCIRbd++nS5dukSzZ88mMzMzKikpUVk/JSWF9PT06PPPP6fMzExavnw5GRgY0G+//dbGkWsedcdiypQptGXLFrpw4QJlZWXRjBkzyNTUlK5fv97GkWsmdcejTkFBAXXp0oWGDh1Kvr6+bROshlN3LO7fv0+DBg2i0aNHU3JyMhUUFFBSUhKlp6e3ceSaR92xiImJIbFYTDExMVRQUEBHjx4lGxsbWrRoURtHrnni4+Np2bJltH//fgJABw4caLR+fn4+GRkZ0eLFiykzM5M2b95Menp6lJCQoNZxNS4RcnNzo/nz5wvbcrmcbG1tac2aNSrr+/n50WuvvaZU5u7uTm+//XarxqkN1B2Lx9XW1pKJiQnt2LGjtULUKs0Zj9raWvL09KStW7fS9OnTORFqIeqOxTfffEN2dnZUU1PTViFqDXXHYv78+TRixAilssWLF9OQIUNaNU5t05RE6IMPPqDnnntOqczf35+8vb3VOpZG3RqrqalBWloavLy8hDJdXV14eXkhNTVVZZvU1FSl+gDg7e3dYH3WNM0Zi8fdu3cPDx48aPEX7Gmj5o7Hxx9/DEtLSwQFBbVFmFqhOWNx6NAheHh4YP78+bCyskK/fv2wevVqyOXytgpbIzVnLDw9PZGWlibcPsvPz0d8fDxGjx7dJjGzv7XU9/cz8WTplnLz5k3I5XLh9Rx1rKyscPnyZZVtiouLVdYvLi5utTi1QXPG4nEffvghbG1t6/1FZ+przngkJydj27ZtSE9Pb4MItUdzxiI/Px8nTpxAYGAg4uPjkZubi3nz5uHBgwcIDQ1ti7A1UnPGYsqUKbh58yZefPFFEBFqa2vxr3/9C0uXLm2LkNkjGvr+Li8vR1VVFSQSSZP60agrQkxzrF27FrGxsThw4AAMDQ3bOxytc/fuXUydOhWRkZGwsLBo73C0nkKhgKWlJSIiIuDq6gp/f38sW7YM3377bXuHpnWSkpKwevVqfP311zh//jz279+Pw4cP45NPPmnv0FgzadQVIQsLC+jp6aGkpESpvKSkBNbW1irbWFtbq1WfNU1zxqLOl19+ibVr1+L48ePo379/a4apNdQdj7y8PBQWFmLs2LFCmUKhAADo6+sjOzsbvXr1at2gNVRzfjdsbGxgYGAAPT09oaxPnz4oLi5GTU0NRCJRq8asqZozFitWrMDUqVMxa9YsAMDzzz+PyspKzJkzB8uWLVN6SThrXQ19f0ul0iZfDQI07IqQSCSCq6srEhMThTKFQoHExER4eHiobOPh4aFUHwCOHTvWYH3WNM0ZCwD4/PPP8cknnyAhIQGDBg1qi1C1grrj0bt3b/z2229IT08Xfl5//XUMHz4c6enpkMlkbRm+RmnO78aQIUOQm5srJKMAkJOTAxsbG06CnkJzxuLevXv1kp26BJX41Z1tqsW+v9Wbx/3si42NJbFYTNHR0ZSZmUlz5swhMzMzKi4uJiKiqVOnUkhIiFA/JSWF9PX16csvv6SsrCwKDQ3l5fMtRN2xWLt2LYlEItq3bx8VFRUJP3fv3m2vU9Ao6o7H43jVWMtRdyyuXr1KJiYmFBwcTNnZ2fTzzz+TpaUlffrpp+11ChpD3bEIDQ0lExMT2r17N+Xn59N//vMf6tWrF/n5+bXXKWiMu3fv0oULF+jChQsEgNavX08XLlygP/74g4iIQkJCaOrUqUL9uuXz77//PmVlZdGWLVt4+XydzZs3U7du3UgkEpGbmxv997//FfYNGzaMpk+frlR/z5495OjoSCKRiJ577jk6fPhwG0esudQZi+7duxOAej+hoaFtH7iGUvd341GcCLUsdcfi9OnT5O7uTmKxmOzs7Cg8PJxqa2vbOGrNpM5YPHjwgFatWkW9evUiQ0NDkslkNG/ePCorK2v7wDXMyZMnVX4H1H3+06dPp2HDhtVr4+zsTCKRiOzs7CgqKkrt4+oQ8bU8xhhjjGknjZojxBhjjDGmDk6EGGOMMaa1OBFijDHGmNbiRIgxxhhjWosTIcYYY4xpLU6EGGOMMaa1OBFijDHGmNbiRIgxpiQ6OhpmZmbtHUaz6ejo4Keffmq0zowZMzBu3Lg2iYcx9mzjRIgxDTRjxgzo6OjU+8nNzW3v0BAdHS3Eo6uri65du2LmzJm4ceNGi/RfVFSEV199FQBQWFgIHR0dpKenK9XZuHEjoqOjW+R4DVm1apVwnnp6epDJZJgzZw5KS0vV6oeTNsZal0a9fZ4x9jcfHx9ERUUplXXu3LmdolEmlUqRnZ0NhUKBixcvYubMmfjzzz9x9OjRp+67obeGP8rU1PSpj9MUzz33HI4fPw65XI6srCy89dZbuHPnDuLi4trk+IyxJ+MrQoxpKLFYDGtra6UfPT09rF+/Hs8//zyMjY0hk8kwb948VFRUNNjPxYsXMXz4cJiYmEAqlcLV1RW//vqrsD85ORlDhw6FRCKBTCbDwoULUVlZ2WhsOjo6sLa2hq2tLV599VUsXLgQx48fR1VVFRQKBT7++GN07doVYrEYzs7OSEhIENrW1NQgODgYNjY2MDQ0RPfu3bFmzRqlvutujfXs2RMAMHDgQOjo6ODll18GoHyVJSIiAra2tkpvdgcAX19fvPXWW8L2wYMH4eLiAkNDQ9jZ2SEsLAy1tbWNnqe+vj6sra3RpUsXeHl5YdKkSTh27JiwXy6XIygoCD179oREIoGTkxM2btwo7F+1ahV27NiBgwcPCleXkpKSAADXrl2Dn58fzMzMYG5uDl9fXxQWFjYaD2OsPk6EGNMyurq62LRpEy5duoQdO3bgxIkT+OCDDxqsHxgYiK5du+LcuXNIS0tDSEgIDAwMAAB5eXnw8fHBhAkTkJGRgbi4OCQnJyM4OFitmCQSCRQKBWpra7Fx40asW7cOX375JTIyMuDt7Y3XX38dV65cAQBs2rQJhw4dwp49e5CdnY2YmBj06NFDZb9nz54FABw/fhxFRUXYv39/vTqTJk3CrVu3cPLkSaGstLQUCQkJCAwMBACcOnUK06ZNwzvvvIPMzEx89913iI6ORnh4eJPPsbCwEEePHoVIJBLKFAoFunbtir179yIzMxMrV67E0qVLsWfPHgDAkiVL4OfnBx8fHxQVFaGoqAienp548OABvL29YWJiglOnTiElJQUdOnSAj48PampqmhwTYwzQyLfPM6btpk+fTnp6emRsbCz8TJw4UWXdvXv3UqdOnYTtqKgoMjU1FbZNTEwoOjpaZdugoCCaM2eOUtmpU6dIV1eXqqqqVLZ5vP+cnBxydHSkQYMGERGRra0thYeHK7UZPHgwzZs3j4iIFixYQCNGjCCFQqGyfwB04MABIiIqKCggAHThwgWlOtOnTydfX19h29fXl9566y1h+7vvviNbW1uSy+VERPTKK6/Q6tWrlfrYuXMn2djYqIyBiCg0NJR0dXXJ2NiYDA0NhTdpr1+/vsE2RETz58+nCRMmNBhr3bGdnJyUPoP79++TRCKho0ePNto/Y0wZzxFiTEMNHz4c33zzjbBtbGwM4OHVkTVr1uDy5csoLy9HbW0tqqurce/ePRgZGdXrZ/HixZg1axZ27twp3N7p1asXgIe3zTIyMhATEyPUJyIoFAoUFBSgT58+KmO7c+cOOnToAIVCgerqarz44ovYunUrysvL8eeff2LIkCFK9YcMGYKLFy8CeHhba+TIkXBycoKPjw/GjBmDUaNGPdVnFRgYiNmzZ+Prr7+GWCxGTEwMAgICoKurK5xnSkqK0hUguVze6OcGAE5OTjh06BCqq6vxww8/ID09HQsWLFCqs2XLFmzfvh1Xr15FVVUVampq4Ozs3Gi8Fy9eRG5uLkxMTJTKq6urkZeX14xPgDHtxYkQYxrK2NgY9vb2SmWFhYUYM2YM5s6di/DwcJibmyM5ORlBQUGoqalR+YW+atUqTJkyBYcPH8aRI0cQGhqK2NhYvPHGG6ioqMDbb7+NhQsX1mvXrVu3BmMzMTHB+fPnoaurCxsbG0gkEgBAeXn5E8/LxcUFBQUFOHLkCI4fPw4/Pz94eXlh3759T2zbkLFjx4KIcPjwYQwePBinTp3Chg0bhP0VFRUICwvD+PHj67U1NDRssF+RSCSMwdq1a/Haa68hLCwMn3zyCQAgNjYWS5Yswbp16+Dh4QETExN88cUXOHPmTKPxVlRUwNXVVSkBrfOsTIhn7J+CEyHGtEhaWhoUCgXWrVsnXO2om4/SGEdHRzg6OmLRokWYPHkyoqKi8MYbb8DFxQWZmZn1Eq4n0dXVVdlGKpXC1tYWKSkpGDZsmFCekpICNzc3pXr+/v7w9/fHxIkT4ePjg9LSUpibmyv1VzcfRy6XNxqPoaEhxo8fj5iYGOTm5sLJyQkuLi7CfhcXF2RnZ6t9no9bvnw5RowYgblz5wrn6enpiXnz5gl1Hr+iIxKJ6sXv4uKCuLg4WFpaQiqVPlVMjGk7nizNmBaxt7fHgwcPsHnzZuTn52Pnzp349ttvG6xfVVWF4OBgJCUl4Y8//kBKSgrOnTsn3PL68MMPcfr0aQQHByM9PR1XrlzBwYMH1Z4s/aj3338fn332GeLi4pCdnY2QkBCkp6fjnXfeAQCsX78eu3fvxuXLl5GTk4O9e/fC2tpa5UMgLS0tIZFIkJCQgJKSEty5c6fB4wYGBuLw4cPYvn27MEm6zsqVK/H9998jLCwMly5dQlZWFmJjY7F8+XK1zs3DwwP9+/fH6tWrAQAODg749ddfcfToUeTk5GDFihU4d+6cUpsePXogIyMD2dnZuHnzJh48eIDAwEBYWFjA19cXp06dQkFBAZKSkrBw4UJcv35drZgY03rtPUmJMdbyVE2wrbN+/XqysbEhiURC3t7e9P333xMAKisrIyLlycz379+ngIAAkslkJBKJyNbWloKDg5UmQp89e5ZGjhxJHTp0IGNjY+rfv3+9yc6Penyy9OPkcjmtWrWKunTpQgYGBjRgwAA6cuSIsD8iIoKcnZ3J2NiYpFIpvfLKK3T+/HlhPx6ZLE1EFBkZSTKZjHR1dWnYsGENfj5yuZxsbGwIAOXl5dWLKyEhgTw9PUkikZBUKiU3NzeKiIho8DxCQ0NpwIAB9cp3795NYrGYrl69StXV1TRjxgwyNTUlMzMzmjt3LoWEhCi1u3HjhvD5AqCTJ08SEVFRURFNmzaNLCwsSCwWk52dHc2ePZvu3LnTYEyMsfp0iIjaNxVjjDHGGGsffGuMMcYYY1qLEyHGGGOMaS1OhBhjjDGmtTgRYowxxpjW4kSIMcYYY1qLEyHGGGOMaS1OhBhjjDGmtTgRYowxxpjW4kSIMcYYY1qLEyHGGGOMaS1OhBhjjDGmtTgRYowxxpjW+n8onynHw/UyGgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, lw=2, label=\"Train ROC curve (area = %0.2f)\" % auroc_train)\n",
    "plt.plot(fpr_eval, tpr_eval, lw=2, label = \"Test ROC curve (area = %0.2f)\" % auroc)\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "1. Why do we pack the sequences in PyTorch | Saturn Cloud Blog. (2023, September 7). https://saturncloud.io/blog/why-do-we-pack-the-sequences-in-pytorch/\n",
    "2. Why do we “pack” the sequences in PyTorch? (n.d.). Stack Overflow. https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch\n",
    "3. W&B. (2023). Weights & biases. W&B. https://wandb.ai/wandb_fc/LayerNorm/reports/Layer-Normalization-in-Pytorch-With-Examples---VmlldzoxMjk5MTk1\n",
    "4. Reyna, M. A., Josef, C. S., Jeter, R., Shashikumar, S. P., Westover, M. B., Nemati, S., Clifford, G. D., & Sharma, A. (2020). Early prediction of sepsis from clinical data: The PhysioNet/Computing in Cardiology Challenge 2019. Critical Care Medicine, 48(2), 210–217. https://doi.org/10.1097/ccm.0000000000004145"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a8603ccbd2d8b070d71fca113a51bee38e5d9eaeb1ab901c45d498760580e24"
  },
  "kernelspec": {
   "display_name": "Python 3.11.3 64-bit ('deep_learning_test-Kesrgl1c': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
